\chapter*{Introduction}

\begin{figure}[htb]
    \vspace{-10pt}
    \begin{center}
        \includegraphics[trim= 30mm 10mm 30mm 10mm, clip, width=\textwidth]{resources/citations}
    \end{center}
    \vspace{-20pt}
    \caption{\small Approximate number of papers (by year) published between $1980$ and $2011$ containing the terms "anomaly detection", "outlier detection" and "novelty detection". All three terms exhibit strong upward trends in recent years. Source: Google Scholar.}
    \vspace{-0pt}
    \label{fig:citations}
\end{figure}

In recent years, anomaly detection has become increasingly important in a variety of domains in business, science and technology. Roughly defined as the automated detection within data sets of elements that are somehow abnormal, anomaly detection encompasses a broad set of techniques and problems. In part due to the emergence of new application domains, and in part due to the evolving nature of many traditional domains, new applications of and approaches to anomaly detection and related subjects are being developed at an increasing rate, as indicated in Figure \ref{fig:citations}.

In spite of this increase in research activity, relatively few papers have been published that compare different approaches to anomaly detection tasks in relation to specific domains. As mentioned in the foreword, the main objective of this thesis project was to mitigate this deficiency in the context of large sets of temporal machine-generated data, with the goal of identifying and implementing the methods most appropriate for this domain.

The term \emph{machine-generated data} (or \emph{machine data} for short) refers to any data consisting of discrete events that have been created automatically from a computer process, application, or other machine without the intervention of a human. Common types of machine-generated data include computer, network, or other equipment logs; environmental or other types of sensor readings; or other miscellaneous data, such as location information \cite{machine_data}. Splunk is designed for this type of data, especially data sets where each event has an associated time stamp; such data is referred to as \emph{temporal machine data} throughout this report.

This project resulted in several contributions to the subject of anomaly detection. New theory was introduced in the form of the \emph{tasks and problems framework} and the \emph{component framework}. The former can be used to consolidate the different approaches to anomaly detection and to simplify reasoning about and comparing anomaly detection methods and problems systematically. It is used in this report to present the subject of anomaly detection in detail and to determine the anomaly detection tasks most pertinent to the target domain. Based on this discussion, a specific, previously unstudied task is chosen as a focus for the remainder of the report.

The component framework is used to simplify reasoning about and comparing problem formulations derived from this target task. It is used to derive a single, simple algorithm for solving a large class of problem formulations derived from the target task, as well as to derive a set of interesting problem formulations belonging to this class.

Finally, a software implementation of the component framework along with an extensive set of evaluation utilities, called \texttt{ad-eval}, is introduced. This software, which was released under an open-source license at \url{http://github.com/aeriksson/ad-eval} was developed as part of this project in order facilitate the unbiased and reproducible evaluation of anomaly detection methods on the target task, as well as to mitigate some of the deficiencies in evaluations found in the literature. While an appropriate evaluation could unfortunately not be performed within the scope of this project due to a lack of data, a qualitative evaluation of the methods implemented in \texttt{ad-eval} was performed and included in this report. All scripts used in this evaluation were included in the \texttt{ad-eval} source code repository, so that they can be reused for a proper evaluation once sufficient data is available.

In Chapter \ref{ch:background}, various background information pertinent to the rest of the report is presented. Specifically, the subject of anomaly detection is presented in more depth, along with some of the issues with current approaches to the subject; the framework of tasks and problems is introduced; and the application of anomaly detection to Splunk is discussed. 

Chapter \ref{ch:tasks} contains a thorough, structured review of anomaly detection within the framework of tasks and problems. Each of the factors required to specify an anomaly detection problem is investigated, emphasizing the enumeration of choices of these factors. Where pertinent, the implications of and relations between these choices are discussed.

Since the types of anomalies that can be captured by a problem formulation depend heavily on how the data sets are presented, the selection of appropriate data representations constitutes an important aspect of the design of anomaly detection methods. The lack of structure often exhibited by machine-generated data sets makes this task especially challenging. Chapter \ref{ch:transformations} is dedicated to the discussion of various aspects of data representations and how they relate to sets in the target domain. 

In Chapter \ref{ch:problems}, the preceding two chapters are used as a basis to derive the most important anomaly detection tasks when analyzing the types of machine-generated data encountered in Splunk and similar applications. The specific task of detection of contextual collective anomalies in long univariate time series is selected for implementation and analysis.

In Chapter \ref{ch:methods}, the chosen task is treated in detail. The component framework is presented and used to identify potentially interesting problem formulations. Ways in which the framework may be modified to address other tasks is presented.

In Chapter \ref{ch:evaluation}, some of the difficulties in evaluating the accuracy of anomaly detection methods for the target task are considered. A few error measures for evaluations on the target task are also proposed.

The design and structure of \texttt{ad-eval}, which consists of an implementation of the component framework and a comprehensive set of evaluation utilities, is discussed in Chapter \ref{ch:implementation}. 

The results of the evaluation are presented in Chapter \ref{ch:results}. Specifically, this chapter contains an explanation of the evaluation methodology used, a brief exposition relating the concept of parameter spaces to the framework developed in Chapter \ref{ch:methods}, and a thorough qualitative evaluation of how parameter choices affect the analysis for kNN-based methods. 

Finally the report is concluded in Chapter \ref{ch:discussion} with a summary of the project and a few possible directions for future study.
