\chapter{A Framework for Anomaly Detection}
\label{ch:framework}

In this chapter, a framework for reasoning about anomaly detection problem formulations is presented. This framework can be utilised in order to limit the scope of the optimisation problem outlined in the previous chapter by enabling the systematic construction of tractable problem sets.

The core idea of the framework is that anomaly detection problems can be almost exhaustively classified based on a few independent factors, and that by studying the factor choices handled in the anomaly detection literature, insights may be obtained into what problem formulations are appropriate for specific applications as well as how to formulate algorithms which solve these problem formulations.

As mentioned in Section~\ref{sect:problem_formulation}, a problem formulation is a specification that associates with each element in the set $\mathcal{D}$ of possible datasets a unique element of the set $\mathcal{S}$ of possible solutions. In other words, problem formulations can be seen as functions $P: \mathcal{D} \rightarrow \mathcal{S}$, and the problem set $\mathcal{P}$ can be seen as the set of all such functions. Correspondingly, the task of selecting an appropriate restricted problem set $\mathcal{P}^*$ is equivalent to the task of finding an appropriate restricted subset of such functions.

One interesting aspect of anomaly detection is that almost all problem formulations found in the literature share common structure, in that they involve selecting a set of subsets of the input data, comparing each element of this set to some set of reference elements to produce a set of anomaly scores, and aggregating these anomaly scores into a result.

If this common structure could be captured by decomposing $P$ into a collection of functions between sets, then the task of finding an appropriate $\mathcal{P}^*$ could be simplified to the task of placing appropriate restrictions on these individual functions. Correspondingly, formulating an oracle which can solve arbitrary $P \in \mathcal{P}^*$ could be simplified to the task of computing each individual function. If an efficient such oracle could be found, then software could be constructed which could, given a problem set in the form of a set of restrictions on the functions, automatically solve the optimisation problem for arbitrary applications.

\newcommand{\exinput}{\includegraphics[width=33.0ex, trim=0 27.0ex 0 0]{resources/components/input}}
\newcommand{\extransformedinput}{\includegraphics[width=33.0ex, trim=0 27.0ex 0 0]{resources/components/transformed_input}}
\newcommand{\exevaluationset}{\includegraphics[width=33.0ex, trim=0 27.0ex 0 0]{resources/components/evaluation_set}}
\newcommand{\exevaluationitem}{\includegraphics[width=5.5ex, trim=0 7.0ex 0 0]{resources/components/evaluation_item}}
\newcommand{\excontext}{\includegraphics[width=17.0ex, trim=0 27.0ex 0 0]{resources/components/context}}
\newcommand{\exreferenceset}{\includegraphics[width=17.0ex, trim=0 27.0ex 0 0]{resources/components/reference_set}}
\newcommand{\exanomalyscore}{\includegraphics[width=5.5ex, trim=0 5.5ex 0 0]{resources/components/anomaly_score}}
\newcommand{\exanomalyscores}{\includegraphics[width=33.0ex, trim=0 27.0ex 0 0]{resources/components/anomaly_scores}}
\newcommand{\exsolution}{\includegraphics[width=33.0ex, trim=0 27.0ex 0 0]{resources/components/solution}}
\newcommand{\extransformedsolution}{\includegraphics[width=33.0ex, trim=0 27.0ex 0 0]{resources/components/transformed_solution}}

\section{The problem decomposition}
\label{sect:data_format}

We now present such a decomposition of $P$, which covers almost all previously studied problem formulations by capturing the common structure mentioned above. This decomposition is now presented in detail, assuming that $\mathcal{D}$ and $\mathcal{S}$ are fixed by the application. In parallel, illustrations are presented of a decomposition of an actual anomaly detection problem.

\begin{figure}[htb]
\begin{center}
\leavevmode
\includegraphics[width=40.0ex, trim=0 7.0ex 0 0]{resources/components/input}
\includegraphics[width=40.0ex, trim=0 7.0ex 0 0]{resources/components/transformed_solution}
\end{center}
\caption{\small{The example input data $D \in \mathcal{D}$ and the corresponding solution $P(D) = S \in \mathcal{S}$.}}
\label{fig:input_data}
\end{figure}

Roughly, our example problem associates an anomaly score with each element in a grid of colour values. These anomaly scores are also colours; red and green signify high and low anomaly scores, respectively. Specifically, the problem involves finding contextual collective anomalies---i.e.\ contiguous subsets of the data which are anomalous with regards to their surroundings---in such grids.  To illustrate this problem, we will use the dataset shown in figure~\ref{fig:input_data}. This dataset contains an interesting anomaly to the left; a blue region that is larger than and has a different shape than nearby blue regions. The problem $P$ we will decompose can be used to identify this anomaly, and the corresponding solution is shown to the right in the figure.

We make the assumption that the input data is an ordered collection, i.e.\ a list~\footnote{We will denote a list containing the items $a, b$ and $c$ by $[a, b, c]$, and we will denote the set of all lists with items in some set $X$ by $X$. We will also assume that items in lists implicitly carry indices, and that any function $f: [X] \rightarrow [X]$ that maps a list to one of its sub-lists will preserve these indices, i.e.\ if $f([a, b, c]) = [b]$, then it is apparent that $f([a, b, c])$ is the second element of $[a, b, c]$, even if $b = a$ or $b = c$.}, of homogeneous items, which we denote by $\mathcal{D} = [D]$ for some arbitrary set $D$. Likewise, we take the solution format to be a list as well: $\mathcal{S} = [S]$ for some arbitrary set $S$. This means that we consider the set of problems to be equivalent to the set of all functions $P: [D] \rightarrow [S]$.

The proposed decomposition splits each such $P$ into a composition of the following functions:

\begin{enumerate}
    \item{A transformation $T_D: [D] \rightarrow [D']$,}which transforms a list of input data with elements in some set $D$ to a list of elements in some other set $D'$. Typically, this is done in order to speed up or simplify the analysis. In our example~\footnote{Note that we here implicitly assume that a well-defined ordering of the elements in the example is provided, such that the data can be treated as a list.}, this transformation reduces the dimensionality of the dataset, by averaging the values of adjacent elements:
    \[
        T_{D}\left( \exinput \right) = \extransformedinput
    \]
    \item{A evaluation filter $F_E: [D'] \rightarrow [[D']]$,}which maps the transformed data to an \emph{evaluation set}---a list of subsets of the transformed data, corresponding to potential anomalies. In our example, $F_E$ simply partitions its input into collections of four elements:
    \[
        F_E\left( \extransformedinput \right) = \exevaluationset
    \]
\item{A context function\footnote{We will take $(X, Y)$ to mean the set of tuples with the first element in $X$ and the second element in $Y$. In other words, $(X, Y)$ is just the Cartesian product $X \times Y$. } $C: ([D'], [D']) \rightarrow [D']$,}which takes a dataset and a corresponding candidate anomaly (i.e.\ a sublist of the dataset), and produces an associated \emph{context}. In our example, the context function $C(X, Y)$ produces a set of elements in $X$ adjacent to $Y$:
    \[
        C\left(\extransformedinput, \exevaluationitem \right) = \excontext
    \]
\item{A reference filter $F_R: [D'] \mapsto [[D']] $,}which works analogously to the evaluation filter, but operates on contexts instead of input data. In our case, $F_R$ is identical to $F_E$; it partitions the context into subsets of four items:
    \[
        F_R\left( \excontext \right) = \exreferenceset
    \]
    \item{An anomaly measure $M: ([D'], [[D']]) \rightarrow A$,} which takes an item $x \in [D']$ and a list $[x_1, x_2, \dots, x_n] \in [[D']]$ of reference items, and computes an anomaly score (in some arbitrary set $A$) based on how anomalous $x$ is with regards to $[x_1, x_2, \dots, x_n]$. In our example, $M$ works by computing an average distance between $x$ and the $x_i$, which is mapped to a colour:
    \[
        M\left( \exevaluationitem , \exreferenceset \right) = \exanomalyscore
    \]
    Computing $f(E) = [m(e_1), m(e_2), \dots, m(e_n)]$, where $E = [e_1, e_2, \dots, e_n]$ is the evaluation set, $X$ is the input dataset and $m(e) = M(e, F_R(C(X, e)))$ gives:
    \[
        f\left(\exevaluationset\right) = \exanomalyscores %\includegraphics[width=33.0ex, trim=0 5.0ex 0 0]{resources/components/anomaly_scores}
    \]
    \item{An aggregation function $\Sigma: ([[X']], [A]) \rightarrow [S']$,} which aggregates the anomaly scores for the elements of the evaluation set to form a `preliminary' solution for elements of $[D']$. In our example, the aggregation function associates with each element the anomaly score of the candidate anomaly to which that element belongs:
    \[
        \Sigma \left( \includegraphics[width=23.0ex, trim=0 23ex 0 0]{resources/components/evaluation_set}, \includegraphics[width=23.0ex, trim=0 23ex 0 0]{resources/components/anomaly_scores} \right) = \\ \includegraphics[width=23.0ex, trim=0 23ex 0 0]{resources/components/solution}
    \]
\item{A transformation $T_S: [S'] \rightarrow [S]$}, which transforms the preliminary solution into an actual solution. In our example, $T_S$ uses bilinear interpolation to produce anomaly scores for all elements of the input data:
    \[
        T_S \left( \exsolution \right) = \extransformedsolution
    \]
\end{enumerate}

The process of computing a solution to a problem $P$ with associated $T_D$, $F_E$, $C$, $F_R$, $M$, $\Sigma,$ and $T_S$ for an input dataset $d \in [D]$ can be seen as a series of transformations on $d$.

Now, the sets $\mathcal{D}$ and $\mathcal{S}$, as well as the functions $T_D$, $F_E$, $C$, $F_R$, $M$, $\Sigma$, and $T_S$ are discussed in detail.

% TODO: remove the following
%
% First, we allow for an input transformation $T$, which takes an element $D \in \mathcal{D}$ to an element $D' \in \mathcal{D}'$ for some problem specific set $\mathcal{D}'$, as follows:
% \[
%     D \in \mathcal{D} \xmapsto{\quad T \quad} D' \in \mathcal{D}'
% \]
% Such transformations are commonly applied to reduce the dimensionality or numerosity of $D$ in order to facilitate the analysis (typically $\mathcal{D}'$ has a lower cardinality than $\mathcal{D}$).
%
% Next, we associate an \emph{evaluation set} $E_{D'}$ of candidate anomalies (subsets of $D'$) with $D'$, by means of a function $F_E$ (the \emph{evaluation filter}):
% \[
%     D' \in \mathcal{D}' \xmapsto{\quad F_E \quad} E_{D'} \subset 2^{\mathcal{D}'}
% \]
% The most general (and least tractable) $F_E$ simply selects every subset of $D'$ (i.e.\ $F_E(D') = 2^{D'}$). A more common approach is to let $F_E$ be the singleton sets of $D'$ (i.e.\ $F_E(D') = \{\{d\}\,:\,d \in D'\}$).
%
% For each candidate anomaly $E \in E_{D'}$, we define a \emph{context} set $C_E \in X$, where $X$ is some problem-specifc set, by means of a \emph{context function}:
% \[
%     E \in E_{D'} \xmapsto{\quad C \quad} C_E \in X
% \]
% The point of the context set is to provide a convenient means of selecting, or limiting, the set of elements with which $E$ is to be compared. For unsupervised anomaly detection, it will be the case that $X = D'$, while for semi-supervised anomaly detection, $X$ is some arbitrary set of training data. We will assume that $E \cap C(E) = \emptyset$.
%
% Most previously studied problem formulations involve the comparison of each $E \in E_{D'}$ with a set of elements of similar size to $E$. Since $C(E)$ will typically not fulfill this requirement, we allow for a \emph{reference filter}, which maps $C_E$ to a set $R_E$ of reference data:
% \[
%     C_E \in X \xmapsto{\quad F_R \quad} R_E \in 2^X
% \]
% While for some problem formulations $F_R(C_E) = C_E$, for the vast majority of applications $F_R(C_E)$ is some subset of $\{c \subset C_E\,:\, |c| = |E|\}$
%
% Next, we introduce what is arguably the most important part of the decomposition; the \emph{anomaly measure} $A$. This is a function that maps a candidate anomaly $E$, along with a reference set $R_E$, to numerical \emph{anomaly score} representing how anomalous $E$ is with regards to the elements of $R_E$:
% \[
%     (E, R_E) \in (E_D \times 2^X) \xmapsto{\quad A \quad} A_E \in \mathbb{R}^+
% \]
% A wide variety of anomaly measures have been studied; common examples include measures of distance (how far is $E$ from the rest of $R_E$ with regards to some distance $\delta$?) and statistical estimates (how likely is $E$ to have been generated from the same process as $R_E$?).
%
% At this point, we have provided the means for finding anomalous subsets of $D'$. However, it is typically not the case that $\{A_E\,:\,E \in E_{D'}\} \in \mathcal{S}$. To provide a means of giving an actual solution in $\mathcal{S}$, an aggregation method must be provided. We do this in the form of a function $\Sigma$ which maps the $E$ and $A_E$ to some $S \in \mathcal{S}$:
% \[
%     (E_D, A[E_D, (F_R \circ C)(E_D)]) \in (2^{\mathcal{D}'} \times (\mathbb{R}^+)^{|E|}) \xmapsto{\quad \Sigma \quad} S(D) \in \mathcal{S}
% \]

\section{The input data format $\mathcal{D}$}
\label{sect:data_format}

As mentioned above, we represent the set of possible input datasets by means of a set $\mathcal{D} \subset [D]$, i.e.\ a set of lists over some application-specific set $D$. A problem formulation associates with each element of $\mathcal{D}$ a solution in the set of solutions $\mathcal{S}$.

Methods are commonly classified based on characteristics of $D$. For instance, a distinction is typically made between categorical, discrete, and real-valued data based on the cardinality of $D$. The input data is said to be \emph{categorical} (or \emph{symbolic}) if $D$ is finite, \emph{discrete} if $D$ is countable and \emph{real-valued} if $D \subseteq \mathbb{R}^n$ for some $n$ (other uncountable sets are typically not encountered). It is also frequently the case that $D$ consists of some combination of categorical, discrete and real-valued data, in which case the input data is referred to as \emph{mixed}\cite{TODO}.

\begin{wrapfigure}{r}{0.5\textwidth}
\changecaptionwidth
\captionwidth{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{resources/multi_vs_univariate}
\caption[derp]{Two sine curves regarded as two separate univariate time series (dotted lines) and as one multivariate time series (solid lines).}
\label{fig:multi_vs_univariate}
\end{wrapfigure}

Another classification, also based on characteristics of $D$, is that between uni- and multivariate data. If $D = X^n$ for some set $X$, the input data is called \emph{multivariate}; otherwise it is called \emph{univariate}. An illustration of uni- and multivariate time series is shown in figure~\ref{fig:multi_vs_univariate}.

Characteristics such as the dimensionality of the data typically prove important in applications. For instance, categorical data is typically both computationally and conceptually easier to handle than either discrete or real-valued data. Likewise, univariate data is typically much easier to handle than multivariate data.

\section{The set of solutions $\mathcal{S}$}

The collection of possible solutions is modelled as a set $\mathcal{S} \subset [S]$, i.e.\ some set of lists over some application-specific set $S$. In this section, typical choices of $\mathcal{S}$---which will henceforth be referred to as the \emph{solution format}---are discussed. In practice, only a few distinct solution formats are used.

One common solution format involves associating \emph{anomaly scores} with each element of the input dataset. In other words, for some input data $[d_1, d_2, \dots, d_n]$, the corresponding solution will have the format $[s_1, s_2, \dots, s_n] \in [S]$, where each $s_i$ indicates how anomalous the corresponding $d_i$ is. Typically,  $S = \mathbb{R}^+$.

Another common approach is to, for some input data $[d_1, d_2, \dots, d_n]$, let $S$ contain the indices of the most anomalous elements of $D$, i.e.\ $S = \mathbb{Z}_n$. Two approaches can be distinguished here. Either, the solution has a fixed size, in which case the solution format is typically referred to as \emph{discords}~\cite{keogh1} \cite{bu} \cite{yankov} \cite{fu} \cite{lin}., or the solution will contain the indices of all elements which are considered sufficiently anomalous.

Finally, it might be interesting to let the solution consist of a list of anomalous subsets of the input data, i.e.\ $S = [D]$.

Which of these solution formats is used depends on the performance requirements of the application. For instance, while any set of anomalous indices can be constructed from a real-valued anomaly score vector, producing a set of anomalous indices (and especially discords) directly can be far less computationally intensive.

\section{The transformations $T_D$ and $T_S$}

It is common for the input data to be preprocessed to make it more amenable to analysis. To account for this, two transformations $T_D: [D] \rightarrow [D']$ and $T_S: [S'] \rightarrow [S]$ are included in the framework. These transformations are complementary, in the sense that $T_D$ maps the input data to some set $[D']$, while $T_S$ takes a solution $[S']$ to some problem defined on elements in $[D']$ and maps it to a solution in $[S]$ for the data fed into $T_D$.

Typically, $T_D$ involves either dimensionality or numerosity reduction. \emph{Dimensionality reduction} involves reducing the dimensionality in the individual elements of the input dataset; i.e.\ a transformation $T_D: [D] \rightarrow [D']$ is a dimensionality reduction transformation if $D'$ is of lower dimensionality than $D$.

Such transformations invariably involve some degree of information loss. Ideally, the information which they retain should be that which is most relevant to the analysis. Many methods have been designed with this goal in mind. A distinction is typically made between feature selection and feature extraction methods. \emph{Feature selection} methods select a subset of the features present in the original data, while \emph{feature extraction} methods create new features from the original data. An example of feature extraction is shown in figure~\ref{fig:dimensionality_reduction}.

Common feature extraction methods for data in $\mathbb{R}^n$ include \emph{principle component analysis}~\cite{TODO} (PCA), \emph{semidefinite embedding}~\cite{TODO}, \emph{partial least-squares regression}~\cite{TODO}, and \emph{independent component analysis}~\cite{TODO}. Which methods are appropriate to use depends heavily on the application.

\emph{Numerosity reduction}, on the other hand, serves to reduce the cardinality of the data, either by converting real-valued data to discrete or categorical data, by converting discrete data to categorical data, or by compressing categorical data. An example of numerosity reduction in time series is shown in figure~\ref{fig:types_of_data}.

\begin{figure}[htb]
\begin{center}
\leavevmode
\includegraphics[width=\textwidth]{resources/multivariate}
\end{center}
\caption{\small{An example of dimensionality reduction in a point anomaly detection problem in $R^2$. The left figure shows a set of $500$ data points $(x_i, y_i)$ containing one anomaly. The top right figure shows a histogram of the $x_i$, while the bottom right figure shows a histogram of the distance from the center point. In each figure, the location of the anomalous point is marked by an arrow. While the anomaly is easy to detect in the left and bottom right figures, it can not be seen in the top right figure. This is due to the linear inseparability of the data, and illustrates how dimensionality reduction can lead to information losses if not performed properly.}}
\label{fig:dimensionality_reduction}
\end{figure}

\section{The evaluation filter $F_E$}

An important aspect of any problem is which subsets of the transformed data are considered candidate anomalies; i.e.\ which sublists of the transformed data $[d_1', d_2', \dots, d_n'] \in [D']$ constitute the \emph{evaluation set} $E \in [[D']]$. Letting the evaluation set consist of all sublists is not computationally feasible, and considering only single element lists is likely to be overly limiting for many applications. To allow for greater flexibility in the choice of evaluation set, the framework includes a function, the \emph{evaluation filter} $F_E: [D'] \rightarrow [[D']]$, which includes the choice of evaluation set as part of the problem formulation.

What $F_E$ is appropriate depends on whether or not there is any structure that relates the elements of the input data. If no such structure is present, allowing candidate anomalies with more than one element is not meaningful, and $F_E$ should be given by $F_E([d_1', d_2', \dots, d_n']) = [[d_1'], [d_2'], \dots, [d_n']]$. On the other hand, if any such structure (such as an ordering of or distance between the elements) exists (and is pertinent to the analysis), then $F_E$ ought to take that structure into account.

As an example, consider the case where the input elements $X = [d_1', d_2', \dots, d_k']$ constitute a sequence. Here, a concept of locality is naturally induced by the sequence ordering, and it is reasonable that $F_E(X)$ consist of contiguous sublists of $X$:
\[
    F_E([d_1', d_2', \dots, d_k']) = [[d_{a_1}', d_{a_1 + 1}', \dots, d_{b_1}'], [d_{a_2}', d_{a_2 + 1}', \dots, d_{b_2}'], \dots, [d_{a_n}', d_{a_n + 1}', \dots, d_{b_n}']]
\]
for some arbitrary $n$, $a_1, a_2, \dots, a_n$ and $b_1, b_2, \dots, b_n$, where $\forall i: b_i > a_i$.

An evaluation filter which extracts contiguous sublists of a fixed length is illustrated in Figure~\ref{fig:filters}.

\section{The context function $C$}

\begin{figure}[htb]
    \begin{center}
        \includegraphics[width=1\textwidth]{resources/filters}
    \end{center}
    \caption{{\small Schematic illustration of filters and contexts acting on a sequence $\mathbf{s} = [s_1, s_2, \dots, s_{40}]$. The top panel shows the evaluation set $E = F_E(\mathbf{s}) = [e_1, e_2, \dots, e_{19}]$ extracted by an evaluation filter which selects contiguous subsequences of fixed length. The bottom panel shows the context of $e_{10}$ given by $C(\mathbf{s}, e_{10}) = [c_1, c_2, \dots, c_{24}]$, as well as the corresponding reference set $F_R(e_{10}) = [t_1, t_2, \dots, t_{10}]$ extracted by a reference filter identical to the evaluation filter.}}
\label{fig:filters}
\end{figure}

Once the candidate anomalies have been identified, a \emph{context} must be associated with each candidate anomaly. The context for a candidate anomaly represents the set of elements with which that candidate anomaly is to be compared. To this end, the decomposition associates with each problem a \emph{context function} $C: ([D'], [D']) \rightarrow [D']$, which takes the transformed input data and a list of candidate anomalies, and associates with each candidate anomaly a context (in $[D'])$.

As with the evaluation filter, what constitutes an appropriate context function is entirely dependent on what structure is present in the input data. If no such structure is present, the only reasonable choices of $C$ are
\[
    C([d_1', d_2', \dots, d_k'], [d_{a_1}', d_{a_2}', \dots, d_{a_l}']) = [d_{b_1}', d_{b_2}', \dots, d_{b_m}']
\]
where $\{b_1, b_2, \dots, b_m\} = \mathbb{Z}_k \setminus \{a_1, a_2, \dots, a_l\}$, or $C(X, Y) = T$ for some fixed data $T \in [D']$. If, on the other hand, there is such structure present, a context function which takes it into account might be preferable. As a concrete example of such an application with such structure, consider a long sequence: often how an item compares to those items `closest' to it (in the ordering) is more relevant to the analysis than how it compares to the rest of the sequence.

\begin{figure}[thb]
    \vspace{-4pt}
    \begin{center}
        \leavevmode
        \includegraphics[width=1\textwidth]{resources/contextz}
    \end{center}
    \vspace{-15pt}
    \caption{{\small Schematic view of a dataset illustrating a few contexts. In each panel, the black dots represent selected items, the dark grey dots represent items in the context of the selected items, and the light grey dots indicate items not in the context of the selected items. The left panel shows the trivial context---all items are part of the context. The middle panel shows a local context of a single item. The right panel shows a local context of a subset of the dataset.}}
\label{fig:contexts}
    \vspace{-5pt}
\end{figure}

Context functions generalise several of the concepts discussed in the previous chapter; semi-supervised anomaly detection with normal training data and unsupervised anomaly detection correspond to $C(X, Y) = T$ and $C(X, Y) \subset X$ respectively, while contextual and point anomalies are just special cases of $C(X, Y) \subset X$. Context functions also generalise anomaly detection problems to various tasks that have traditionally been considered separate from anomaly detection. For instance, \emph{novelty detection}~\cite{chandola}, which refers to the detection of novel---or previously unseen---items or subsequences in a sequence~\footnote{It should be noted that the term `novelty detection' is occasionally used in the literature to refer to what is referred to as semi-supervised anomaly detection in this report.}, is really just the use of a one-sided context
\[
    C([s_1, s_2, \dots, s_n], [s_i, s_{i+1}, \dots s_j]) = [s_{i-w}, s_{i - w + 1}, \dots, s_{i - 1}],
\]
for some $w$, in an anomaly detection problem.

A few possible context functions are shown in Figures~\ref{fig:filters} and~\ref{fig:contexts}.

\section{The reference filter $F_R$}

Once a set of candidate anomalies has been produced and a context has been associated with each candidate anomaly, all that remains is to assign anomaly scores to each candidate anomaly (and then combine the results of this computation into a solution). However, a candidate anomaly and its context are likely to be of different sizes, which can be inconvenient, since most anomaly measures only work on elements of the same size. To get around this, the context can be split into chunks of the same size as the candidate anomaly.

To accommodate for this step, the framework includes a \emph{reference filter} $F_R: [D'] \rightarrow [[D']] $, which (analogously to the evaluation filter) maps a context to a \emph{reference set} of items with which the candidate anomaly can be effectively compared. A possible reference filter for sequences is illustrated in Figure~\ref{fig:filters}. If the anomaly measure can operate on elements of different sizes, then $F_R(X) = [X]$ might be a good choice.

\section{The anomaly measure $M$}

The anomaly measure $M: ([D'], [[D']]) \rightarrow A$ takes a candidate anomaly and a reference set and produces an anomaly score in some set $A$. Realistically, letting $A = \mathbb{R}+$ ought to be sufficient in most cases. The anomaly measure essentially defines (often in unpredictable ways) what types of features will be considered anomalous, so it is vital that it is chosen appropriately.

A large number of different anomaly measures are used in the literature, and listing them all is not possible within the scope of this report. Methods are often categorised based on what type of anomaly measure they employ; for instance, a method might be classified as statistical, classifier-based, distance-based, or information-theoretic based on the anomaly measure. A good presentation of these categories is given in~\cite{chandola}.

% TODO: move the survey part of this section to the next chapter and make it about sequences only (?)
%
% A more useful way of restricting this factor is to simply consider which of the anomaly measures studied in the literature are applicable to the target application, and only allow problem formulations which involve these. For this reason, we here settle for a broad overview of commonly studied anomaly measures. We begin by discussing statistical measures, which are especially interesting since they can be theoretically justified.
%
% Statistical measures usually operate under the assumption that $C(e_i)$ has been generated from some underlying distribution or stochastic process, and associates an anomaly score with $e_i$ based on how likely it is to have been generated by the same distribution or process. Typically, statistical measures work by using some standard inference method, coupled with a few assumptions about the dataset, to estimate some simple distribution underlying the $C(e_i)$. Statistical measures have been applied to a wide range of domains, often with good results. Several books and surveys have been published on the subject of anomaly detection using statistical methods~\cite{barnett}~\cite{bakar}~\cite{leroy}~\cite{hawkins}.
%
% Statistical measures are usually classified as either parametric or non-parametric. \emph{Parametric statistical measures} assume that distribution underlying $C(e_i)$ is known, but has unknown parameters (for instance, it might be assumed that the data is $N(\mu, \sigma^2)$, where $\mu$ and $\sigma$ are unknown). \emph{Non-parametric statistical measures}, on the other hand, do not assume that the distribution is known and instead try to estimate the distribution itself by assigning weights to a set of basis functions.
%
% While non-parametric approaches are more widely applicable (the distribution of data is usually not known), the extra information provided to parametric methods mean that they converge faster and are more accurate (as long as the given assumptions are correct). Of course, parametric methods are also less widely applicable, since the underlying distribution is often not known.
%
% For datasets that can be modeled by stochastic processes, \emph{predictive models}, such as Markov chains~\cite{TODO}, hidden Markov models~\cite{TODO}, and autoregressive models~\cite{TODO} are frequently used as anomaly measures. It should be noted that most predictive models presuppose an ordering and a one-sided context.
%
% Due to the relatively high computational cost of density estimation, statistical methods are mainly used to find point anomalies. Since contextual anomalies require different training sets for each $e_i \in E$, detecting contextual anomalies requires $|E|$ density estimations (unless some clever optimisation is employed), which is typically prohibitively expensive. Since most density estimation methods scale poorly with increasing dimensionality, collective anomalies can also be prohibitively expensive to detect using statistical methods.
%
% A relatively novel and interesting class of anomaly measures is \emph{information theoretic measures}. Mainly used for symbolic datasets, these measures judge similarity by estimating how much information is shared between items or subsets of items (i.e.\ by computing measures of shared information between elements). Like statistics, information theory can be given a convenient theoretical justification.
%
% Several different measures of shared information have been suggested, such as the compressive-based dissimilarity measure (CDM)~\cite{keogh2} and (relative) conditional entropy~\cite{xiang}. While information theoretic approaches are mainly useful for symbolic data, they have shown promise for describing anomalies in continuous data when combined with a discretization and numerosity reduction~\cite{keogh2}.
%
% Anomaly measures inspired by traditional machine learning methods are also common and have been extensively researched in various contexts. For instance, classifier-based methods such as support vector machines are commonly used (TODO: add citation here). While classifiers only produce as many distinct outputs as there are classes, ensembles or weighting schemes can be utilized to produce finer grained output. Like statistical anomaly measures, classifier-based anomaly measures are relatively expensive to train, so they are typically not suitable for non-trivial contexts.
%
% Distance-based anomaly measures are also commonly used. These assign anomaly scores to elements by means of some local point density estimate. Examples include k-nearest neighbors (TODO: cite) and local outlier factor (TODO: cite). Distance-based typically measures scale well with increasing dimensionality, and are appropriate for non-trivial contexts since they are often simple to compute.

\section{The aggregration function $\Sigma$}

The aggregation function $\Sigma: ([[D']], [A]) \rightarrow [S']$ takes a list of candidate anomalies and a list of corresponding anomaly scores, and produces a ``preliminary'' solution in $[S']$. As mentioned previously, this solution is preliminary in the sense that it corresponds to a solution to a problem $P': [D'] \rightarrow [S']$. If a problem does not involve any transformation, i.e.\ if  $D' = D$ and $T_D$ is the identity transformation, then $T_S$ must also be the identity transformation, and $\Sigma$ will produce the actual solution.

How this is done depends on the nature of both $\mathcal{S}$ and the specific problem formulation. If the solution is expected to be a list of anomalous subsets, then $\Sigma$ ought to simply select a subset of the candidate anomalies, possibly along with the corresponding $A_j$. The same goes if the evaluation set contains only single element lists (i.e.\ if only point anomalies are sought).

If, on the other hand, the evaluation set contains non-singleton subsets of the input data, and the sought solution format is not a list of anomalous subsets, then the anomaly scores $A$ must be weighted together to form a list in $[S']$. Since all other solution formats can be retrieved from a list of anomaly scores, it seems reasonable to in these cases only consider aggregation functions which produce a list of anomaly scores.

Reasonably, such an aggregation function ought to be on the form
\[
    \Sigma([e_1, e_2, \dots, e_n], [a_1, a_2, \dots, a_n]) = [\sigma(\{a_i : d_1' \in e_i\}), \sigma(\{a_i : d_2' \in e_i\}), \dots, \sigma(\{a_i : d_n' \in e_i\})],
\]
where $\sigma$ is some function that maps a set of elements in $A$ to an element of $S'$.

\section{Constructing an oracle}

Assuming that each of the functions in some problem formulation $P = (T_D, F_E, C, F_R, M, \Sigma, T_S)$ can be computed, the construction of an oracle $O$ that computes the solution to $P$ is an easy task. Specifically, the following algorithm solves $P$:

\begin{algorithmic}
    \Require{Some $X \in [D]$.}
    \State{$X' \gets T_D(X)$}
    \State{$A \gets []$ \Comment{initialize anomaly scores to empty list}}
    \For{$E \in F_E(X')$} \Comment{iterate over the evaluation set}
        \State{$R_E \gets F_R(C(X', E))$ \Comment{compute a reference set}}
        \State{$append(A, M(E, R_E))$ \Comment{compute and store anomaly scores}}
    \EndFor{}
    \State{\Return{$F_S(\Sigma(F_E(X'), A))$ \Comment{aggregate scores to form anomaly vector}}}
\end{algorithmic}

The filters $F_E$ and $F_R$, the context function $C$ and the aggregation function $\Sigma$ can all be expected to be simple to compute, since they really just involve selecting subsets of or aggregating their input data; given a description of any of these functions, producing an algorithm that computes it is trivial.

On the other hand, the transformations $T_D$ and $T_S$, as well as the anomaly measure $M$ can in principle be arbitrarily difficult to compute. Consequently, special attention must be paid to ensure that these functions are dealt with properly.

\section{Constructing a problem set}

Using the framework presented in this chapter, the construction of an appropriate problem set for some specific application can be approached in a systematic manner.

First, the format of the input and solution elements $D$ and $S$ must be formally defined. The set of all possible problems then corresponds to the set of functions $P: [D] \rightarrow [S]$. Assuming that all interesting problems can be decomposed as a combination of the components $T_D$, $F_E$, $C$, $F_R$, $M$, $\Sigma$, and $T_S$, an appropriate restricted problem set can be constructed by individually restricting the components.

As has been previously pointed out, what restrictions should reasonably be placed on the filters and context function $F_E$, $F_R$ and $C$ depends entirely on the presence of structure in the input data $D \in \mathcal{D}$ which relates the $d_i \in D$. If the $d_i$ are completely unrelated, then it is reasonable to bypass these components by restricting them to only consider point anomalies. If, on the other hand, there is a relation between the $d_i \in D$, then it is reasonable to restrict these components to take this structure into account.

The aggregation function $\Sigma$ is similarly limited. For problems involving point anomalies, all of the solution formats presented above can be produced directly from the output of the anomaly measure, so $\Sigma$ is naturally constrained. For problems involving collective anomalies, anomaly scores for several candidate anomalies might have to be weighted together to produce a solution. Depending on the application, it might be appropriate to constrain the possible $\Sigma$ to a single weighting method or to a set of different weighting methods.

Restricting $T_D$ and $T_S$ is more difficult, since it is typically not known what types of transformations are appropriate for a given application. With this in mind, it might be appropriate to simply restrict $T_D$ and $T_S$ to some set of transformations which have been used in the specific application or similar applications. That way, the problem set can be said to generalise previously studied methods while remaining tractable.

Finally, the anomaly measure $M$ must be appropriately restricted. Since $M$ can not be appropriately restricted based on information about the application, it is reasonably to take a similar approach as with $R_{\mathcal{D}}$ and restrict $M$ to some set of anomaly measures which have been previously studied in conjunction with the specific application.

While restricting the components individually simplifies problem set construction, it might lead to a problem set that is overly permissive, in the sense that some combinations of components might not be useful. In this case, it is reasonable to place additional restrictions on the problem set based on combinations of components.

\section{Error measures}

Before the optimisation problem
\[
    P^*_{opt} = \argmin_{P \in \mathcal{P}^*} \sum_{T_i \in \mathcal{T}} \delta(s(T_i), O^*(P, T_i)).
\]
can be solved, the training set $\mathcal{T}$ and the error function $\delta$ must be defined. As was mentioned in the previous chapter, we assume that the set $\mathcal{T}$ of training data consists of a finite set of pairs $T_i = (D_i, S_i)$ in $([D], [S])$. This means that the training set is entirely application-specific, and discussing it in a general context is not likely to be useful.

The choice of error measure, which is formally a distance function for $\mathcal{S} = [S]$, i.e.\ $\delta: ([S], [S]) \rightarrow \mathbb{R}^+$, however, has more to do with the solution format than with the application itself. A few simple error measures which are likely to work well across most applications are now discussed.

If the solution format is a list of anomalous items, then $S$ can be seen as the binary set $\{0, 1\}$ and the solution $[s_1, s_2, \dots, s_n]$ as a binary list, where $s_i$ indicates whether or not the corresponding input element $d_i$ is considered anomalous. Reasonably the Hamming distance~\cite{TODO}, corresponding to
\[
    \delta([a_1, a_2, \dots, a_n], [b_1, b_2, \dots, b_n]) = \sum_i \delta'(a, b),
\]
where $\delta'(a, b) = 1$ if $a \neq b$ and $0$ otherwise, ought to be used for such cases.

If the solution format is a list of anomaly scores the situation becomes more complex. Assuming that $S = \mathbb{R}$, one might imagine that any ordinary real-valued distance measure, such as the Euclidean distance could be used. However, an error measure must be invariant under scalings and transformations of its input data in order to be useful, and this is not the case for the Euclidean distance. To get around this, the input data can be normalised to some interval (i.e.\ $[a_1, a_2, \dots, a_n] \mapsto [\frac{a_1 - a_{min}}{a_{max} - a_{min}}, \frac{a_2 - a_{min}}{a_{max} - a_{min}}, \dots, \frac{a_n - a_{min}}{a_{max} - a_{min}}]$, where $a_{min}$ and $a_{max}$ are the minimum and maximum elements of $a$, respectively) before the error measure is applied.

However, since providing meaningful real-valued training data can be difficult, it might be more reasonable to restrict the training data to be a set of anomalous items, and convert the solution to a binary list before applying the error measure. Reasonably, this conversion ought to be done by mapping all values above some threshold $\tau$ to $1$, and the rest of the values to $0$.

This approach gives gives rise to a number of possible error measures, corresponding to different methods of selecting $\tau$. The following measures are used later in this report:
\begin{description}
    \item[$\epsilon_{E}$,]which corresponds to setting $\tau$ such that the number of non-zero elements in both lists are equal.
    \item[$\epsilon_{F}$,]which corresponds to using the largest $\tau$ for which all non-zero elements of the training data vector are non-zero in the solution vector.
    \item[$\epsilon_{B}$,]which corresponds to using the $\tau$ that gives the smallest error value.
\end{description}

\section{Optimisation}

Once the sets $D$ and $S$ of input and solution items, the training data $\mathcal{T}$, and the error measure $\delta$ have been defined, and appropriate restrictions have been placed on $T_D$, $F_E$, $C$, $F_R$, $M$, $\Sigma$, and $T_S$, all that remains is to perform the optimisation.

TODO: continue here
