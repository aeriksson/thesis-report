\chapter{Component framework}
\label{ch:methods}

Before task \ref{task:main} can be evaluated, an understanding of the set of problems that can be derived from it must be acquired. In this chapter, the \emph{component framework} is proposed, which facilitates a structured approach to this problem.

First, section \ref{sect:framework} introduces the component framework. In section \ref{sect:related_tasks_framework}, modifying the component framework to handle various related tasks is discussed. Finally, the component framework is used in section \ref{sect:deriving_problems} to discuss and derive interesting problems from task \ref{task:main}.

\section{The framework}
\label{sect:framework}

While task \ref{task:main} has not been previously studied, several papers exist covering special cases of it (typically corresponding to certain combinations of context functions, filters and anomaly measures). A key realization in the development of the component framework was that almost all these methods could be specified using a set of independent components, used by a common algorithm to find anomalies. If the components could be formalized and abstracted to pertain to task \ref{task:main}, then an understanding of the set of problems derived from that task could be obtained through the study of the individual components.

The first step towards this goal was to consider which factors are needed to specify a problem from task \ref{task:main}. We note that, at minimum, the following factors must be specified:

\begin{description}
  \item[The context function.] Since we are dealing with contextual collective anomalies, the context must be defined.
  \item[The filters.] Filters for extracting both \emph{evaluation subsequences} (from the input sequence) and \emph{reference subsequences} (from the context) must be specified.
  \item[The anomaly measure.] Arguably the most significant factor still unspecified.
  \item[Transformations.] Which transformations, if any, are to be applied to the data before analysis.
  \item[The aggregation method.] A method for aggregating individual anomaly scores into an anomaly vector must be provided.
\end{description}

Next, these five \emph{components} were defined mathematically, in order to facilitate their standardization and use in algorithms. Specifically, the tuple of components $(\mathcal{F}_E, \mathcal{C}, \mathcal{F}_R, \mathcal{M},\mathcal{A})$ was defined, where (letting $T \in X^n$ be the sequence under consideration and $X$ be some arbitrary set, $S_T$ be the set of subsequences of $T$, and $2^{\mathbb{Z}_n}$ be the set of corresponding indices in $T$):
\begin{description}
   \item[The evaluation filter $\mathcal{F}_E$] selects which subsequences of $T$ are to be evaluated by the anomaly measure. We let $\mathcal{F}_E(T) = \{(T_1, I_1), (T_2, I_2), \dots, (T_k, I_k)\} \subseteq \mathcal{S}_T \times 2^{\mathbb{Z}_n}$ be a set of tuples of subsequences of $T$ and their corresponding indices.
   \item[The context function $\mathcal{C}$] is responsible for selecting the appropriate context for subsequences. Formally, for $(T_i, I_i) \in \mathcal{F}_e(T)$, $\mathcal{C}(I_i) \subset \mathcal{S}_T$ gives one or more subsequences constituting the context.
   \item[The reference filter $\mathcal{F}_E$] extracts subsequences from the context to produce a set of reference items.
   \item[The anomaly measure $\mathcal{M}$] assigns an anomaly score to each of the selected items with regard to their reference set: $\mathcal{M}(T_i, \mathcal{F}_r{\mathcal{C}(I_i))} \in \mathbb{R}$.
   \item[The aggregation function $\mathcal{A}$] aggregates the anomaly scores to produce an anomaly vector; i.e. if $S = \{(a_i, I_i)\}_{i \in \mathbb{Z}_k}$ is a vector of anomaly scores and indices for individual elements, then $\mathcal{A}(S) \in \mathbb{R}^{n}$.
\end{description}

Finally, an algorithm could be formulated that solves an arbitrary problem specified using this component framework, as follows:
\begin{algorithmic}
\Require A sequence $T$ and a tuple $(\mathcal{F}_E, \mathcal{C}, \mathcal{F}_R, \mathcal{M}, \mathcal{A}).$
\State $S \gets \emptyset$ \Comment{initialize anomaly score container}
\For{$(T_i, I_i) \in \mathcal{F}_E(T)$} \Comment{iterate over subsequences (and indexes) selected by filter}
    \State $C \gets \mathcal{C}(I_i)$ \Comment{acquire context}
    \State $R \gets \mathcal{F}_R(C)$ \Comment{extract reference set from context}
    \State $S \gets S \cup (\mathcal{M}(T_i, R), I_i)$ \Comment{save anomaly score with indexes of subsequence}
\EndFor
\State \Return $\mathcal{A}(S)$ \Comment{aggregate scores to form anomaly vector}
\end{algorithmic}

Restating existing problems within this framework provides several advantages. Firstly, it facilitates comparison of methods, making it easier to see their similarities and differences. Secondly, it makes it easy to overview the entire set of problems and propose novel problems based on combinations of existing components. Finally, the components can be easily interchanged, simplifying method evaluations.

The fact that a single algorithm can be used for all problems suggests the possibility of automated, accurate handling of diverse data sets. Typically, the extent to which problems can capture anomalies varies drastically between series from different sources. If this ability could be related to underlying characteristics of the series, a preliminary analysis could guide the choice of components. Since the proposed framework simplifies automated analysis and allows for problems to be applied interchangeably on the same data, it could help both in finding these underlying characteristics and in applying them to anomaly detection.

As an example of such a characteristic, in \cite{chandola3}, various anomaly measures are compared on a task derived from \ref{task:ad_splunk2} (finding anomalous real-valued sequences in a set of similar sequences) and a correlation is noted for some problems between the share of anomalies captured and the periodicity of the sequences. Based on this, investigating links between problem accuracy and characteristics of series in the frequency domain may prove fruitful.

\section{Related tasks}
\label{sect:related_tasks_framework}

It is important to note that the component framework is not limited to task \ref{task:main}, but can be modified to handle several related tasks as well. For instance:
\begin{description}
    \item[Semi-supervised] tasks are retrieved by specifying a reference sequence $R$ in addition to the evaluation sequence $T$, and letting $\mathcal{C}(T_i) \equiv R$.
    \item[Collective anomaly] detection tasks are retrieved by specifying $\mathcal{C}(T_i) = T \setminus T_i$.
    \item[Point anomaly and contextual anomaly] detection tasks can be retrieved by specifying $\mathcal{F}_{E,R}(T) = \{(t_i)\}_{t_i \in T}$ and selecting an appropriate context function.
    \item[Sequential and frequential] tasks need no modifications; they merely require different types of pre-processing.
    \item[Alternative anomaly scores] can be retrieved through trivial modifications of the aggregation method $\mathcal{A}$.
    \item[Finding anomalous sequences among a set of sequences] mainly requires simplification: $\mathcal{F}_E$, $\mathcal{C}$, $\mathcal{F}_R$ and $\mathcal{A}$ can all be ignored for such tasks. The only relevant part is the anomaly measure.
\end{description}

Based on this, the component framework could help elucidate how different sequential anomaly detection tasks are related. Furthermore, since all derived problems can be solved using a single algorithm when formulated using the component framework, the implementation and evaluation of a large class of anomaly detection tasks could be performed relatively easily. Indeed, it would be simple to modify \texttt{ad-eval} (which implements the component framework) to handle any of the above tasks.

\section{Components}
\label{sect:deriving_problems}

We now discuss possible choices of the components $\mathcal{F}_E$, $\mathcal{C}$, $\mathcal{F}_R$, $\mathcal{M}$ and $\mathcal{A}$ and the implications of these choices.

While task \ref{task:main} has not been previously studied, components based on existing methods for other tasks can be used with it. In the following discussion, several such component choices, as well as a few new components, are mentioned.

\subsection{The evaluation filter}

The evaluation filter takes a sequence and extracts subsequences from it to be used in the analysis. In the literature, \emph{sliding window} approaches are almost always taken. In these methods, evenly spaced subsequences of equal length are extracted from the series:
\[
  \mathcal{F}_E((t_1, t_2, \dots, t_n)) = \{(t_1, t_2, \dots, t_w), (t_{1 + s}, t_{2+s}, \dots, t_{w+s}), \dots, (t_{\alpha-s}, t_{\alpha-s+1}, \dots, t_{\alpha})\},
\]
where $w$ is the window length, $s$ is the displacement, and $\alpha = \lfloor \frac{n}{s} \rfloor \times w$. A constant value of $w$ is almost always used, since most anomaly measures can not deal with items of varying dimensionality. The value of $s$ is typically taken to be $1 \leq s \leq w$. The total number of elements is $\lfloor \frac{n-w}{s} \rfloor + 1$, and thus decreases with larger $s$.

Other approaches might be appropriate for anomaly measures which support variable-size elements. In \cite{keogh2}, a binary search is performed on $T$ to find discords. Note that such an approach requires a slight modification of the algorithm given above.

\subsection{The context function}
As mentioned before, several different context functions are plausible for use in sequential anomaly detection. We have previously mentioned the context (assuming that $T = (t_1, t_2, \dots, t_n)$ is the series under consideration):
\[ 
    \mathcal{C}((t_i, t_{i+1}, \dots, t_j)) = \{ (t_{i-m}, t_{i-m+1}, \dots t_{i-1}), (t_{j+1}, t_{j+2}, \dots, t_{j+n}) \},
\]
which we refer to as the \emph{symmetric local context} if $m = n$ and the \emph{asymmetric local context} otherwise; the \emph{novelty context}:
\[
    \mathcal{C}((t_i, t_{i+1}, \dots, t_j)) = \{ (t_1, t_2, \dots, t_{i-1}) \};
\]
and the \emph{trivial context} (for detecting collective or point anomalies):
\[
    \mathcal{C}((t_i, t_{i+1}, \dots, t_j)) = \{ (t_1, t_2, \dots, t_{i-1}), (t_{j+1}, t_{j+2}, \dots, t_n) \}.
\]
We also introduce the \emph{semi-supervised context}, in which $\mathcal{C}(T_i) \equiv X$ for some reference sequence $X$.

Note that the number of elements in the context will typically have a significant impact on analysis time. While the sizes of the local and semi-supervised contexts are both constant as functions of the sequence length, the novelty and trivial contexts grow linearly, which is likely to cause problems in large data sets.

\subsection{The reference filter}

The reference filter works like the evaluation filter, but extracts subsequences from the context instead of from the evaluation series. As with the evaluation filter, it is usually best to use a sliding window as the reference filter. If the anomaly measure does not require items of equal length, the reference filter can be ignored, i.e. $\mathcal{F}_R(T) = T$.

\subsection{The anomaly measure}

As is typically the case, the anomaly measure is the central part of any problem derived from task \ref{task:main}. Formally, any anomaly measure that takes an evaluation vector and a set of reference vectors as inputs and returns a real value is a candidate for $\mathcal{M}$. We now discuss a few such anomaly measures, following the classification in section \ref{sect:anomaly_measures}.

As previously mentioned, \emph{statistical measures} are attractive due to the theoretical justification they provide for anomaly detection. However, there are certain factors which render their use problematic for the task at hand. Firstly, as discussed in section \ref{sect:goals} we can not assume that the data belongs to any particular distribution, so parametric problem formulations are not appropriate. 

Since few non-parametric methods for anomaly detection in sequential data take into account either collective anomalies or context, and since naive approaches are likely to suffer from convergence issues, \footnote{For instance, the expectation maximization algorithm for Gaussian mixture models has convergence issues in high dimensions with low sample sizes.} suggesting appropriate non-parametric methods for task \ref{task:main} is difficult. However, in the case of task \ref{task:frequency}, point anomalies (for which statistical methods have been extensively researched) are more interesting than collective anomalies, and statistical methods are likely to be applicable.

\emph{Information theoretical measures} are potentially interesting for the chosen task. However, most such methods are essentially distance-based anomaly measures equipped with information theoretical distance measures. For this reason, we do not cover information theoretical anomaly measures separately from distance-based measures.

Instead, we emphasize \emph{distance-based measures}. Since they encompass a wide variety of methods, which have been shown to be successful for related problems \cite{chandola3}, we dedicate section \ref{sect:distance_mezurez} to discussing them.

\emph{Classifier-based measures} have also shown promise for related problems \cite{chandola3}. Generally, any \emph{one}-class classifier is potentially suitable for the task; see \cite{classification} for an exhaustive discussion on this topic. While one-class classifiers produce binary output, appropriate anomaly vectors can still be produced through a suitable choice of $\mathcal{A}$.

\emph{Predictive model-based measures} are also potentially interesting. They are naturally well suited for novelty detection, since they necessarily implement a novelty context. However, existing predictive model-based approaches seem to be lacking for the task at hand. In \cite{chandola3}, a leading model-based novelty detection method \cite{perkins2} which uses an autoregressive model was shown to perform poorly on a related task. While models are important as a means of reducing the computational complexity of methods, they appear appropriate to consider only as approximations to other anomaly measures.

\subsubsection{Distance measures}
\label{sect:distance_mezurez}

When dealing with distance-based problems, the choice of distance measure has a profound impact on which anomalies are detected. As with other aspects of anomaly measures, however, drawing conclusions about method efficacy through theory alone is difficult; implementing, evaluating, and comparing several measures will likely prove more instructive.

Possible interesting measures include the \emph{Euclidean distance} or the more general \emph{Minkowski distance}; measures focused on time series, such as \emph{dynamic time warping} \cite{dtw}, \emph{autocorrelation measures} \cite{autocorrelation}, or the \emph{Linear Predictive Coding cepstrum} \cite{cepstrum}; or measures developed for other types of series (accessible through transforms), such as the \emph{compression-based dissimilarity measure} \cite{keogh2}.

Additionally, the choice of distance measure affects how well methods can be optimized. Naive approaches to distance-based problems typically scale prohibitively slowly, and are not suitable for large amounts of data. Optimizations typically involve exploiting properties of the distance measure in order to reduce the number of distance computations (for instance, the commonly used k-d trees algorithm requires the distance to be a Minkowski metric).

\subsubsection{Transformations}
Anomaly measures can be combined with one or more transformations of the data extracted by the filters to speed up the analysis or to facilitate the detection of certain types of anomalies.

One commonly used data transformation is the Z-normalization transform, which modifies a sequence to exhibit zero empirical mean and unit variance. It has been argued that comparing time series is meaningless unless the Z-normalization transform is used \cite{keogh5}. However, this is doubtful, as the transform masks anomalies consisting of subsequences similar to their surrounding data but with different variance or mean.

Transformations that transform the data into some alternative domain can also be useful. For example, transformations based on the \emph{discrete Fourier transform} (DFT) and \emph{discrete wavelet transform} (DWT) \cite{fu} have shown promise for sequential anomaly detection.

Furthermore, transformations for real-valued data which produce symbolic sequences are very important, since they enable the application of symbolic approaches (studied in bioinfomatics, for instance) to real-valued sequences. While several such transformations exist, \emph{symbolic aggregate approximation} (SAX) \cite{sax} is by far the most commonly used.

\subsection{The aggregation method}
\label{sect:aggregation_method}

Once the extracted subsequences have been assigned anomaly scores, they must be aggregated before an anomaly vector can be formed. While little has been written on this subject, it is not difficult to describe reasonable aggregation methods. It seems clear that all aggregation functions, or \emph{aggregators}, ought to have the form
\[
    \mathcal{A}(\{(a_1, I_1), (a_2, I_2), \dots, (a_k, I_k) \}) = (f(\{a_i: 1 \in I_i\}), f(\{a_i: 2 \in I_i\}), \dots, f(\{a_i: n \in I_i\})),
\]
where $I_i$ are intervals, $a_i$ are assigned anomaly scores, and $f$ is some aggregator-specific function acting on a vector $\mathbb{R}^n$ and producing an aggregate score in $\mathbb{R}$. The \emph{maximum}, \emph{minimum}, \emph{median} and \emph{mean} of the values in $S$ all constitute reasonable choices of $f(S)$. We will refer to aggregators using any of these four functions as maximum, minimum, median, and mean aggregators, respectively.
