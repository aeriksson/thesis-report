\chapter{Evaluation}
\label{ch:evaluation}

As mentioned throughout this report, thorough empirical evaluations on real-world data sets are pivotal to elucidating the performance characteristics of problems, methods and tasks. In this chapter, two factors important when performing evaluations are discussed: evaluation data and error measures.

\section{Evaluation data}
\label{sect:evaluation_data}

For obvious reasons, the choice of evaluation data largely determines the outcome of the evaluation. Since the spaces of possible evaluation items for most anomaly detection methods are enormous (often $\mathbb{R}^n$ for very large $n$), there is little hope of performing exhaustive analyses. Furthermore, while only relatively small subsets of these spaces are typically interesting for any given application, discovering and delimiting these subsets is usually impossible, due to a limited understanding of the processes underlying the data.

For this reason, it is critical that a set of evaluation items is used, containing as representative a selection of relevant phenomena exhibited by data items in the application domain as possible. Since such sets can not be obtained through purely synthetic means (arguably, the existence of a model that could accurately create such sets would presuppose a level of understanding of the application domain that would render anomaly detection unnecessary), real-world data must be used.

However, simply obtaining real-world data is \emph{not} sufficient; if the anomalies in the data are not labeled, no baseline can be established, and the evaluation is useless. In the context of task \ref{task:main}, this means that, along with each sequence $S_i = (s_1, s_2, \dots, s_n)$ in the evaluation set, a reference anomaly vector $A_i = (a_1, a_2, \dots, a_n)$ must be provided, where each $a_i$ indicates how anomalous $s_i$ is.

When appropriate data is unavailable, there are two possible options. The first is to generate both series and anomalies, in order to create completely artificial sequences. The second is to take unlabeled real-world data sets and superposition artificially generated anomalies onto them.

Due to how greatly the data set affects the outcome of anomaly detection analyses, the first approach is not recommended, other than as a means of gaining a qualitative understanding of tasks, problems or methods. Even if the artificial data appears highly similar to data from the target domain, it is very unlikely that the "best" methods for such data and real-world data will be the same. The second approach, while also problematic, can at least suggest how well methods are able to recognize normal data from the target domain from anomalous data.

In the initial stages of this project, it was hoped that sufficient data would be obtained by searching the internet and existing papers on the subject, or by examining sequences of machine data with a domain expert. However, enough suitable series could not be obtained through these means, and it became clear that completely artificial data would have to be used in the evaluation. Because such data can not adequately capture the intricacies of the target domain, a qualitative evaluation of implemented methods was performed instead. Still, since a major goal of the project was to facilitate evaluations using real-world data, utilities for performing appropriate evaluations were added to \texttt{ad-eval}.
 
\section{Error measures}
\label{sect:evaluation_measures}

For any evaluation, some way of assessing the accuracy of the produced anomaly vectors is required. Formally, given a sequence $S = (s_1, s_2, \dots, s_n)$, a reference anomaly vector $R = (r_1, r_2, \dots, r_n) \in \mathbb{R}^{n}$ and a method that takes a sequence as input and produces an anomaly vector $A = (a_1, a_2, \dots, a_n) \in \mathbb{R}^{n}$, a function $\epsilon(A, R): \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}^+$ is required that assigns a similarity score, or \emph{error measure} to $A$ based on how well it approximates $R$.

Considering the difficulties involved in obtaining any kind of labeled sequences, and the fact that ranking elements based on relative degrees of anomalousness is even more difficult, it is reasonable to assume that all reference anomaly vectors are binary: $R = (r_1, r_2, \dots, r_n) \in \{0,1\}^n$. Since any method derived from task \ref{task:main} produces real-valued output, this means that error measures should work by comparing one binary and one real-valued vector; i.e. $\epsilon(A, R): \mathbb{R}^{n} \times \{0,1\}^n \rightarrow \mathbb{R}^+$.

Since constant factors do not affect how accurate an anomaly vector appears, any $\epsilon(A, A_R)$ should be invariant under uniform translations and scalings of $A$. This means that regular real-valued distance measures (such as the Euclidean distance) are not applicable. This can be avoided by normalizing $A$---by scaling and translating it such that all its elements lie in $[0,1]$---before computing the distance. For instance, the \emph{normalized Euclidean error} $\epsilon_E$, given by
\[
  \epsilon_E = \sqrt{\sum_{i=1}^n (\frac{a_i-a_{min}}{a_{max}-a_{min}} - r_i)^2}
\]
ought to constitute a reasonable choice of error measure. However, since $\epsilon_E$ is equally sensitive to the behaviour of $A$ on normal elements as on anomalous elements, how $A$ treats normal elements (which should not matter unless they have high anomaly scores) can significantly harm the accuracy of this measure, as we will see in \ref{sect:error_measure_eval}. 

This pitfall can be avoided by converting $A$ to a binary string $A_B$ as well, and using a binary distance between $A_B$ and $R$ as the error measure. Since this is equivalent to selecting a set $S_A$ of indexes of elements from $A$ and comparing it to the set $S_R$ of indexes of nonzero elements of $R$, such error measures can be seen as evaluating the performance of $A_B$ on task \ref{task:anomalous_set}, which is reasonable considering the binary nature of $R$.

What remains is to decide how $A_B$ should be constructed from $A$ and what binary distance measure to use. Since the individual elements of $S_R$ have equal weight, and since the locations of discrepancies between $A_B$ and $R$ are not important, the \emph{Hamming distance} $\delta_H$ constitutes a suitable choice of distance measure. Furthermore, $A_B$ should be constructed such that $\forall i, j \leq n: a_i < a_j \wedge i \in S_A \Rightarrow j \in S_A$, which is equivalent to setting a threshold $a_{min} \leq \tau \leq a_{max}$ and letting $A_B$ be given by 
\[
    A_B = (a_{b,1}, a_{b,2}, \dots, a_{b,n}), \quad \text{where } a_{b,i} = \left\{ 
    \begin{array}{l l}
        0 & \quad \text{if } a_i < \tau \\
        1 & \quad \text{if } a_i \geq \tau
    \end{array} \right..
\]

This gives rise to a number of possible $A_B$ equal to the number of distinct values of $A$. We here propose two error measures corresponding to different choices of $\tau$, both based on $\delta_H$. The \emph{equal support error} $\epsilon_{ES}$ corresponds to setting $\tau$ such that $|S_A| = |S_R|$, while the \emph{full support error} $\epsilon_{FS}$ corresponds to choosing the largest $\tau$ such that $\forall i: i \in S_R \Rightarrow i \in S_A$. While other choices (such as choosing the $\tau$ that minimizes $\delta_H$) might also be appropriate, only these two were selected in order to limit the scope of the discussion.

Since the three error measures proposed above are expected to produce different results, and since they have not been (to the author's knowledge) used in the context of series anomaly detection, they should be evaluated empirically before being used. Such an evaluation is performed in section \ref{sect:error_measure_eval}.
