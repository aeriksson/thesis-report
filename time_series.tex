\chapter{An application to time series}
\label{ch:time_series}

In this chapter, the framework presented in the previous chapter is applied to univariate real-valued time series, in order to derive an instance of the omptimisation problem defined in Section~\ref{sect:problem_formulation} appropriate for univariate, real-valued time series. As mentioned, the optimisation problem can be stated as
\[
    p^*_{opt} = \argmin_{p \in P^*} \sum_{(i_j, s_j) \in T} \epsilon^*(s_j, O(p, i_j)),
\]
where the objects that need to defined are the problem set $P^*$, the test data $T$, the error function $\epsilon^*$, and the oracle $O$.

In Section~\ref{sect:background}, some terminology related to time series is presented, along with a brief summary of previous research on time series anomaly detection. The construction of regular time series from irregular data is also discussed.

In Section~\ref{sect:component_framework}, a problem set $P^*$ is constructed, which generalises many of the approaches presented in the previous section. An oracle $O$ for problems in this set is then presented.

Next, in Section~\ref{sect:evaluation}, issues related to the test data $T$, as well as a few possible choices of $\epsilon^*$, are discussed.

Finally, Section~\ref{sect:implementation} details an implementation of the optimisation problem using the objects from the previous sections.

\section{Background}
\label{sect:background}

In this section, various previous research on time series is presented, using the framework from the previous chapter, along with some of the terminology and definitions which are used later in the report.

\subsection{Terminology}
\label{sect:terminology}

Since various incompatible definitions of sequences and time series are used in the literature, we begin by defining what we mean when we use these concepts.

From here on, a \emph{sequence} will be taken to mean a progression $S = (s_1, s_2, \dots)$, where $\forall i: s_i \in X$ for some set $X$. Furthermore, a \emph{time series} is taken to be any sequence $T = ((s_1, t_1), (s_2, t_2), \dots)$, where $\forall i: (s_i, t_i) \in X \times \mathbb{R}^+$ for some set $X$ and $\forall i, j: i > j \rightarrow t_i \geq t_j$. In other words, a time series is any sequence in which each item is associated with a point in time. We refer to sequences and time series as symbolic/categorical, discrete, real-valued, vector-valued et cetera based on the characteristics of $X$.

When a time series is sampled at regular intervals in time (i.e.\ $\forall i, j: t_{i+1} - t_i = t_{j+1} - t_j$), it is said to be \emph{regular}. We will treat regular time series as sequences, suppressing the $t_i$ and writing $T = (s_1, s_2, \dots)$. Henceforth all time series will be taken to be regular unless explicitly stated\footnote{As mentioned above, there is some confusion surrounding these terms in the literature. Specifically, what we would refer to as `symbolic sequences' and `regular time series' are often simply referred to as `sequences' or `time series'. In such contexts, other types of sequences (and time series) are usually ignored}.

\subsection{Previous research}
\label{sect:prev_research}

Anomaly detection in sequence is an important and active area of research, and plenty of problems related to anomaly detection in sequences have been studied over the years. In this section, a selection of previously researched problems are presented, arranged according to the framework presented in the previous chapter. More detailed surveys of anomaly detection in sequences are available in~\cite{chandola} and~\cite{TODO}.

\subsubsection{Dataset format}

Naturally, categorical, discrete, and real-valued sequences have all been studied extensively. Categorical sequences arise naturally in bioinfomatics~\cite{TODO} and intrusion detection~\cite{TODO} applications. Discrete sequences are typically encountered when monitoring the frequency of events over time. Finally, real-valued sequences are encountered in any application that involves measuring physical phenomena (such as audio, video and other sensor-based applications).

Essentially, the dataset formats in sequences can be classified into two main categories: \emph{Detecting anomalous sequences in a set of sequences}, and \emph{detecting anomalous subsequences in a long sequence}.

Detecting anomalous sequences in a set of sequences is mainly interesting when the dataset consists of large amounts of similar sequences, for instance when analyzing user command records (as in Figure~\ref{fig:calls}). Many methods dealing with such applications have been published~\cite{blender}~\cite{chan}~\cite{ye}~\cite{forrest}~\cite{sekar1}~\cite{sekar2}. More thorough reviews are found in~\cite{chandola2} and~\cite{chandola3}.

Detecting anomalous subsequences in a long sequence has not been as extensively researched, and is mainly interesting for monitoring and diagnostics applications~\cite{TODO}. TODO: write something more here 

\begin{figure}[htb]
  \begin{center}
    \leavevmode
    \includegraphics[width=\textwidth]{resources/types_of_data}
  \end{center}
  \caption{\small{Illustration of numerosity and dimensionality reduction in a conversion of a real-valued sequence to a symbolic sequence. The top frame shows a real-valued time series sampled from a random walk. The second frame shows the resulting series after a (piecewise constant) dimensionality reduction has been performed. In the third frame, the series from the second frame has been numerosity-reduced through rounding. The bottom frame shows how a conversion to a symbolic sequence might work; the elements from the third series is mapped to the set $\{a,b,c,d,e,f\}$.}}
\label{fig:types_of_data}
\end{figure}

Feature extraction is commonly performed to reduce the dimensionality of sequences, and especially of real-valued time series. In this context, the task of feature extraction can be rephrased as follows: Given a sequence $S = ( s_1, s_2, \dots, s_n )$ where $s_i \in \mathbb{R}$, find a set of basis functions $\{ \phi_1, \phi_2, \dots, \phi_m \}$ where $m < n$ that $T$ can be projected onto, such that $T$ can be recovered with little error. Many different methods for obtaining such bases have been proposed, including the discrete Fourier transform~\cite{faloutsos1}, discrete wavelet transforms~\cite{pong}~\cite{fu}, various piecewise linear and piecewise constant functions~\cite{keogh3}~\cite{geurts}, and singular value decomposition~\cite{keogh3}. An overview of different representations is provided in~\cite{fabian}.

Arguably the simplest of these bases are piecewise constant functions $(\phi_1, \phi_2, \dots, \phi_n)$:
\[
  \phi_i(t) = \left\{
    \begin{array}{l l}
      1 & \quad \text{ if } \tau_i < t < \tau_{i+1} \\
      0 & \quad \text{ otherwise.} \\
    \end{array} \right.
\]
where $(\tau_1, \tau_2, \dots \tau_n)$ is a partition of $[t_1, t_n]$.

Different piecewise constant representations have been proposed, corresponding to different partitions. The simplest of these, corresponding to a partition with constant $\tau_{i+1} - \tau_i$ is proposed in~\cite{keogh4} and~\cite{faloutsos2} and is usually referred to as \emph{piecewise aggregate approximation (PAA)}. As shown in~\cite{keogh5},~\cite{keogh3} and~\cite{faloutsos2}, PAA rivals the more sophisticated representations listed above.

Numerosity reduction is also commonly utilised in analysis of real-valued sequences. One scheme that combines numerosity and dimensionality reduction in order to give real-valued sequences into a categorical representation is \emph{symbolic aggregate approximation} (SAX)~\cite{sax}. This representation has been used to apply categorical anomaly measures to real-valued data with good results~\cite{TODO}. A simplified variant of SAX is shown in figure~\ref{fig:types_of_data}.

\begin{figure}[htb]
    \begin{center}
        %\leavevmode
        \begin{tabular}{| l | l l l l l l l l |}
            \hline
            $\mathbf{S_1}$ & login & passwd & mail & ssh & \dots & mail & web & logout \\ \hline
            $\mathbf{S_2}$ & login & passwd & mail & web & \dots & web & web & logout \\ \hline
            $\mathbf{S_3}$ & login & passwd & mail & ssh & \dots & web & web & logout \\ \hline
            $\mathbf{S_4}$ & login & passwd & web & mail & \dots & web & mail & logout \\ \hline
            $\mathbf{S_5}$ & login & passwd & login & passwd & login & passwd & \dots & logout \\\hline
        \end{tabular}
    \end{center}
    \caption{{\small Multiple symbolic sequences consisting of user commands. In this context, anomaly detection tasks involving finding individual anomalous sequences are interesting. Arguably, problems based on such tasks should capture $\mathbf{S_5}$ as an anomaly due to its divergence from the other sequences. Based on a figure in~\cite{chandola2}.}}
\label{fig:calls}
\end{figure}

In general, sequences are much easier to deal with than irregular time series. For this reason, irregular time series are commonly transformed to form regular time series, which can be treated as sequences. Formally, such transformations transform a time series $((t_1, x_1), (t_2, x_2), \ldots, (t_n, x_n))$ into some sequence $(s_1, s_2, \ldots, s_m)$.

The simplest such transformation involves simply dropping the $t_i$ to form the sequence $(x_1, x_2, \ldots, x_n)$. This is useful when only the order of items is important, as is often the case when dealing with categorical sequences. An example of such an application is shown in figure~\ref{fig:calls}.

Another common class of transformations involves estimating the (weighted) frequency of events. This is useful in many scenarios, especially in applications involving machine-generated data.

Several methods can be used to generate sequences appropriate for this task from time series, such as histograms, sliding averages, etc. These can be generalised as the following transformation:

Given a time series $T = ((s_1, t_1), (s_2, t_2), \dots)$ where $\forall i: (s_i, t_i) \in X \times \mathbb{R}$, with associated weights $w_i$ and some envelope function $e(s, t): X \times \mathbb{R} \rightarrow X$, as well as a spacing and offset $\Delta, t_0 \in \mathbb{R}^+$, a sequence $S' = ((s_{1}^{'}, \tau_1), (s_{2}^{'}, \tau_2), \dots)$ is constructed where $\tau_i = t_0 + \Delta \cdot i$ and $s_{i}^{'} = \sum_{(s_j, t_j) \in S} s_i w_i e(t_j - \tau_i)$.

The $\tau_i$ can then be discarded and the regular time series treated as a sequence. Histograms are recovered if $e(s, t) = 1$ when $|t| < \Delta/2$ and $e(x, t) = 0$ otherwise. Note that this method requires multiplication and addition to be defined for $X$, and is thus not applicable to most symbolic/categorical data. Also note that $S'$ is really just a sequence of samples of the convolution $f_S \ast e$ where $f_S = \sum_i \delta(t_i) s_i w_i$.

How this aggregation is performed has a large and often poorly understood impact on the resulting sequence. As an example, when constructing histograms, the bin width and offset have implications for the speed and accuracy of the analysis. A small bin width leads to both small features and noise being more pronounced, while a large bin width might obscure smaller features. Similarly, the offset can greatly affect the appearance of the histograms, especially if the bin width is large. There is no `optimal' way to select these parameters, and various rules of thumb are typically used~\cite{density_estimation}.

Finally, irregular or noisy data is often resampled to form regular time series. In this case, any of a number of resampling methods from the digital signal processing literature~/cite{TODO} may be employed.

\subsubsection{Training data}

As in most other machine learning applications, problems involving various degrees of supervision have been researched.

The detection of point anomalies in sequences is a well-researched problem, and has mainly been researched in conjunction with statistical anomaly measures~\cite{TODO}. Of course, detecting point anomalies in sequences is rather uninteresting since it amounts to disregarding the extra information provided by the sequence ordering.

The ordering present in sequences naturally give rise to a few interesting contexts when dealing with anomalous subsequences. A few interesting such contexts are now presented using context functions (assuming a sequence $S = (s_1, s_2, \dots, s_n)$).

As mentioned in the previous chapter, contexts can be used to generalise the concept of training data. Semi-supervised anomaly detection corresponds to the \emph{semi-supervised context}
\[
    C((s_i, s_{i+1}, \dots, s_j)) = T,
\]
where $T$ is some training set data. Many semi-supervised sequence and time series anomaly detection problems data have been studied~\cite{TODO}.

Likewise, traditional unsupervised anomaly detection for subsequences can be formulated using the \emph{trivial context}
\[
    C((s_i, s_{i+1}, \dots, s_j)) = \{ (s_1, s_2, \dots, s_{i-1}), (s_{j+1}, s_{j+2}, \dots, s_n) \}.
\]
This corresponds to finding either point anomalies or collective anomalies in a sequence, and is studied in~\cite{TODO}.

Another interesting context is the \emph{novelty context}
\[
    C((s_i, s_{i+1}, \dots, s_j)) = \{(s_1, s_2, \dots, s_{i-1})\}.
\]
This context captures the task of novelty detection in sequences, which has been researched in~\cite{TODO}.

Finally, a family of \emph{local contexts}
\[
    C_{n,m}((s_i, s_{i+1}, \dots, s_j)) = \{(s_{i-m}, s_{i-m+1}, \dots, s_{i-1}), (s_{j+1}, s_{j+2}, \ldots s_{j+n})\}
\]
may be defined, in order to handle anomalies such as the one in the last sequence of figure~\ref{fig:anomaly_types}.

It should finally be noted that contextual collective anomalies in sequences do not appear to have been researched.

For the problem of finding anomalous sequences in a set of sequences, there are fewer interesting contexts. Given a set of sequences $S = \{s_a, s_b, \dots, s_m\} = \{(s^a_1, s^a_2, \ldots, s^a_{n_a}), (s^b_1, s^b_2, \ldots, s^b_{n_b}), \ldots, (s^m_1, s^m_2, \ldots, s^m_{n_m}) \}$, the main contexts of interest are the \emph{semi-supervised context} $C(s_a) = \{t_a, t_b, \dots, t_o\}$ (corresponding to semi-supervised anomaly detection) and the \emph{trivial context} $C(s_a) = S \setminus s_a$ (corresponding to traditional unsupervised anomaly detection).

\subsubsection{Anomaly types}

Just as the sequence ordering induces natural contexts, it also naturally induces filters which produce subsequences. Since most anomaly measures defined on sequences require all selected subsequences to have the same length, sliding window approaches are by far the most commonly used. Using the concept of filters, such approaches correspond to sliding window filters, as described in Section~\ref{sect:anomaly_types}.

For anomaly measures that do not require all sequences to be of the same length, such as some information-theoretic measures, filters that select elements of non-uniform size can be useful. In~\cite{TODO}, a subdivision approach is taken in which a binary search is performed over the sequence in order to find anomalous substrings of some specific length. No training filter is required, since the anomaly measure used can compare substrings of different size.

\subsubsection{Anomaly measures}

As is typically the case, the anomaly measure is the most important aspect of any anomaly detection problem for sequences. Formally, any anomaly measure that takes an evaluation vector and a set of reference vectors as inputs and returns a real value is a candidate for $\mathcal{M}$. We now discuss a few such anomaly measures, following the classification in Section~\ref{sect:anomaly_measures}.

As previously mentioned, \emph{statistical measures} are attractive due to the theoretical justification they provide for anomaly detection. However, there are certain factors which render their use problematic for general applications. To begin with, it can generally not be assumed that the data belongs to any particular distribution, and parametric statistical measures are only appropriate in specific circumstances. Nevertheless, parametric statistical methods in sequences are an active area of research~\cite{TODO}.

Non-parametric methods are more widely applicable.

Since few non-parametric methods for anomaly detection in sequential data can take into account either collective anomalies or context, and since naive approaches are likely to suffer from convergence issues,~\footnote{For instance, the expectation maximization algorithm for Gaussian mixture models has convergence issues in high dimensions with low sample sizes~\cite{TODO}.} suggesting appropriate non-parametric methods for Task~\ref{task:main} is difficult. However, in the case of Task~\ref{task:frequency}, point anomalies (for which statistical methods have been extensively researched) are more interesting than collective anomalies, and statistical methods are likely to be applicable.

\emph{Information theoretical measures} are especially interesting for anomaly detection in categorical sequences. However, most such methods are essentially distance-based anomaly measures equipped with information theoretical distance measures. For this reason, we do not cover information theoretical anomaly measures separately from distance-based measures.

\emph{Classifier-based measures} have also shown promise, especially for the task of finding anomalous sequences in a set of sequences~\cite{chandola3}. Generally, any one-class classifier is potentially suitable for the task; see~\cite{classification} for an exhaustive discussion of this topic. While one-class classifiers produce binary output, appropriate anomaly vectors can still be produced through a suitable weighting scheme.

\emph{Predictive model-based measures} are also potentially interesting, since are naturally well suited for dealing with the novelty context. However, existing predictive model-based approaches seem to be lacking for the task at hand. In~\cite{chandola3}, a leading model-based novelty detection method~\cite{perkins2} which uses an autoregressive model was shown to perform relatively poorly.

\emph{Distance-based measures} are especially interesting, due to their flexibility and scalability. A few kNN-based anomaly measures were shown to perform very well for detecting anomalous sequences in sets of sequences in~\cite{chandola3}.

When dealing with distance-based problems, the choice of distance measure has a profound impact on which anomalies are detected. As with other aspects of anomaly measures, however, drawing conclusions about method efficacy through theory alone is difficult; implementing, evaluating, and comparing several measures is likely to be more useful.

Possible interesting measures include the \emph{Euclidean distance} or the more general \emph{Minkowski distance}; measures focused on time series, such as \emph{dynamic time warping}~\cite{dtw}, \emph{autocorrelation measures}~\cite{autocorrelation}, or the \emph{Linear Predictive Coding cepstrum}~\cite{cepstrum}; or measures developed for other types of series (accessible through transforms), such as the \emph{compression-based dissimilarity measure}~\cite{keogh2}.

Additionally, the choice of distance measure affects how well methods can be optimized. Naive approaches to distance-based problems typically scale prohibitively slowly, and are not suitable for large amounts of data. Optimizations typically involve exploiting properties of the distance measure in order to reduce the number of distance computations (for instance, the commonly used k-d tree nearest neighbor algorithm requires the distance to be a Minkowski metric).

\subsubsection{Solution format}

Since any solution format can be used with any problem, and since the solution format has little impact on the analysis (except performance-wise), it is not treated in depth here. As noted in the previous chapter, all common output formats can be produced from anomaly scores, so other output formats are interesting mainly as optimisations.

\section{Optimisation problem}

The construction of appropriate problem sets for the tasks of detecting anomalous sequences and subsequences, as well as corresponding oracles, is now discussed.

First, in Section~\ref{sect:problem_set}, a few reasonable restrictions on the problem sets, as well a suggested set of components useful in parametrising the problem sets, are introduced.

Next, in Sections~\ref{sect:subsequence_oracle} and~\ref{sect:sequence_oracle}, the components are formally defined, and oracles which operate on them are presented.

Finally, in Section~\ref{sect:components}, a few component choices are presented along with the parameters they take.

\subsection{Problem set}
\label{sect:problem_set}

As mentioned in the section~\ref{sect:problem_formulation}, the problem set must be limited in order for it to admit an oracle. The goal of this section is to present a problem set that is general enough to admit most of the problems while remaining limited enough to be useful in practice.

Based on the previous discussion, a few sensible limitations can be derived. Since other solution formats can be constructed from anomaly scores, anomaly scores are a reasonable solution format. Furthermore, since contextual collective anomalies generalise all other mentioned anomaly types (point anomalies, contextual anomalies, and collective anomalies), as well as the most commonly used forms of supervision (semi-supervised and unsupervised), limiting the problem set to this anomaly type is reasonable. While further limiting the problem set is possible, in the interest of generality, this is not done here.

In order for a problem set to be tractable, it needs to be parametrisable. To this end, it is useful to, if possible, try to define the set in terms of a few components which can be chosen independently (and then parametrised and optimised individually).

Since which factors remain to be specified depends on the task being studied, we will treat the tasks of detecting anomalous subsequences and detecting anomalous sequences in a set of sequences separately, beginning with the former.

For the task of detecting anomalous subsequences, we propose that the following set of components be used:
\begin{description}
  \item[The context.] Since we are dealing with contextual collective anomalies, the context must be defined.
  \item[The filters.] Filters for extracting sequences from the input data and context must be specified.
  \item[The anomaly measure.] The measure by which extracted sequences are judged as anomalous or normal must be specified.
  \item[Transformations.] Which transformations (including discretisation, numerosity reduction and dimensionality reduction transformations), if any, are to be applied to the extracted data items before the anomaly measure is applied.
  \item[The aggregation method.] A method for aggregating individual anomaly scores into an anomaly vector must be provided.
\end{description}

Finding anomalous sequences in a set of sequences is really a (contextual) point anomaly detection; it corresponds to finding anomalous sequences (points) in a set of sequences, either with regard to the rest of that set (unsupervised context) or to some other set of sequences (semi-supervised context). As such, there is no need for either filters or an aggregation method, and the task can be specified using the following components:
\begin{description}
  \item[The context.] Since we are dealing with contextual anomalies, the context must be defined.
  \item[The anomaly measure.] The measure by which extracted sequences are judged as anomalous or normal must be specified.
  \item[Transformations.] Which transformations (including discretisation, numerosity reduction and dimensionality reduction transformations), if any, are to be applied to the individual vectors.
\end{description}

While searching over all possible choices of any of these components is still not feasible, this is not really problematic since a vast majority of the possible choices can be expected to perform poorly. Instead, a constructive approach can be taken in which the problem set is built up from component choices which have already been shown to be effective. Indeed, searching over most or all of the previously studied choices for each individual component is likely to be feasible. If the components can chosen and optimised somewhat independently, the development of efficient optimisation heuristics should be possible.

Taking a constructive approach has the additional advantage that proposed new components (such as novel anomaly measures) can be efficiently compared to existing methods. Another interesting advantage is that the approach could facilitate research into possible connections between characteristics of the input datasets and appropriate choices of components. Typically, the extent to which problems can capture anomalies varies drastically between series from different sources. If this ability could be related to underlying characteristics of the series, a preliminary dataset analysis could be used to seed the optimisation. As an example of such a characteristic, in~\cite{chandola3}, various anomaly measures compared on the task of finding anomalous real-valued sequences in a set of similar sequences, and it is concluded that different anomaly measures are appropriate depending on the periodicity of the sequences.

\subsection{An oracle for anomalous subsequence problems}
\label{sect:subsequence_oracle}

Before the oracle can be specified, the dataset and solution formats, as well as the unspecified components specified above, must be formally defined.

For the task of detecting anomalous subsequences in a long sequence, the input dataset consists of a sequence $\mathbf{x} \in X^n$, for some set $X$. The corresponding anomaly scores are a vector $\mathbf{a} \in \mathbb{R}^n$.

The components can then be defined as (where $S(\mathbf{x})$ is the set of indexed subsequences of $\mathbf{x}$, e.g. $S((x_1, x_2)) = \{((x_1), (1)), ((x_2), (2)), ((x_1, x_2), (1, 2))\}$):
\begin{description}
    \item[The evaluation filter $F_E$]maps a sequence $\mathbf{x}$ to a subset of $S(\mathbf{x})$ to be evaluated.
    \item[The context function $C$]maps a sequence $\mathbf{x}$ and a vector of indices $\mathbf{i} \in \mathbb{Z}_n^m$ (representing a subsequence) to a subset of $S(\mathbf{x})$ (representing the context of $\mathbf{i}$).
    \item[The training filter $F_T$,]like the evaluation filter, is a function that takes a sequence $\mathbf{x}$ (from the output of $C$) to some subset of $S(\mathbf{x})$.
    \item[The transformation $T$]maps a sequence $\mathbf{x}$ to some other sequence $\mathbf{y} \in Y^l$ for some set $Y$ and some $l \in \mathbb{N}$.
    \item[The anomaly measure $M$]maps a sequence $\mathbf{y}$ and a set of sequences ${\{\mathbf{y}_i\}}_{i \in \mathbb{Z}_n}$ to an anomaly score in $\mathbb{R}$.
    \item[The aggregation function $A$]aggregates a set of subsequences and anomaly scores to produce an anomaly vector; i.e.\  if $S = {\{(a_i, \mathbf{i}_i)\}}_{i \in \mathbb{Z}_k}$ is a set of anomaly scores and indices for individual subsequences, then $A(S) \in \mathbb{R}^{n}$.
\end{description}

Given these definitions, the oracle can be formulated as follows:
\begin{algorithmic}
    \Require{A sequence $\mathbf{x}$ and a tuple $(F_E, C, F_T, T, M, A)$.}
    \State{$S \gets \emptyset$ \Comment{initialize anomaly score container}}
    \For{$(\mathbf{x}_i, \mathbf{i}_i) \in F_E(\mathbf{x})$} \Comment{iterate over subsequences (and indexes) selected by filter}
        \State{$\{\mathbf{c}_1, \mathbf{c}_2, \dots, \mathbf{c}_j\} \gets C(\mathbf{i}_i)$ \Comment{compute context}}
        \State{$\{\mathbf{t}_1, \mathbf{t}_2, \dots, \mathbf{t}_k\} \gets \bigcup_{\mathbf{c} \in \{\mathbf{c}_1, \mathbf{c}_2, \dots, \mathbf{c}_j\}} F_T(\mathbf{c})$ \Comment{extract training set from context}}
        \State{$S \gets S \cup (M(T(\mathbf{x}_i), \{T(\mathbf{t}_1), T(\mathbf{t}_2), \dots, T(\mathbf{t}_k)\}), \mathbf{i}_i)$ \Comment{save transformed anomaly score with indexes}}
    \EndFor{}
    \State{\Return{$A(S)$ \Comment{aggregate scores to form anomaly vector}}}
\end{algorithmic}

The corresponding optimisation problem is to find
\[
    \argmin_{(F_E, C, F_T, T, M, A)} \sum_i \epsilon^*(\mathbf{s}_i, O(\mathbf{x}_i, (F_E, C, F_T, T, M, A))),
\]
where $\{(\mathbf{x}_1, \mathbf{s}_1), (\mathbf{x}_2, \mathbf{s}_2), \dots, (\mathbf{x}_k, \mathbf{s}_k)\}$ is some set of labeled training data.

\subsection{An oracle for anomalous sequence problems}
\label{sect:sequence_oracle}

For the chosen task of detecting anomalous sequences in a set of sequences, the input dataset consists of a collection $(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n) \in X^n$, for some arbitrary set $X$ (typically $X$ is the set all vectors of some specific length over some set $Y$). The corresponding anomaly scores are a vector $\mathbf{a} \in \mathbb{R}^n$.

The components can then be defined as:
\begin{description}
    \item[The context function $C$] maps a sequence $(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n) \in X^n$ and an index $i \in \mathbb{Z}_n$ (representing one of the sequences) to a subset of ${\{\mathbf{x}_j\}}_{j \in \mathbb{Z}_n} \setminus \mathbf{x}_i$ (representing the context of $\mathbf{x}_k$).
    \item[The transformation $T$]maps a sequence $\mathbf{x}$ and to some other sequence $\mathbf{y} \in Y^o$ for some set $Y$ and some $o \in \mathbb{N}$.
    \item[The anomaly measure $M$] maps a sequence $\mathbf{y}$ and a set of sequences $\{\mathbf{y}_1, \mathbf{y}_2, \dots, \mathbf{y}_n\}$ (representing the context) to an anomaly score in $\mathbb{R}$.
\end{description}

Given these components, a simple oracle can be formulated as follows:
\begin{algorithmic}
    \Require{A vector of sequences $\mathcal{X} = (\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_m})$ and a tuple $(C, M)$.}
    \For{$\mathbf{x}_i \in S(\mathcal{X})$} \Comment{iterate over sequences}
        \State{$\{\mathbf{c}_1, \mathbf{c}_2, \dots, \mathbf{c}_j\} \gets C(\mathcal{X}, i)$ \Comment{compute context}}
        \State{$a_i \gets M(T(\mathbf{x}_i), \{T(\mathbf{c}_1), T(\mathbf{c}_2), \dots, T(\mathbf{c}_j)\})$ \Comment{save anomaly score}}
    \EndFor{}
    \State{\Return{$(a_1, a_2, \dots, a_n)$} \Comment{aggregate scores to form anomaly vector}}
\end{algorithmic}

The corresponding optimisation problem is to find
\[
    \argmin_{(C, T, M)} \sum_i \epsilon^*(\mathcal{S}_i, O(\mathcal{X}_i, (C, T, M))),
\]
where $\{(\mathcal{X}_1, \mathcal{S}_1), (\mathcal{X}_2, \mathcal{S}_2), \dots, (\mathcal{X}_k, \mathcal{S}_k)\}$ is some set of labeled training data.

\subsection{Components}
\label{sect:components}

Possible choices of the individual components above are now discussed. Before the optimisation problem can be implemented, a set of possible parameters must be defined for each component. With this in mind, the component choices discussed in this section include parameter descriptions.

\subsubsection{The filters}

The evaluation filter takes a sequence and extracts subsequences from it to be used in the analysis. In the literature, \emph{sliding window} approaches, as described in section~\ref{sect:prev_research} are almost always taken.

The reference filter works like the evaluation filter, but extracts subsequences from the context instead of from the evaluation series. As with the evaluation filter, there is typically little reason to use any filter other than a sliding window filter as the reference filter. If the anomaly measure does not require items of equal length, the reference filter can be ignored, i.e.\ $F_R(T) = T$.

Sliding windows are parametrised by the \emph{step length} $s \in \mathbb{N}$ and \emph{window width} $w \in \mathbb{N}$; i.e.\ any sliding window filter will have the form
\[
    F(\mathbf{x}) = \{(x_1, x_2, \dots, x_w), (x_{s+1}, x_{s+2}, \dots, x_{s+w}), \dots, (x_{n-w}, x_{n-w+1}, \dots, x_{n})\}.
\]

\subsubsection{The context function}

For the task of finding anomalous subsequences, all of the context functions described in section~\ref{sect:prev_research} are potentially interesting.

Note that the number of elements in the context will typically have a significant impact on analysis time. While the sizes of the local and semi-supervised contexts are both constant as functions of the sequence length, the novelty and trivial contexts grow linearly, which is likely to cause problems in large datasets.

For the task of detecting anomalous sequences, contexts other than the semi-supervised and unsupervised contexts are not likely to be interesting.

Different contexts admit different numbers of parameters. The trivial context and the novelty context are both parameter-free, while the local contexts admit two integral parameters.

\subsubsection{The transformation}

Anomaly measures can be combined with one or more transformations of the data extracted by the filters, in order speed up or otherwise improve the analysis.

One commonly used transformation for real-valued data is the Z-normalization transform, which modifies a sequence to exhibit zero empirical mean and unit variance.\footnote{It has been argued that comparing time series is meaningless unless the Z-normalization transform is used~\cite{keogh5}. However, this is doubtful, as the transform masks sequences that are anomalous because they are displaced or scaled relative to other sequences.}

Transformations that transform the data into some alternative domain can also be useful. For example, transformations based on the \emph{discrete Fourier transform} (DFT) and \emph{discrete wavelet transform} (DWT)~\cite{fu} have shown promise. The DFT is parameter-free, while the DWT can be said to be parametrised due to the variety of possible wavelet transforms.

Furthermore, transformations for real-valued data which produce symbolic sequences are very important, since they enable the application of symbolic approaches to real-valued sequences. While several such transformations exist, \emph{symbolic aggregate approximation} (SAX)~\cite{sax} is by far the most commonly used. This transformation takes two integral parameters which control the degrees of numerosity and dimensionality reduction, respectively.

\subsubsection{The anomaly measure}
As mentioned previously, the anomaly measure is likely the most important component. It is also the component with the largest number of interesting choices. Properly defining the required parameters for all of the anomaly measures discussed in Section~\ref{sect:prev_research} is not possible within the scope of this report, so we only discuss the most interesting anomaly measures here as indicated in~\cite{chandola3}.

Distance-based anomaly measures, and especially kNN-based anomaly measures are among these. Essentially, the kNN anomaly measure takes two parameters: a distance measure (defined on the specific type of sequences under consideration) $\delta$, and a k-value $k \in \mathbb{N}$, and given a sequence $\mathbf{x}$ and a set of context sequences $\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}$, computes the anomaly score by taking the average of the $k$ smallest $\delta(\mathbf{x}, \mathbf{x}_i)$.

Thus, the kNN anomaly measure has the two parameters $k$ and $\delta$, where $\delta$ may (for instance) be any of the distance measures discussed in Section~\ref{sect:prev_research}. Note that the distance measure, in turn, may be parametrised (for instance, the Minkowski measure has an order parameter $p \in \mathbb{R}^+$).

TODO: discuss parameters for distance measures?

Classifier-based anomaly measures are also interesting. A support vector machine-based anomaly measure is shown to perform especially well in~\cite{chandola3}. Support vector machine-based anomaly measures take a few parameters: a kernel, eventual kernel parameters, and a soft margin parameter $C$. For a more thorough discussion of support vector machines and their parameters, see~\cite{TODO}.

Note that further anomaly measures can be constructed by chopping up the input sequences $\mathbf{x}$ and $\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}$ into smaller sequences using filters before applying the distance measure, and then aggregating the result into an anomaly score using some aggregation function. This is done for the support vector machine anomaly measure in~\ref{chandola3}.

TODO: maybe mention a few other anomaly measures

\subsubsection{The aggregation method}
\label{sect:aggregation_method}

Once the extracted subsequences have been assigned anomaly scores, they must be aggregated into an anomaly vector. We here suggest a few aggregation functions, on the form
\[
    A(\{(\mathbf{a}_1, \mathbf{i}_1), (\mathbf{a}_2, \mathbf{i}_2), \dots, (\mathbf{a}_k, \mathbf{i}_k) \}) = (f(\{\mathbf{a}_i: 1 \in \mathbf{i}_i\}), f(\{\mathbf{a}_i: 2 \in \mathbf{i}_i\}), \dots, f(\{\mathbf{a}_i: n \in \mathbf{i}_i\})),
\]
where $I_i$ are intervals, $a_i$ are assigned anomaly scores, and $f$ is some aggregation method-specific function that produced a real-valued aggregate score from a set of real-valued element-wise scores. The \emph{maximum}, \emph{minimum}, \emph{median} and \emph{mean} of the values in $S$ all constitute reasonable choices of $f(S)$. Aggregation functions using any of these four functions will be referred to as maximum, minimum, median, and mean aggregators, respectively. Each of these is parameter-free.

\section{Evaluation}
\label{ch:evaluation}

Two aspects of the optimisation problem which have have not yet been discussed are the training data and error measure. Since these aspects essentially define the objective function used to guide the optimisation, it is important that they are chosen appropriately. In this section, they are discussed in detail.

\subsection{Training data}
\label{sect:evaluation_data}

As has been previously mentioned, while the optimisation should ideally operate on an expected error over some stream of data, this is typically infeasible. In practice, the optimisation must be performed over some pre-selected set of training data. If the optimisation is to be truly useful, this training data must also somehow be \emph{labeled} (otherwise, the optimisation would hinge on some built-in assumption of what constitutes an anomaly and what does not, which is problematic concerning the subjective nature of anomaly detection in applications).

For the task of finding anomalous subsequences, this means that the training data should ideally consist of a set of sequences and corresponding label vectors $\{(\mathbf{x}_1, \mathbf{s}_1), (\mathbf{x}_2, \mathbf{s}_2), \dots, (\mathbf{x}_k, \mathbf{s}_k)\}$, where $\mathbf{x}_i \in X^{n_i}$, $\mathbf{s}_i \in \mathbb{R}^{n_i}$.

However, obtaining real-valued anomaly scores that are meaningful objective is typically not possible (again, due to the subjective nature of the subject). A more realistic scenario would be for the $\mathbf{s}_i$ to instead be binary vectors, i.e.\ $\mathbf{s}_i \in {\{0,1\}}^n$. Such vectors can typically readily be constructed for any dataset by a domain expert.

If labeled training data can not be provided, an interesting alternative is to superimpose artificial anomalies onto a set of unlabeled training sequences. With this approach, the objective function measures how well problem formulations discern regular data from the application from similar data that contains (specific, artificial) anomalies. Of course, unlike with true labeled training data, there is no guarantee that the superimposed anomalies are relevant to the application, so the optimisation might still not lead to an accurate problem formulation.

\subsection{Error measures}
\label{sect:evaluation_measures}

As mentioned in section~\ref{sect:problem_formulation}, an error measure must be defined, which given a sequence and an anomaly vector judges how accurately the anomaly vector captures anomalies in the sequence. Ideally, this error measure should consist of an objective and infallible domain expert, who carefully judges each sequence $\mathbf{x}_i$ and anomaly vector $\mathbf{a}_i \in \mathbb{R}^n$. Failing this, the next best thing would be to have a reference anomaly vector $\mathbf{r}_i$, with which $\mathbf{a}_i$ can be compared.

As mentioned above, realistically such reference anomaly vectors would have to be binary. Since we have made the assumption that problems have real-valued vectors as solutions, our error measures thus have to operate on one binary and one real-valued vector, i.e.\ they must be functions $\epsilon: \mathbb{R}^n \times {\{0, 1\}}^n \rightarrow {\mathbb{R}}^+$. To our knowledge, such error methods have not been previously studied, so we now suggest a few such error measures.

Since constant factors do not affect how accurate an anomaly vector appears, any error measure $\epsilon(\mathbf{a}, \mathbf{r})$ should be invariant under uniform translations and scalings of $\mathbf{a}$. This means that regular real-valued distance measures (such as the Euclidean distance) are not applicable. This can be avoided by normalizing $\mathbf{a}$---by scaling and translating it such that all its elements lie in $[0,1]$---before computing the distance. For instance, we can define the \emph{normalized Euclidean error} $\epsilon_E$, given by
\[
    \epsilon_E = \sqrt{\sum_{i=1}^n {(\frac{a_i-a_{\min}}{a_{\max}-a_{\min}} - r_i)}^2},
\]
which ought to constitute a reasonable choice of error measure. However, since $\epsilon_E$ is as sensitive to the accuracy of $\mathbf{a}$ on normal elements as on anomalous elements, comparatively high or low values of $\mathbf{a}$ for non-anomalous elements may distort this error measure.

This pitfall can be avoided by converting $\mathbf{a}$ to a binary string $\mathbf{a}_B \in {\{0, 1\}}^n$ as well, and using a binary distance between $\mathbf{a}_B$ and $\mathbf{r}$ as the error measure. Since this is equivalent to selecting a set $S_\mathbf{a}$ of indexes of elements from $\mathbf{a}$ and comparing it to the set $S_\mathbf{r}$ of indexes of nonzero elements of $\mathbf{r}$, this can be seen as converting the anomaly score to a set of anomalous elements, and comparing this set to a reference set.

What remains is to decide how $\mathbf{a}_B$ should be constructed from $\mathbf{a}$ and what binary distance measure to use. If all elements in the sequence are to be assigned equal importance, the natural choice of distance measure is the \emph{Hamming distance} $\delta_H$~\cite{TODO}. Similarly, for $\mathbf{a}_B$ it ought to hold that $\forall i, j \leq n: a_i < a_j \wedge i \in S_A \Rightarrow j \in S_\mathbf{a}$, which is equivalent to setting a threshold $a_{\min} \leq \tau \leq a_{\max}$ and letting $\mathbf{a}_B$ be given by:
\[
    \mathbf{a}_B = (a_{B,1}, a_{B,2}, \dots, a_{B,n}), \quad \text{where } a_{B,i} = \left\{
    \begin{array}{l l}
        0 & \quad \text{if } a_i < \tau \\
        1 & \quad \text{if } a_i \geq \tau
    \end{array} \right..
\]

This gives rise to a number of possible error measures, based on how the threshold value $\tau$ is set. We define the following measures:
\begin{description}
    \item[The equal support error $\epsilon_{ES}$,]which corresponds to setting $\tau$ such that $|S_\mathbf{a}| = |S_\mathbf{r}|$.
    \item[The full support error $\epsilon_{FS}$,]which corresponds to using the largest $\tau$ for which $\forall i: i \in S_\mathbf{r} \Rightarrow i \in S_\mathbf{a}$.
    \item[The optimal support error $\epsilon_{BS}$,]which corresponds to using the $\tau$ that gives the smallest error value.
\end{description}

Since the error measures discussed above have not previously been studied in the context of series anomaly detection, an empirical evaluation of their performance is performed in Section~\ref{sect:error_measure_eval}.

\section{Implementation}
\label{ch:implementation}

As stated in the abstract, the development of a software framework for the evaluation of anomaly detection methods, called \texttt{ad-eval} and available at \url{http://github.com/aeriksson/ad-eval}, was a significant part of the project. In this chapter, the design, development process, and features of \texttt{ad-eval} are discussed.

Essentially, \texttt{ad-eval} consists of three separate parts: a library implementing the component framework, a comprehensive set of utilities for evaluating the performance of methods and problem, and an executable leveraging the library. The entire project is written in Python.

In this chapter, \texttt{ad-eval} is described in detail. The three parts are described in Sections~\ref{sect:implemented_problems},~\ref{sect:evaluation_package}, and~\ref{sect:executable}, and some of the design choices made in the development of \texttt{ad-eval} are discussed in Section~\ref{sect:design}.

\subsection{Implemented components}
\label{sect:implemented_problems}

The anomaly detection part of \texttt{ad-eval} (given the Python package name \texttt{anomaly\_detection}) is a faithful implementation of the component framework, including the algorithm proposed in Section~\ref{sect:framework}. In order to preserve the modular nature of this framework, the individual components are implemented as autonomous modules, described below.

As there are no real alternatives found in the literature, the sliding window filter is the only implemented evaluation filter. Since the optimal window width $w$ and step length $s$ depend on the application, both of these parameters were left to be user-specified.

Since new new context functions can be implemented relatively easily, and since they have a relatively major impact on the analysis, all previously discussed contexts (specifically, the asymmetric and symmetric local contexts, the novelty context, the trivial context, and the semi-supervised context) were implemented.

As reference filters, a sliding window filter and the identity filter $F_E(X)=X$ were implemented, the latter because it is a better fit for dimension-independent distance measures.

Due to the limited scope of the project, the only implemented anomaly measures (henceforth referred to as \emph{evaluators}) were a variant of k-Nearest Neighbors (kNN), in which the distance to the $k$'th nearest element is considered, and a one-class support vector machine (SVM). For the kNN evaluator, the Euclidean distance as well as the compression-based dissimilarity measure~\cite{keogh2} and dynamic time warping~\cite{dtw} distances were made available. Furthermore, the symbolic aggregate approximation (SAX) and discrete Fourier transform (DFT) transformations were added as an optional pre-anomaly measure transformations. All parameters of the evaluators, distances, and transformations were left for the user to specify.

Finally, the mean, median, maximum, and minimum aggregators were implemented.

\subsection{Evaluation utilities}
\label{sect:evaluation_package}

The set of possible interesting tests that could be run on problems derived from Task~\ref{task:main} is considerable. A large number of component combinations can be used; most components have many possible parameter values, and it is important to assess how these affect the results; and a large set of methods with various optimizations and approximations can be proposed. For all of these choices, it is important that accuracy and performance are properly evaluated.

However, the performance of methods is highly dependent on the characteristics of the datasets to which they are applied. As mentioned in Section~\ref{sect:evaluation_data}, there is no hope to exhaustively cover the space of possible evaluation sets. Instead, sample data from the target application domain must be obtained before any tests are performed. Furthermore, obtaining adequate labeled test data is often difficult, and artificial anomaly generation must be considered as an option.

With this in mind, it was decided that an evaluation framework should be added to \texttt{ad-eval} to help facilitate the implementation, standardization, and duplication of accuracy and performance evaluations. Due to the variety of interesting tests highlighted above, an approach focused on the provision of tools that assist in scripting custom tests was deemed preferable to one focused on the construction of a single configurable testing program. To this end, utilities were developed for:

\begin{itemize}
    \item Saving and loading time series to/from file, with or without reference anomaly vectors.
    \item Pseudorandomly selecting and manipularing subsequences of series (e.g.\ for adding anomalies).
    \item Facilitating the testing of large numbers of parameter values.
    \item Generating various types of artificial anomalies and superpositioning them onto sequences.
    \item Calculating the anomaly vector distance measures $\epsilon_E$, $\epsilon_{ES}$ and $\epsilon_{FS}$ discussed in Section~\ref{sect:evaluation_measures}.
    \item Facilitating the automated comparison of several problems and methods on individual datasets.
    \item Automating the collection of performance metrics.
    \item Reporting results.
    \item Generating various types of custom plots from results.
\end{itemize}

With these tools in place, it is simple to write scripts that, for instance, generate large amounts of similar series containing random anomalies and evaluate the performance of several problems on this data in various ways. 

The tools were included in \texttt{ad-eval} as a separate Python module (called \texttt{eval\_utils} and located in the \texttt{evaluation} directory of the repository). This module was used to perform all tests in the evaluation phase of this project.

\subsection{Executable}
\label{sect:executable}

To enable the stand-alone use of the anomaly\_detection package, an executable was added to the \texttt{ad-eval} repository (called \texttt{anomaly\_detector} and located in the \texttt{bin} directory of the repository). This executable is used through a command-line interface and a \texttt{key:value} style configuration file, can perform supervised or semi-supervised anomaly detection on sequences from files or standard input, and can use any of the components implemented in \texttt{anomaly\_detection}.

To avoid having to modify this program every time a component in \texttt{anomaly\_detection} was changed, the executable was made unaware of all internal details of that package. Consequently, a command-line interface capable of configuring the components could not be implemented; instead, a configuration file parser is used to read and pass the component configuration to \texttt{anomaly\_detection}.

\subsection{Design}
\label{sect:design}

The development of \texttt{ad-eval} began at the start of the project. Initially, development efforts focused on the implementation of a few optimized methods found in the literature, to produce a Splunk app. However, as the project progressed and the issues discussed in Section~\ref{sect:adb} (that most methods were targeted at subtly different tasks, and that due to lacking evaluations, assessing which methods are really the `best' is not possible), this approach was recognized as fruitless, and abandoned.

Development then shifted towards an implementation of the component framework, with the goals of maximizing the ease of implementing and evaluating large amounts of components.

Consequently, modularity was a major focus throughout the development process, achieved through various means. As mentioned previously, the individual components were separated into independednt modules. Additionally, the evaluation utilities and the executable were decoupled from the component framework implementation, interfacing with it through only two method calls. Finally, the decision was made to distribute the configuration of the component framework implementation, letting each component handle its own configuration and making the rest of the package configuration-agnostic.

It was a natural choice to write the entire implementation in Python, for several reasons. First, Python is well suited for small, flexible projects such as \texttt{ad-eval}, thanks to its simplicity and flexibility. Furthermore, a number of great libraries for data mining and machine learning exist for Python, which were used to accelerate the development. Finally, if \texttt{ad-eval} becomes adopted for real-world use, Python's good C integration could be leveraged to write optimized code. 

Finally, the evaluation utilities were designed with ease of use and flexibility in mind. For instance, while classes facilitating test data generation, evaluation, and reporting are provided, their use is optional. As a result, evaluation scripts could be short and simple---the scripts used in the next chapter are all 30 to 70 lines long---without sacrificing flexibility.

% \section{Important tasks}
% \label{ch:problems}
% 
% Having discussed principal factors and transformations in depth, we are finally in a position to reason about which tasks might be the most relevant to anomaly detection in the target domain.
% 
% This chapter begins in Section~\ref{sect:goals} with a presentation and discussion of objectives in designing anomaly detection methods for machine data.
% 
% In Section~\ref{sect:relevant_tasks}, the principal factors and high level tasks presented in Chapter~\ref{ch:framework} are considered, from the perspective of their applicability to temporal machine data and how well they match the chosen objectives.
% 
% Finally, the chapter is concluded in Section~\ref{sect:suggested_tasks} with a summary of the anomaly detection tasks most relevant to the target domain.
% 
% \subsection{Objectives}
% \label{sect:goals}
% 
% A set of objectives for anomaly detection methods dealing with temporal machine data are now presented, which will be used to reason about which tasks and methods are appropriate for such data.
% 
% The most fundamental objective is \emph{accuracy}. Methods must detect anomalies as accurately as possible. For obvious reasons, a high rate of false negatives will render the analysis useless, while a high false positive rate will require a high degree of manual analysis of the results---something that is is problematic since such an analysis might be hard to perform and can be sensitive to bias on part of the analyst. This objective can be stated as follows:
% 
%   \emph{Methods should be as accurate as possible. They should have low misclassification rates.}
% 
% Another important property is \emph{generality}. Many methods are accurate only on few sets of data. For instance, parametric statistical methods typically rely on the assumption that the data is sampled from a specific distribution. When this assumption fails, the results are usually poor. As the target domain is arbitrary sets of machine data, methods should require as few assumptions as possible. This objective is summarized as follows:
% 
%   \emph{Methods should perform well on as many datasets as possible. They should make as few assumptions as possible about the data.}
% 
% A related objective is that methods should have as \emph{few parameters} as possible. The goal is to provide tools that facilitate the monitoring and analysis of very large sets of data, and methods that require a lot of parameter tweaking in order to work properly will likely hamper this effort, since appropriately configuring such methods requires knowledge of the intricacies of both methods and data. Furthermore, methods with many parameters can easily lead to misinterpreted results~\cite{keogh2}. Finally, the amount of `peripheral' configuration (selection of training data, output format, etc.) should be kept to a minimum. This leads to the following objective:
% 
%   \emph{Methods should require as few parameters as possible. Configuration should be kept to a minimum.}
% 
% Finally, it is vital that methods be \emph{scalable}. Since the hope is to provide methods that can be useful in monitoring and diagnosis scenarios, methods that can handle large sets of data are required. Specifically, this means that methods should have low complexity in terms of both time and memory, and be able to give accurate results even for large datasets. Furthermore, it is preferable for methods can be distributable and work in a streaming fashion, e.g.\ not require random disk accesses or multiple passes over the data. This objective can be stated as follows:
% 
%   \emph{Methods should be scalable: they should be able to handle very large amounts of data without long computation times or disk issues.}
% 
% \subsection{High-level tasks}
% \label{sect:relevant_tasks}
% 
% Which high level tasks are relevant for temporal machine data is now discussed. The principal factors are treated sequentially, according to the order they were encountered in Chapter~\ref{ch:framework}.
% 
% This Section is also based in large part on the discussions in the previous Section and those of Chapter~\ref{ch:transformations}. 
% 
% \subsubsection{Data format}
% \label{sect:relevant_data_format}
% 
% As per the discussion in Sections~\ref{sect:splunk_techniques} and~\ref{sect:series_mining}, it is appropriate to focus on extracting sequences from indexes for use with Tasks~\ref{task:ad_splunk} or~\ref{task:ad_splunk2}.
% 
% What remains is to settle on a specific presentation of these sequences. While all three aggregation tasks presented in Section~\ref{sect:aggregation} are potentially useful for temporal machine data, their relative utility to the stated objectives can still be judged. We will now discuss each of the three tasks in turn, and estimate their utility.
% 
% First, consider Task~\ref{task:sequential_anomaly_detection} (sequential anomaly detection). While interesting in certain contexts (such as system call or web session traces), it is not likely to be able to capture a majority of interesting sequences in temporal machine datasets, since it is not appropriate for non-categorical sequences. While there are certainly interesting categorical sequences that can be analyzed in many temporal machine datasets, it can be expected that these constitute a low share of the total number of interesting sequences in Splunk applications. For this reason, it ought to be argued that this task, while interesting in some cases, should have lower priority than the other two aggregation tasks.
% 
% Task~\ref{task:frequential_anomaly_detection} (frequential anomaly detection) is more interesting. Since it enables anomaly detection to be applied to the frequency of any recurring event or value, it should be applicable to a wider range of datasets. However, it is limited in the sense that it can not accurately capture continuous quantities or other types of quantities that vary over time. This limits its applicability to temporal machine datasets, since a large share of the interesting quantities in such data (at least in Splunk applications) vary over time.
% 
% In such cases, Task~\ref{task:continuous_anomaly_detection} is more appropriate, as it allows for correct estimates of evolving quantities. As pointed out in Section~\ref{sect:aggregation}, the aggregation transformations involved in the Tasks~\ref{task:frequential_anomaly_detection} and~\ref{task:continuous_anomaly_detection} are very similar, and implementing them both should not be difficult. However, they are likely to require different anomaly measures; to limit the scope of the project, it was decided that the focus would be placed on the latter.
% 
% The next issue to settle is whether uni- or multivariate sequences should be the focus. As discussed in Section~\ref{sect:unimultivariate} multivariate sequences are more difficult to analyze than univariate sequences, and can often be broken down into univariate sequences without significant losses in analysis power. Therefore, it is reasonable to (at least initially) focus solely on univariate sequences.
% 
% Finally, it must be decided which of the Tasks~\ref{task:ad_splunk} (detecting anomalous subsequences) and~\ref{task:ad_splunk} (detecting anomalous sequences in a set of sequences) to focus on. While the latter task is interesting and occurs naturally in many contexts, e.g.\ system call traces (figure~\ref{fig:calls}), it is more reasonable to focus on the former since temporal machine datasets typically feature long and largely dissimilar sequences.
% 
% Based on the above, the following two tasks ought to be promoted as important, with the latter being preferential as a first step:
% \begin{task}
% \label{task:frequency}
%   Detect anomalous subsequences in real-valued univariate sequences containing the frequency of some event over time.
% \end{task}
% \begin{task}
% \label{task:continuous}
%   Detect anomalous subsequences in real-valued univariate sequences sampled from a continuous process.
% \end{task}
% 
% \subsubsection{Reference data}
% 
% One of the four objectives stated in Section~\ref{sect:goals} is that methods should be as generally applicable and require as little configuration as possible. Obviously, the reference data tasks presented in Section~\ref{sect:reference_data} meet these demands to varying degrees. Since unsupervised anomaly detection (task~\ref{task:unsupervised_anomaly_detection}) does not require the user to provide reference data, it is superior to supervised and semi-supervised anomaly detection in this regard.
% 
% However, it is not clear whether unsupervised methods can scale adequately; there seems to be a trade-off between the amount of required supervision and the scalability of algorithms. Most naive methods scale as $O(f(R \cdot N))$, where $f(n) \in \Omega(n)$, $R$ is the number of reference items, and $N$ is the number of items to be evaluated. When performing semi-supervised anomaly detection, it is usually the case that $R < N$. In unsupervised anomaly detection, however, $N = R$, and so methods scale as $O(f(N^2))$. This is problematic, and some form of pruning is required to allow large-scale analysis. One form of pruning that should perform relatively well is the use of some form of constant-size context (e.g.\@, the local context). Nevertheless, it was decided that unsupervised methods should be the focus of this project.
% 
% \subsubsection{Anomaly types}
% 
% The choice of both anomaly types and anomaly measure together define which anomalies will be captured by problem formulations.
% 
% Reviewing Section~\ref{sect:anomaly_types}, and especially Figure~\ref{fig:anomaly_types}, can help when reasoning about anomaly types. \emph{Point anomalies} are obviously not appropriate, as they can not capture most anomalies in continuous sequences (to see this, consider that none of the aberrations in the second, third, or fourth panels of Figure~\ref{fig:anomaly_types} are point anomalies).
% 
% \emph{Contextual point anomalies} are more relevant, especially in the context of Task~\ref{task:frequency}. Here, each individual data point in the resulting data series encodes the frequency of some event at some specific time. Assuming that these frequencies among points sharing context are independent, contextual anomaly methods should be sufficient to find temporary anomalous frequencies.
% 
% However, contextual point anomalies do not occur in continuous sequences, such as those encountered in Task~\ref{task:continuous} or in the third and fourth panels of Figure~\ref{fig:anomaly_types}.
% 
% \emph{Collective anomalies}, on the other hand, are very interesting in continuous time series, since they can cover all anomalies that arise in such series, as long as the anomalies are global (i.e.\@, they do not constitute normal behaviour elsewhere in the series).
% 
% However, non-global anomalies do occur, especially in long time series such as those encountered in monitoring applications. With this in mind, it is reasonable to conclude that being able to take \emph{contextual collective anomalies} into account is highly desirable. Further, based on the fact that all the previous anomaly types are special cases of contextual collective anomalies, it can be concluded that finding this type of anomaly should be sufficient\footnote{Note that while this is certainly true, it does not mean that methods optimized for finding contextual anomalies (for instance) would not be better at that specific task than methods for finding contextual collective anomalies. A thorough evaluation of all methods---for both tasks---would have to be performed to answer this question}.
% 
% As with any task involving contextual anomalies, a \emph{context function} must be specified before this method can be useful. Natural choices of sequence context functions include the \emph{symmetric local context}, which considers $k$ items before and after the subsequence in question, and the \emph{asymmetric local context}, which considers $k$ items before and $j$ items after the subsequence.
% 
% Since which context functions are appropriate depends heavily on the dataset, context functions should be evaluated empirically.
% 
% \subsubsection{Anomaly measures}
% 
% As stressed in Section~\ref{sect:anomaly_measures}, the anomaly measure is probably the most important part of any anomaly detection problem. However, due to the unpredictability of how they affect results, there is not much that can said about which anomaly measures are the most appropriate for the target domain. For this reason, we will leave this factor unspecified, and instead focus on evaluating as many different choices of it as possible.
% 
% \subsubsection{Output format}
% 
% In one sense, the choice of output format is cosmetic, as it depends entirely on the context in which the results should be presented. 
% 
% However, as noted in Section~\ref{sect:output_format}, most other output format tasks can be reduced to Task~\ref{task:anomaly_scores} (anomaly scores). In order to reduce the risk of focusing on an inappropriate output format, this task was chosen as a focus.
% 
% \subsubsection{High-level tasks related to anomaly detection}
% 
% Most of these tasks are not immediately useful for anomaly detection in Splunk. For instance, signal recovery is a complex task that is unlikely to be required for most machine data and for which using specialized software is more appropriate.
% 
% The task of \emph{novelty detection} (task~\ref{task:novelty_detection}), however, has potential in both monitoring and diagnosis applications. In the former case, anomalies that occur periodically will be diagnosed every time they occur, possibly leading to an unnecessarily large number of discovered anomalies. In the latter application, anomalies that occur periodically will be completely ignored (i.e.\ treated as normal if they occur many times), which can be problematic. Novelty detection methods avoid these problems.
% 
% Note that (as stated in Section~\ref{sect:related_tasks}) novelty detection is really just contextual (or contextual collective) anomaly detection with a specific context function. For this reason, we do not treat novelty detection on sequences as a task separate from anomaly detection, but as a specific choice of context, which we refer to as the \emph{novelty context}. Finally, the problems in monitoring and diagnosis applications described above can also be mitigated by other context functions.
% 
% \subsection{Suggested tasks}
% \label{sect:suggested_tasks}
% 
% The tasks selected in the previous Section into complete tasks in which all principal factors are fixed.
% 
% Based on the discussion in Section~\ref{sect:relevant_data_format}, the following task ought to be given the highest priority:
% \begin{task}
% \label{task:main}
%   Detect contextual collective anomalies in, and assign anomaly scores to, the elements of a single univariate real-valued sequence sampled from a continuous process.
% \end{task}
% 
% The remainder of this report will focus on this task. In the next chapter, the component framework, which simplifies the specification of remaining factors for this task, is proposed. Both ad-eval and the evaluation performed as part of this project focus on this task as well.
% 
% Of course, there are other, similar tasks that could also be very important to machine data anomaly detection. A set of tasks which naturally complement Task~\ref{task:main} in monitoring and diagnosing of sets of machine data are now presented.:
% 
% To begin with, as discussed in Section~\ref{sect:relevant_data_format}, frequential anomaly detection is very similar to continuous anomaly detection and is likely to be relevant to Splunk applications. We present this as the following task:
% 
% \begin{task}
%   Detect contextual collective anomalies in, and assign anomaly scores to the elements of, a single univariate real-valued sequence containing the frequency of some event over time.
% \end{task}
% 
% Furthermore, symbolic time series are potentially interesting in several applications, suggesting the following task (which is purposely vague, since it requires a different choice of principal factors):
% 
% \begin{task}
%   Detect anomalous subsequences in long symbolic sequences.
% \end{task}
% 
% Of course, applying Task~\ref{task:ad_splunk2} (detecting anomalous sequences in a set of sequences) to the above tasks can also be interesting. The resulting tasks can be briefly stated as follows:
% 
% \begin{task}
%   Detect anomalous sequences in a set of univariate real-valued time series sampled from continuous processes.
% \end{task}
% 
% \begin{task}
%   Detect anomalies sequences in a set of univariate real-valued time series containing the frequency of some event over time.
% \end{task}
% 
% \begin{task}
%   Detect anomalous sequences in a set of similar symbolic sequences.
% \end{task}
% 
% Since all of the tasks presented in this section are complementary, and since they share many similarities, they should ideally be implemented and evaluated in tandem. However, due to time and space constraints, it was not possible to study most of them in sufficient depth this project (although the component framework and \texttt{ad-eval} can both support these tasks).
