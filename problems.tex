\chapter{Important tasks}
\label{ch:problems}

Having discussed principal factors and transformations in depth, we are finally in a position to reason about which tasks might be the most relevant to anomaly detection in the target domain.

This chapter begins in Section~\ref{sect:goals} with a presentation and discussion of objectives in designing anomaly detection methods for machine data.

In Section~\ref{sect:relevant_tasks}, the principal factors and high level tasks presented in Chapter~\ref{ch:framework} are considered, from the perspective of their applicability to temporal machine data and how well they match the chosen objectives.

Finally, the chapter is concluded in Section~\ref{sect:suggested_tasks} with a summary of the anomaly detection tasks most relevant to the target domain.

\section{Objectives}
\label{sect:goals}

A set of objectives for anomaly detection methods dealing with temporal machine data are now presented, which will be used to reason about which tasks and methods are appropriate for such data.

The most fundamental objective is \emph{accuracy}. Methods must detect anomalies as accurately as possible. For obvious reasons, a high rate of false negatives will render the analysis useless, while a high false positive rate will require a high degree of manual analysis of the results---something that is is problematic since such an analysis might be hard to perform and can be sensitive to bias on part of the analyst. This objective can be stated as follows:

  \emph{Methods should be as accurate as possible. They should have low misclassification rates.}

Another important property is \emph{generality}. Many methods are accurate only on few sets of data. For instance, parametric statistical methods typically rely on the assumption that the data is sampled from a specific distribution. When this assumption fails, the results are usually poor. As the target domain is arbitrary sets of machine data, methods should require as few assumptions as possible. This objective is summarized as follows:

  \emph{Methods should perform well on as many data sets as possible. They should make as few assumptions as possible about the data.}

A related objective is that methods should have as \emph{few parameters} as possible. The goal is to provide tools that facilitate the monitoring and analysis of very large sets of data, and methods that require a lot of parameter tweaking in order to work properly will likely hamper this effort, since appropriately configuring such methods requires knowledge of the intricacies of both methods and data. Furthermore, methods with many parameters can easily lead to misinterpreted results~\cite{keogh2}. Finally, the amount of `peripheral' configuration (selection of training data, output format, etc.) should be kept to a minimum. This leads to the following objective:

  \emph{Methods should require as few parameters as possible. Configuration should be kept to a minimum.}

Finally, it is vital that methods be \emph{scalable}. Since the hope is to provide methods that can be useful in monitoring and diagnosis scenarios, methods that can handle large sets of data are required. Specifically, this means that methods should have low complexity in terms of both time and memory, and be able to give accurate results even for large data sets. Furthermore, it is preferable for methods can be distributable and work in a streaming fashion, e.g.\ not require random disk accesses or multiple passes over the data. This objective can be stated as follows:

  \emph{Methods should be scalable: they should be able to handle very large amounts of data without long computation times or disk issues.}

\section{High-level tasks}
\label{sect:relevant_tasks}

Which high level tasks are relevant for temporal machine data is now discussed. The principal factors are treated sequentially, according to the order they were encountered in Chapter~\ref{ch:framework}.

This Section is also based in large part on the discussions in the previous Section and those of Chapter~\ref{ch:transformations}. 

\subsection{Data format}
\label{sect:relevant_data_format}

As per the discussion in Sections~\ref{sect:splunk_techniques} and~\ref{sect:series_mining}, it is appropriate to focus on extracting sequences from indexes for use with Tasks~\ref{task:ad_splunk} or~\ref{task:ad_splunk2}.

What remains is to settle on a specific presentation of these sequences. While all three aggregation tasks presented in Section~\ref{sect:aggregation} are potentially useful for temporal machine data, their relative utility to the stated objectives can still be judged. We will now discuss each of the three tasks in turn, and estimate their utility.

First, consider Task~\ref{task:sequential_anomaly_detection} (sequential anomaly detection). While interesting in certain contexts (such as system call or web session traces), it is not likely to be able to capture a majority of interesting sequences in temporal machine data sets, since it is not appropriate for non-categorical sequences. While there are certainly interesting categorical sequences that can be analyzed in many temporal machine data sets, it can be expected that these constitute a low share of the total number of interesting sequences in Splunk applications. For this reason, it ought to be argued that this task, while interesting in some cases, should have lower priority than the other two aggregation tasks.

Task~\ref{task:frequential_anomaly_detection} (frequential anomaly detection) is more interesting. Since it enables anomaly detection to be applied to the frequency of any recurring event or value, it should be applicable to a wider range of data sets. However, it is limited in the sense that it can not accurately capture continuous quantities or other types of quantities that vary over time. This limits its applicability to temporal machine data sets, since a large share of the interesting quantities in such data (at least in Splunk applications) vary over time.

In such cases, Task~\ref{task:continuous_anomaly_detection} is more appropriate, as it allows for correct estimates of evolving quantities. As pointed out in Section~\ref{sect:aggregation}, the aggregation transformations involved in the Tasks~\ref{task:frequential_anomaly_detection} and~\ref{task:continuous_anomaly_detection} are very similar, and implementing them both should not be difficult. However, they are likely to require different anomaly measures; to limit the scope of the project, it was decided that the focus would be placed on the latter.

The next issue to settle is whether uni- or multivariate sequences should be the focus. As discussed in Section~\ref{sect:unimultivariate} multivariate sequences are more difficult to analyze than univariate sequences, and can often be broken down into univariate sequences without significant losses in analysis power. Therefore, it is reasonable to (at least initially) focus solely on univariate sequences.

Finally, it must be decided which of the Tasks~\ref{task:ad_splunk} (detecting anomalous subsequences) and~\ref{task:ad_splunk} (detecting anomalous sequences in a set of sequences) to focus on. While the latter task is interesting and occurs naturally in many contexts, e.g.\ system call traces (figure~\ref{fig:calls}), it is more reasonable to focus on the former since temporal machine data sets typically feature long and largely dissimilar sequences.

Based on the above, the following two tasks ought to be promoted as important, with the latter being preferential as a first step:
\begin{task}
\label{task:frequency}
  Detect anomalous subsequences in real-valued univariate sequences containing the frequency of some event over time.
\end{task}
\begin{task}
\label{task:continuous}
  Detect anomalous subsequences in real-valued univariate sequences sampled from a continuous process.
\end{task}

\subsection{Reference data}

One of the four objectives stated in Section~\ref{sect:goals} is that methods should be as generally applicable and require as little configuration as possible. Obviously, the reference data tasks presented in Section~\ref{sect:reference_data} meet these demands to varying degrees. Since unsupervised anomaly detection (task~\ref{task:unsupervised_anomaly_detection}) does not require the user to provide reference data, it is superior to supervised and semi-supervised anomaly detection in this regard.

However, it is not clear whether unsupervised methods can scale adequately; there seems to be a trade-off between the amount of required supervision and the scalability of algorithms. Most naive methods scale as $O(f(R \cdot N))$, where $f(n) \in \Omega(n)$, $R$ is the number of reference items, and $N$ is the number of items to be evaluated. When performing semi-supervised anomaly detection, it is usually the case that $R < N$. In unsupervised anomaly detection, however, $N = R$, and so methods scale as $O(f(N^2))$. This is problematic, and some form of pruning is required to allow large-scale analysis. One form of pruning that should perform relatively well is the use of some form of constant-size context (e.g.\@, the local context). Nevertheless, it was decided that unsupervised methods should be the focus of this project.

\subsection{Anomaly types}

The choice of both anomaly types and anomaly measure together define which anomalies will be captured by problem formulations.

Reviewing Section~\ref{sect:anomaly_types}, and especially Figure~\ref{fig:anomaly_types}, can help when reasoning about anomaly types. \emph{Point anomalies} are obviously not appropriate, as they can not capture most anomalies in continuous sequences (to see this, consider that none of the aberrations in the second, third, or fourth panels of Figure~\ref{fig:anomaly_types} are point anomalies).

\emph{Contextual point anomalies} are more relevant, especially in the context of Task~\ref{task:frequency}. Here, each individual data point in the resulting data series encodes the frequency of some event at some specific time. Assuming that these frequencies among points sharing context are independent, contextual anomaly methods should be sufficient to find temporary anomalous frequencies.

However, contextual point anomalies do not occur in continuous sequences, such as those encountered in Task~\ref{task:continuous} or in the third and fourth panels of Figure~\ref{fig:anomaly_types}.

\emph{Collective anomalies}, on the other hand, are very interesting in continuous time series, since they can cover all anomalies that arise in such series, as long as the anomalies are global (i.e.\@, they do not constitute normal behaviour elsewhere in the series).

However, non-global anomalies do occur, especially in long time series such as those encountered in monitoring applications. With this in mind, it is reasonable to conclude that being able to take \emph{contextual collective anomalies} into account is highly desirable. Further, based on the fact that all the previous anomaly types are special cases of contextual collective anomalies, it can be concluded that finding this type of anomaly should be sufficient\footnote{Note that while this is certainly true, it does not mean that methods optimized for finding contextual anomalies (for instance) would not be better at that specific task than methods for finding contextual collective anomalies. A thorough evaluation of all methods---for both tasks---would have to be performed to answer this question}.

As with any task involving contextual anomalies, a \emph{context function} must be specified before this method can be useful. Natural choices of sequence context functions include the \emph{symmetric local context}, which considers $k$ items before and after the subsequence in question, and the \emph{asymmetric local context}, which considers $k$ items before and $j$ items after the subsequence.

Since which context functions are appropriate depends heavily on the data set, context functions should be evaluated empirically.

\subsection{Anomaly measures}

As stressed in Section~\ref{sect:anomaly_measures}, the anomaly measure is probably the most important part of any anomaly detection problem. However, due to the unpredictability of how they affect results, there is not much that can said about which anomaly measures are the most appropriate for the target domain. For this reason, we will leave this factor unspecified, and instead focus on evaluating as many different choices of it as possible.

\subsection{Output format}

In one sense, the choice of output format is cosmetic, as it depends entirely on the context in which the results should be presented. 

However, as noted in Section~\ref{sect:output_format}, most other output format tasks can be reduced to Task~\ref{task:anomaly_scores} (anomaly scores). In order to reduce the risk of focusing on an inappropriate output format, this task was chosen as a focus.

\subsection{High-level tasks related to anomaly detection}

Most of these tasks are not immediately useful for anomaly detection in Splunk. For instance, signal recovery is a complex task that is unlikely to be required for most machine data and for which using specialized software is more appropriate.

The task of \emph{novelty detection} (task~\ref{task:novelty_detection}), however, has potential in both monitoring and diagnosis applications. In the former case, anomalies that occur periodically will be diagnosed every time they occur, possibly leading to an unnecessarily large number of discovered anomalies. In the latter application, anomalies that occur periodically will be completely ignored (i.e.\ treated as normal if they occur many times), which can be problematic. Novelty detection methods avoid these problems.

Note that (as stated in Section~\ref{sect:related_tasks}) novelty detection is really just contextual (or contextual collective) anomaly detection with a specific context function. For this reason, we do not treat novelty detection on sequences as a task separate from anomaly detection, but as a specific choice of context, which we refer to as the \emph{novelty context}. Finally, the problems in monitoring and diagnosis applications described above can also be mitigated by other context functions.

\section{Suggested tasks}
\label{sect:suggested_tasks}

The tasks selected in the previous Section into complete tasks in which all principal factors are fixed.

Based on the discussion in Section~\ref{sect:relevant_data_format}, the following task ought to be given the highest priority:
\begin{task}
\label{task:main}
  Detect contextual collective anomalies in, and assign anomaly scores to, the elements of a single univariate real-valued sequence sampled from a continuous process.
\end{task}

The remainder of this report will focus on this task. In the next chapter, the component framework, which simplifies the specification of remaining factors for this task, is proposed. Both ad-eval and the evaluation performed as part of this project focus on this task as well.

Of course, there are other, similar tasks that could also be very important to machine data anomaly detection. A set of tasks which naturally complement Task~\ref{task:main} in monitoring and diagnosing of sets of machine data are now presented.:

To begin with, as discussed in Section~\ref{sect:relevant_data_format}, frequential anomaly detection is very similar to continuous anomaly detection and is likely to be relevant to Splunk applications. We present this as the following task:

\begin{task}
  Detect contextual collective anomalies in, and assign anomaly scores to the elements of, a single univariate real-valued sequence containing the frequency of some event over time.
\end{task}

Furthermore, symbolic time series are potentially interesting in several applications, suggesting the following task (which is purposely vague, since it requires a different choice of principal factors):

\begin{task}
  Detect anomalous subsequences in long symbolic sequences.
\end{task}

Of course, applying Task~\ref{task:ad_splunk2} (detecting anomalous sequences in a set of sequences) to the above tasks can also be interesting. The resulting tasks can be briefly stated as follows:

\begin{task}
  Detect anomalous sequences in a set of univariate real-valued time series sampled from continuous processes.
\end{task}

\begin{task}
  Detect anomalies sequences in a set of univariate real-valued time series containing the frequency of some event over time.
\end{task}

\begin{task}
  Detect anomalous sequences in a set of similar symbolic sequences.
\end{task}

Since all of the tasks presented in this section are complementary, and since they share many similarities, they should ideally be implemented and evaluated in tandem. However, due to time and space constraints, it was not possible to study most of them in sufficient depth this project (although the component framework and \texttt{ad-eval} can both support these tasks).
