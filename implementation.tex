\chapter{ad-eval}
\label{ch:implementation}

As stated in the abstract, the development of a software framework for the evaluation of anomaly detection methods, called \texttt{ad-eval} and available at \url{http://github.com/aeriksson/ad-eval}, was a significant part of the project. In this chapter, the design, development process, and features of \texttt{ad-eval} are discussed.

Essentially, \texttt{ad-eval} consists of three separate parts: a library implementing the component framework, a comprehensive set of utilities for evaluating the performance of methods and problem, and an executable leveraging the library. The entire project is written in Python.

In this chapter, \texttt{ad-eval} is described in detail. The three parts are described in Sections~\ref{sect:implemented_problems},~\ref{sect:evaluation_package}, and~\ref{sect:executable}, and some of the design choices made in the development of \texttt{ad-eval} are discussed in Section~\ref{sect:design}.

\section{Implemented components}
\label{sect:implemented_problems}

The anomaly detection part of \texttt{ad-eval} (given the Python package name \texttt{anomaly\_detection}) is a faithful implementation of the component framework, including the algorithm proposed in Section~\ref{sect:framework}. In order to preserve the modular nature of this framework, the individual components are implemented as autonomous modules, described below.

As there are no real alternatives found in the literature, the sliding window filter is the only implemented evaluation filter. Since the optimal window width $w$ and step length $s$ depend on the application, both of these parameters were left to be user-specified.

Since new new context functions can be implemented relatively easily, and since they have a relatively major impact on the analysis, all previously discussed contexts (specifically, the asymmetric and symmetric local contexts, the novelty context, the trivial context, and the semi-supervised context) were implemented.

As reference filters, a sliding window filter and the identity filter $\mathcal{F}_E(X)=X$ were implemented, the latter because it is a better fit for dimension-independent distance measures.

Due to the limited scope of the project, the only implemented anomaly measures (henceforth referred to as \emph{evaluators}) were a variant of k-Nearest Neighbors (kNN), in which the distance to the $k$'th nearest element is considered, and a one-class support vector machine (SVM). For the kNN evaluator, the Euclidean distance as well as the compression-based dissimilarity measure~\cite{keogh2} and dynamic time warping~\cite{dtw} distances were made available. Furthermore, the symbolic aggregate approximation (SAX) and discrete Fourier transform (DFT) transformations were added as an optional pre-anomaly measure transformations. All parameters of the evaluators, distances, and transformations were left for the user to specify.


Finally, the mean, median, maximum, and minimum aggregators were implemented.

\section{Evaluation utilities}
\label{sect:evaluation_package}

The set of possible interesting tests that could be run on problems derived from Task~\ref{task:main} is considerable. A large number of component combinations can be used; most components have many possible parameter values, and it is important to assess how these affect the results; and a large set of methods with various optimizations and approximations can be proposed. For all of these choices, it is important that accuracy and performance are properly evaluated.

However, the performance of methods is highly dependent on the characteristics of the datasets to which they are applied. As mentioned in Section~\ref{sect:evaluation_data}, there is no hope to exhaustively cover the space of possible evaluation sets. Instead, sample data from the target application domain must be obtained before any tests are performed. Furthermore, obtaining adequate labeled test data is often difficult, and artificial anomaly generation must be considered as an option.

With this in mind, it was decided that an evaluation framework should be added to \texttt{ad-eval} to help facilitate the implementation, standardization, and duplication of accuracy and performance evaluations. Due to the variety of interesting tests highlighted above, an approach focused on the provision of tools that assist in scripting custom tests was deemed preferable to one focused on the construction of a single configurable testing program. To this end, utilities were developed for:

\begin{itemize}
    \item Saving and loading time series to/from file, with or without reference anomaly vectors.
    \item Pseudorandomly selecting and manipularing subsequences of series (e.g.\ for adding anomalies).
    \item Facilitating the testing of large numbers of parameter values.
    \item Generating various types of artificial anomalies and superpositioning them onto sequences.
    \item Calculating the anomaly vector distance measures $\epsilon_E$, $\epsilon_{ES}$ and $\epsilon_{FS}$ discussed in Section~\ref{sect:evaluation_measures}.
    \item Facilitating the automated comparison of several problems and methods on individual datasets.
    \item Automating the collection of performance metrics.
    \item Reporting results.
    \item Generating various types of custom plots from results.
\end{itemize}

With these tools in place, it is simple to write scripts that, for instance, generate large amounts of similar series containing random anomalies and evaluate the performance of several problems on this data in various ways. 

The tools were included in \texttt{ad-eval} as a separate Python module (called \texttt{eval\_utils} and located in the \texttt{evaluation} directory of the repository). This module was used to perform all tests in the evaluation phase of this project.

\section{Executable}
\label{sect:executable}

To enable the stand-alone use of the anomaly\_detection package, an executable was added to the \texttt{ad-eval} repository (called \texttt{anomaly\_detector} and located in the \texttt{bin} directory of the repository). This executable is used through a command-line interface and a \texttt{key:value} style configuration file, can perform supervised or semi-supervised anomaly detection on sequences from files or standard input, and can use any of the components implemented in \texttt{anomaly\_detection}.

To avoid having to modify this program every time a component in \texttt{anomaly\_detection} was changed, the executable was made unaware of all internal details of that package. Consequently, a command-line interface capable of configuring the components could not be implemented; instead, a configuration file parser is used to read and pass the component configuration to \texttt{anomaly\_detection}.

\section{Design}
\label{sect:design}

The development of \texttt{ad-eval} began at the start of the project. Initially, development efforts focused on the implementation of a few optimized methods found in the literature, to produce a Splunk app. However, as the project progressed and the issues discussed in Section~\ref{sect:adb} (that most methods were targeted at subtly different tasks, and that due to lacking evaluations, assessing which methods are really the `best' is not possible), this approach was recognized as fruitless, and abandoned.

Development then shifted towards an implementation of the component framework, with the goals of maximizing the ease of implementing and evaluating large amounts of components.

Consequently, modularity was a major focus throughout the development process, achieved through various means. As mentioned previously, the individual components were separated into independednt modules. Additionally, the evaluation utilities and the executable were decoupled from the component framework implementation, interfacing with it through only two method calls. Finally, the decision was made to distribute the configuration of the component framework implementation, letting each component handle its own configuration and making the rest of the package configuration-agnostic.

It was a natural choice to write the entire implementation in Python, for several reasons. First, Python is well suited for small, flexible projects such as \texttt{ad-eval}, thanks to its simplicity and flexibility. Furthermore, a number of great libraries for data mining and machine learning exist for Python, which were used to accelerate the development. Finally, if \texttt{ad-eval} becomes adopted for real-world use, Python's good C integration could be leveraged to write optimized code. 

Finally, the evaluation utilities were designed with ease of use and flexibility in mind. For instance, while classes facilitating test data generation, evaluation, and reporting are provided, their use is optional. As a result, evaluation scripts could be short and simple---the scripts used in the next chapter are all 30 to 70 lines long---without sacrificing flexibility.
