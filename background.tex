\chapter{Background}
\label{ch:background}

This chapter gives a brief introduction to the subject of anomaly detection. The framework of tasks and problems used throughout the paper is presented and justified.

\section{Anomaly detection}
\label{sect:adb}

In essence, anomaly detection is the task of automatically detecting items (\emph{anomalies}) in data sets that in some sense do not fit in with the rest of those data sets (i.e.\ are \emph{anomalous} with regard to the rest of the data). The nature of both the data sets and anomalies are dependent on the specific application in which anomaly detection is applied, and vary drastically between application domains. As an illustration of this, consider the two data sets shown in Figures~\ref{fig:example1} and~\ref{fig:example1}. While these are similar in the sense that they both involve sequences, they differ in the type of data points (real-valued vs.\ symbolic), the structure of the data set (one long sequence vs.\ several sequences), as well as the nature of the anomalies (a subseqeuence vs.\ one sequence out of many). Several surveys~\cite{hodge}~\cite{bakar}~\cite{chandola}~\cite{agyemang} and books~\cite{barnett}~\cite{hawkins}~\cite{leroy} have been published which treat various anomaly detection applications in greater depth.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{resources/anomaly_example}
    \caption{\small Long real-valued sequence with an anomaly at the center.}
    \vspace{-0pt}
\label{fig:example2}
\end{figure}

Like many other concepts in machine learning and data science, the term `anomaly detection' does not refer to any single well-defined problem. Rather, it is an umbrella term encompassing a collection of loosely related techniques and problems. Anomaly detection problems are encountered in nearly every domain in business and science in which data is collected for analysis. Naturally, this leads to a great diversity in the applications and implications of anomaly detection techniques. Due to this wide scope, anomaly detection is continuously being applied to new domains despite having been researched for decades.

\begin{figure}[htb]
    \centering
    \begin{tabular}{| l | l l l l l l l l |}
        \hline
        $\mathbf{S_1}$ & login & passwd & mail & ssh & \dots & mail & web & logout \\ \hline
        $\mathbf{S_2}$ & login & passwd & mail & web & \dots & web & web & logout \\ \hline
        $\mathbf{S_3}$ & login & passwd & mail & ssh & \dots & web & web & logout \\ \hline
        $\mathbf{S_4}$ & login & passwd & web & mail & \dots & web & mail & logout \\ \hline
        $\mathbf{S_5}$ & login & passwd & login & passwd & login & passwd & \dots & logout \\\hline
    \end{tabular}
    \caption{Several sequences of user commands. The bottom sequence is anomalous compared to the others.}
\label{fig:example1}
\end{figure}

In other words, anomaly detection as a subject encompasses a diverse set of problems, methods, and applications. Different anomaly detection problems and methods often have few similarities, and no unifying theory exists. Indeed, the eventual discovery of such a theory seems highly unlikely, considering the subjectivity inherent to most anomaly detection problems. Even the term `anomaly detection' itself has evaded any widely accepted definition~\cite{hodge} in spite of multiple attempts.

Despite this diversity, anomaly detection problems from different domains often share some structure, and studying anomaly detection as a subject can be useful as a means of understanding and exploiting such common structure. Anomaly detection methods are vital analysis tools in a wide variety of domains, and the set of scientific and commercial domains which could benefit from improved anomaly detection methods is huge. Indeed, due to increasing data volumes, exhaustive manual analysis is (or will soon be) prohibitively expensive in many domains, rendering effective automated anomaly detection critical to future development.

\section{On Anomaly Detection Research}

Most anomaly detection research work consists of either taking existing methods and applying them to new applications (i.e.\ on new types of data), or investigating new methods for previously studied applications. In order to handle the increasing need for effective anomaly detection in many areas of business and science it is vital that these activities can be performed in a highly automated and straight-forward manner. However, there are a few issues with the current state of the subject, which make anomaly detection research needlessly complicated.

Firstly, comparing different anomaly detection methods found in the literature is difficult, since even though it might not appear so at first glance, papers on anomaly detection often target subtly different problems. For instance, TODO. This renders direct comparisons problematic and makes it hard to assess which methods are appropriate to use in new applications. A systematic way of comparing anomaly detection methods would be helpful in mitigating this problem.

The second problem is that there is often a lack of reproducibility of produced results. Due in part to the subjective nature of the subject, and in part to a historical lack of freely available datasets, new methods are often not adequately compared to previous methods. Furthermore, the performance of many anomaly detection methods is often highly dependent on parameter choices, and only the results for the best parameter values (which might be difficult to find) are often presented~\cite{keogh5}. Finally, source code is often unavailable, which makes verification a tedious process. These issues, when taken together, make it hard to reproduce results, which in turn makes anomaly detection research needlessly difficult.

This work attempts to simplify anomaly detection research by addressing the above issues. First, a general framework for systematically comparing anomaly detection problem formulations is presented, the purpose of which is to help highlight similarities and differences between problems, and thereby simplify the application of existing methods to new domains by mitigating the first problem above.

This framework is then used to formalize the subject of anomaly detection in the domain of real-valued time series, and to reformulate the activity of finding appropriate methods for specific data sets in this domain as an optimization problem over the set of possible algorithms. It is then shown that solutions to this optimization problem can be algorithmically approximated for a large class of algorithms (including most previously published methods).

Finally, a software implementation of this optimization problem is presented, along with some preliminary performance results. This mitigates the second problem above for anomaly detection in real-valued time series, by providing an environment in which previous methods can be easily replicated and compared on arbitrary datasets.

\section{Tasks and problems framework}
\label{sect:tasks_problems}

We now present the tasks and problems framework, which is used throughout this report to reason about anomaly detection problems and discuss tasks pertinent to the target domain. A major feature of this framework is that it provides a perspective where methods and optimizations are de-emphasized in favour of problem formulations. Most of the literature is method-centric, tending to shift the focus away from nuances of the anomaly detection problems these methods address and towards small and often insignificant details, thereby inhibiting the development of a high-level perspective of the subject.

In order to facilitate the comparison of different approaches, we introduce the concepts of tasks and problems. In this context, a \emph{problem} means an exact, unambiguous problem specification with a well-defined answer. In contrast, a \emph{task} is a partial specification; it leaves out one or more factors necessary to formulate a problem. Problems can be regarded as derived from one or more tasks through the specification of additional details. Similarly, tasks can themselves be seen as derived from other, more general tasks. Tasks with a high degree of generality will be referred to as \emph{high level tasks}. Finally, \emph{methods} are defined as specific algorithms for obtaining the answer to some problem.

Due to the inexact nature of anomaly detection, it is usually not clear how to precisely specify problems that can accurately capture specific types of anomalies in specific data sets, so the problem formulations themselves must be evaluated empirically. Trying to find optimal methods before a specific problem formulation has been settled upon constitutes premature optimization and is consequently inadvisable.

Since tasks and problems are only derived from other tasks through the specification of additional details, they form a hierarchy of derivations. This hierarchy can be envisioned as a directed acyclic graph, where problems and tasks constitute sinks and non-sinks respectively. If one task could be found from which the entire graph of anomaly detection tasks and problems can be reached, then this task could be used as a definition for anomaly detection (i.e.\ it would be general enough to cover all methods and problems). We propose that the following task be used for this purpose:

\begin{task}[Anomaly detection]
\label{task:anomaly_detection}
  Given a data set\footnote{Throughout this paper, a `data set' is taken to mean a (countable) set in the mathematical sense, where each item is associated with a unique index (so that multiple items can take on the same value). Tasks to which the structure of data is not relevant are formulated using data sets even though they might apply to sequences or other types of data as well.} $D$ of data, find subsets $s \subseteq D$ that are anomalous.
\end{task}

Throughout this report, this task is used as a basis from which all other tasks and problems are derived. To facilitate the derivation of new tasks and problems, it is necessary to emphasize exactly which \emph{factors} must specified in order to derive a problem from Task~\ref{task:anomaly_detection}. Enumeration of these factors, and the possible choices for each, can provide insight into the structure of the anomaly detection hierarchy (and, consequently, into the subject itself). We note that in order to derive a problem from Task~\ref{task:anomaly_detection}, at least the following five factors must be specified:
\begin{description}
  \item[Data format] The structure of the data set on which the analysis is performed.
  \item[Reference data] The data set on which the anomaly classification should be based.
  \item[Output format] What data should be produced by methods.
  \item[Anomaly measure] The heuristic used to assess how anomalous items are.
  \item[Anomaly type] Which structural properties of the data should be considered.
\end{description}
These factors, which are emphasized throughout the report, will be referred to as \emph{principal factors}. While individual problems might specify factors (such as restrictions) other than the principal factors, \emph{any} anomaly detection problem must be derived from at least five high-level tasks, each of which corresponds to a principal factor choice. This makes the principal factors uniquely interesting. The bulk of Chapter~\ref{ch:tasks} is dedicated to the examination of these factors, the discussion of possible choices for each, and the formulation of corresponding tasks.

One major advantage of this framework is that different problems can be related through the tasks from which they inherit. Studying the `most specific common task' of two problems can be useful in estimating differences and similarities between the two problems. Furthermore, as we will see, high level tasks can often be reduced to one another. Thus, the framework can be utilized to recognize when different problems are similar or can be related through reductions. For these reasons, we believe that a theory designed to relate and contrast the tasks induced by the principal factor choices can be useful in advancing the subject of anomaly detection.

Due to the inexact and subjective nature of many anomaly detection applications---the notion of an `anomaly' is typically vague and can not be given a precise definition---it is often not possible to fully specify all factors necessary to define problems. Anomaly detection often deals with the detection of unforeseen phenomena; in this case, the nature of interesting anomalies can not be known until they are detected. Therefore, tasks are often more relevant than problems when developing approaches to new domains.

The framework can profitably be applied to perform systematic analyses of anomaly detection in new domains, or to develop new anomaly detection methods. Based on the discussion above, we suggest that anomaly detection methods for new domains are best performed through the following four-stage process:
\begin{enumerate}
  \item Consider what information is known about the domain, the type of anomalies that are interesting, et cetera. Derive the most specific possible task based on this information.
  \item Based on the specified task, derive as many problems as possible by experimenting with different choices of factors.
  \item Evaluate these problems with regard to the target domain. Select one or a few problems that seem to accurately reflect the demands of the domain.
  \item Derive and implement efficient methods for solving the selected problem (or problems).
\end{enumerate}

This process was used in the project to discuss and compare approaches to anomaly detection on machine-generated data in a systematic way. This report addresses the use of the three first steps in this project; Chapter~\ref{ch:problems} treats the first step; Chapter~\ref{ch:methods}, the second step; and Chapter~\ref{ch:results} briefly covers the third step.
