\chapter{Background}
\label{ch:background}

In this chapter the subject of anomaly detection is briefly introduced, along with a discussion of some of the major challenges in anomaly detection research. Finally, the task of finding appropriate anomaly detection methods for a given application is formulated as an optimisation problem.

\section{Anomaly detection}
\label{sect:adb}

In essence, anomaly detection is the task of automatically detecting items (\emph{anomalies}) in datasets that in some sense do not fit in with the rest of those datasets (i.e.\ are \emph{anomalous} with regard to the rest of the data). The nature of both the datasets and anomalies are dependent on the specific application in which anomaly detection is applied, and vary drastically between application domains. As an illustration of this, consider the two datasets shown in Figures~\ref{fig:example1} and~\ref{fig:example1}. While these are similar in the sense that they both involve sequences, they differ in the type of data points (real-valued vs.\ categorical), the structure of the dataset (a long sequence vs.\ several sequences), as well as the nature of the anomalies (a subsequence vs.\ one sequence out of many).

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{resources/anomaly_example}
    \caption{\small Real-valued sequence with an anomaly at the center.}
    \vspace{-0pt}
\label{fig:example2}
\end{figure}

Like many other concepts in machine learning and data science, the term `anomaly detection' does not refer to any single well-defined problem. Rather, it is an umbrella term encompassing a collection of loosely related techniques and problems. Anomaly detection problems are encountered in nearly every domain in business and science in which data is collected for analysis. Naturally, this leads to a great diversity in the applications and implications of anomaly detection techniques. Due to this wide scope, anomaly detection is continuously being applied to new domains despite having been researched for decades.

\begin{figure}[htb]
    \centering
    \begin{tabular}{| l | l l l l l l l l |}
        \hline
        $\mathbf{S_1}$ & login & passwd & mail & ssh & \dots & mail & web & logout \\ \hline
        $\mathbf{S_2}$ & login & passwd & mail & web & \dots & web & web & logout \\ \hline
        $\mathbf{S_3}$ & login & passwd & mail & ssh & \dots & web & web & logout \\ \hline
        $\mathbf{S_4}$ & login & passwd & web & mail & \dots & web & mail & logout \\ \hline
        $\mathbf{S_5}$ & login & passwd & login & passwd & login & passwd & \dots & logout \\\hline
    \end{tabular}
    \caption{Several sequences of user commands. The bottom sequence is anomalous compared to the others.}
\label{fig:example1}
\end{figure}

In other words, anomaly detection as a subject encompasses a diverse set of problems, methods, and applications. Different anomaly detection problems and methods often have few similarities, and no unifying theory exists. Indeed, the eventual discovery of such a theory seems highly unlikely, considering the subjectivity inherent to most anomaly detection problems. Even the term `anomaly detection' itself has evaded any widely accepted definition~\cite{hodge} in spite of multiple attempts.

Despite this diversity, anomaly detection problems from different domains often share some structure, and studying anomaly detection as a subject can be useful as a means of understanding and exploiting such common structure. Anomaly detection methods are vital analysis tools in a wide variety of domains, and the set of scientific and commercial domains which could benefit from improved anomaly detection methods is huge. Indeed, due to increasing data volumes, exhaustive manual analysis is (or will soon be) prohibitively expensive in many domains, rendering effective automated anomaly detection critical to future development.

As a consequence of the above, a thorough survey of the subject could not fit within the scope of this report. The interested reader is instead referred to any of several published surveys~\cite{hodge}~\cite{bakar}~\cite{chandola}~\cite{agyemang} and books~\cite{barnett}~\cite{hawkins}~\cite{leroy} have been published which treat various anomaly detection applications in greater depth.

\section{On Anomaly Detection Research}

Most anomaly detection research involves either applying existing methods to new applications (i.e.\ on new types of data) or investigating new methods in the context of previously studied applications. In order to handle the increasing need for effective anomaly detection in many areas of business and science it is vital that these activities can be performed in a highly automated manner. However, little work has been done on developing automated methods and tools for anomaly detection research.

There are a few difficulties which complicate research into anomaly detection for new applications. For one, comparing different anomaly detection methods found in the literature is difficult, since even though it might not appear so at first glance, papers on anomaly detection often target subtly different problems. This renders direct comparisons problematic and makes it hard to assess which methods might be appropriate to use in new applications. A systematic way of comparing anomaly detection methods would be helpful in mitigating this problem.

Furthermore, reproducing existing results as well as applying existing methods to new datasets is often difficult. Due in part to the subjective nature of the subject, and in part to a historical lack of freely available datasets, new methods are often not adequately compared to previous methods. Furthermore, the performance of many anomaly detection methods is often highly dependent on parameter choices, and only the results for the best parameter values (which might be difficult to find) are often presented~\cite{keogh5}. Finally, there is a lack of freely available software implementation of most methods.

An important distinction to make is that between problems and methods. Informally, the process of finding an appropriate anomaly detection method for some application can be described as a two-step process. As a first step, an appropriate problem is formulated, which accurately captures intuitive notions of what constitutes an anomaly in the specific application. Once such a problem formulation has been found, an efficient method of solving or finding approximate solutions to the problem is constructed.

Due to the subjective nature of anomaly detection, radically different problem formulations might be appropriate for applications that are superficially very similar. Furthermore, there is often no obvious connection between the intuitive notion of what constitutes an anomaly in some application and the problem formulations which most accurately capture that notion, so prospective problem formulations must themselves be empirically evaluated.

This means that unless specific information is available on what problems are appropriate for a given application, finding the correct problem formulations should take priority over formulating methods. Finding efficient methods should be done only after it has been shown that the problem the methods are solving is relevant to the application. In the literature, methods, rather than problems, are often emphasised, and it can often be unclear exactly what problem a given method is meant to solve. In this report, the focus is instead placed almost entirely on problems.

This work attempts to simplify the research process by providing tools which help address the hurdles described above. As a means of mitigating the first problem, a general framework for systematically comparing anomaly detection problem formulations is presented, the purpose of which is to facilitate high-level reasoning about anomaly detection problems, and which thereby can help simplify the application of existing methods to new domains.

Furthermore, the process of finding appropriate anomaly detection methods for a given application is studied as an optimisation problem. A software implementation of this optimisation problem for anomaly detection in real-valued sequences is presented, which can help mitigate the reproducibility issues outlined above, as well as streamline the research process by enabling researchers to automatically evaluate a large amount of problem formulations.

\section{Problem formulation}
\label{sect:problem_formulation}
As a first step towards the goal of automated tools for anomaly detection research, the task we are trying to automateâ€“--that of finding appropriate anomaly detection methods for some given application---must be formalised. In this section, the first step of the anomaly detection research process---finding an appropriate problem---is formulated as an optimization problem.

To motivate our optimisation problem formulation, one can consider a stylized variant of the typical anomaly detection research process, in which a researcher equipped with a working hypothesis (in the form of a problem formulation, which associates a set of anomaly scores with each possible input from the application) is given a dataset sampled from the target application, for which she constructs a set of anomaly scores in line with her hypothesis (i.e.\ a solution to her problem formulation). She then shows her results to a domain expert, who rates them based on how well aligned he deems them to be with his notion of what is anomalous and not in the specific application. This is repeated, with the researcher successively improving her problem formulation until the domain expert tends to agree with sufficiently well with its solutions.

A significant share of the work involved in finding appropriate methods could be avoided if this process could be automated, and one way to automate it is to formulate it as an optimisation problem that can be algorithmically solved. To do this, the concepts presented above must be formalised.

To begin with, the sets of valid problem inputs (datasets) and outputs (solutions) must be defined. Here, we simply assume that some set $D$ has been defined containing all possible datasets for the application, along with some set $S$ consisting of all valid corresponding solutions.

Next, a formal description of all allowed problem formulations must be constructed. Here, we simply assume that this description consists of a set of formulae in some logic sufficiently expressive to capture all relevant problem formulation. Let us call this set $P$.

The role of the domain expert can be modeled by means of an error function $\epsilon(d, s): D \times S \rightarrow \mathbb{R}^+$, which associates a score to any solution $s$ based on how accurately it captures the anomalies in the data $d$.

The researcher, on the other hand, really has two roles; finding a new $p$ based on the feedback from the domain expert, and computing a solution $s$ given some dataset $d$ and problem $p$. The former role corresponds to the heuristic driving the optimisation---searching the problem set $P$ for an appropriate problem---and does not need be formalised yet. The latter role can be formalised as an oracle $O(p, d): P \times D \rightarrow S$, which takes a problem $p \in P$ and an input dataset $d \in D$, and computes the associated solution $s \in S$.

The success of $p$ in capturing the anomalies in $d$ can then be stated as $\epsilon(d, O(p, d))$. Finally, since the goal is to minimise the \emph{expected} error for datasets sampled from the given application, a random variable $X$ over $D$ ought to be introduced, that models the probability of generating any given $d \in D$. A suitable objective function would then be $\mathbb{E}_I [\epsilon(d, O(p, d))]$.

The optimisation problem then becomes:
\[
    p_{opt} = \argmin_{p \in P}\mathbb{E}_X [ \epsilon(d, O(p, d)) ].
\]

Here, $p_{opt}$ corresponds to the best possible problem formulation, and $O(p_{opt}, d)$ to the solution of this problem formulation.

Of course, this optimisation problem is not tractable unless heavy restrictions are placed on the problem set $P$, since any logic sufficiently complicated to encompass non-trivial problems is undecidable (TODO: provide source?). Thus, no oracle can not exist when $P$ is the set of all formulae in such a logic. A major challenge is thus to find a reduced problem set $P^*$ which has a tractable oracle $O^*$ and which contains a majority of interesting problem formulations.

Another problem is that it is generally not possible to compute $\epsilon$ or $X$. Indeed, an algorithmic formulation of $\epsilon$ presupposes knowledge of the optimal problem formulation and would consequently render the optimisation process redundant. Likewise, the generation of a stream of data in accordance with $X$ would require an exact model of the underlying process, which could just as well be used directly to detect anomalies. Even if an external stream of datasets would be available, an actual domain expert would be required to represent $\epsilon$.

To get around these issues, $X$ can be replaced with a set of labeled training data, i.e.\ a set $T \subset D$ in which each $t_i \in T$ has an associated $s(t_i) \in S$.
Correspondingly, $\epsilon(d, s)$ can then be replaced with some $\epsilon^*(t_i, s) = \delta(s(t_i), s)$, where $\delta: S \times S \rightarrow \mathbb{R}^+$ is some distance measure.
This approach enables the computation of the following estimate of $p_{opt}$:
\[
    p^*_{opt} = \argmin_{p \in P^*} \sum_{t_i \in T} \delta(s(t_i), O^*(p, t_i)).
\]

A major focus of this report is on constructing restricted problem sets $P^*$ with corresponding tractable oracles $O^*$. In Chapter~\ref{ch:framework}, a framework for reasoning about anomaly detection problems is outlined, which can be used to construct appropriate problem sets for specific applications. This framework is then applied to sequences in Chapter~\ref{ch:time_series}, in order to construct a problem sets that generalise a majority of previously studied problem formulations while admitting a simple oracle.
