\chapter{Results}
\label{ch:results}

In this chapter, a few basic results obtained by analysing time series using ADRT are presented. Since appropriate data could not be obtained for this project, and in order to limit the scope of this report, a comprehensive analysis could not be performed. Instead, a preliminary, qualitative evaluation was performed, with the twofold goal of gaining some insight into the how component choices affect accuracy and performance, as well as demonstrating how ADRT can be used to simplify and standardize the process of evaluating anomaly detection methods.

Specifically, the analysis presented in this chapter concerns the task of finding anomalous subsequences in long real-valued sequences. This task was chosen since it is a very interesting task that has not been extensively researched, and which is complex enough to require the full oracle as presented in Section~\ref{sect:oracle}.

All graphs and data in this chapter were obtained through scripts written using ADRT. Since one of the main goals of the evaluation was to demonstrate how ADRT can be used to perform standardized evaluations, all scripts used to obtain the figures and results in this chapter are available in the ADRT source code repository. Modifying these scripts to use other datasets or to evaluate other components (such as the SVM anomaly measure implemented in ADRT) is trivial.

\section{Method}
As the main goal of the analysis was to demonstrate the utility of the framework in reasoning about and evaluating anomaly detection approaches, the focus of this chapter is on using the framework and ADRT to assess how problem accuracy and performance varies over the problem set.

Since an analysis of the entire problem set would be prohibitively expensive, this is instead achieved by means of parametrising a constrained problem set, fixing a point (in other words, a problem) in this set, and measuring accuracy and performance in a neighbourhood (as defined by the parametrisation) of this point.

Since both the specific task studied and approach taken are novel, the error measures $\epsilon_E$, $\epsilon_F$, and $\epsilon_N$ implemented in ADRT must themselves be empirically evaluated to verify that they accurately capture intuitive error notions. Were this not the case, using them could invalidate the entire analysis. For this reason, an analysis of their performance is presented prior to the problem set analysis.

Due to difficulties in obtaining appropriate real-world test data, and due to the fact that evaluating anomaly detection methods on artificial data is dubious at best, the decision was made to limit the analysis to a qualitative one. Instead, necessary tools are provided (in the form of ADRT scripts) to make it trivial to perform a thorough analysis once the appropriate data is available.

\begin{figure}[h]
   %  \vspace{-10pt}
    \begin{center}
        \includegraphics[trim = 10mm 0mm 5mm 0mm, clip, width=\textwidth]{resources/reference_sequence}
    \end{center}
   %  \vspace{-20pt}
    \caption{\small{The sequence $s^*$ and corresponding reference anomaly vector $a^*$.}}
\label{fig:reference_sequence}
   %  \vspace{-10pt}
\end{figure}

To emphasise this qualitative nature, the analysis in this chapter has been performed on a single artificial sequence, using scripts which can be run on bigger datasets without modification. The chosen sequence, which will be referred to as $s^*$, is shown in Figure~\ref{fig:reference_sequence}, along with the reference anomaly vector used.

\section{Problem set}
Evaluating the set of all problems solvable by ADRT would make for a rather lengthy and dull analysis. To avoid this situation, a few constraints were placed on the problem set. Specifically, $M$ was restricted to kNN-based anomaly measures (as defined in Section~\ref{sect:anomeasure}, and $C$ was restricted to local contexts (as defined in Section~\ref{sect:context}) with $a = b = m$.

Next, a parametrisation of this problem set was defined by parametrising each of the allowed components separately. The filters $F_E$ and $F_R$, being sliding window filters, are naturally parametrised by the step length $s$ and window length $w$. The context function $C$, has only a single parameter $m$ given the above restrictions. The kNN anomaly measures can be considered to be parametrised by the value of $k$ and the distance measure $\delta$. Specific $\delta$ can in turn be parametrised individually. Since only four distinct values of $\Sigma$ are available, it does not need to be parametrised.

Finally, a single point in the restricted problem set was selected, and performance and accuracy characteristics were assessed in the neighbourhood of this point. The specific point, which will henceforth be referred to as $s^*$, is given by $s = 1$, $w = 10$, $m = 400$, $k = 1$, $\delta$ is the standard Euclidean distance, and $\Sigma = \Sigma_{mean}$. This point was chosen by using ADRT to optimise for $\epsilon_B$ over the given problem set.

To simplify the following discussion, we now introduce some notation. We will denote our fixed point by $P^*$, and let $P^*_{\alpha_1, \alpha_2, \dots, \alpha_n}$ be the set of problems where $\alpha_1, \alpha_2, \dots, \alpha_n$ are allowed to take on any value, and all other parameters are given by $P^*$ (e.g., $P^*_k$ corresponds to the set of problems where all parameters other than $k$ agree with the $P^*$). We will further denote the set of corresponding solutions for $s^*$ by $A_{\alpha_1, \alpha_2, \dots, \alpha_n}$.

\section{Error measures}
\label{sect:error_measure_eval}

\begin{figure}
    \centering
    \subbottom{\includegraphics[width=0.49\linewidth]{resources/normalized_euclidean_distance_heat_map}}
    \subbottom{\includegraphics[width=0.49\linewidth]{resources/equal_support_distance_heat_map}}
    \subbottom{\includegraphics[width=0.49\linewidth]{resources/full_support_distance_heat_map}}
    \subbottom{\includegraphics[width=0.49\linewidth]{resources/best_support_distance_heat_map}}
    \caption{Heat maps showing the normalized values of $\epsilon(A(P_{k, w}^*, s^*), a^*)$ for the three errors and $k, w = 1,2,\dots,50$. Blue and red correspond to low and high error values, respectively.}
\label{fig:error_heat_maps}
\end{figure}

In Section~\ref{sect:error_measures}, three error measures for anomaly vectors were proposed: $\epsilon_N$, $\epsilon_E$, and $\epsilon_F$. In this section, an investigation into how well these error measures capture intuitive notions of error is presented.

This investigation was performed by computing and graphing the errors \\ $\epsilon(O^*(P_{k, w}^*, s^*), a^*)$ for $\epsilon = \epsilon_N, \epsilon_E$ and $\epsilon_F$ and $k, w \in \{0,2,\dots,50\}$. Heat maps of the resulting values are shown in Figure~\ref{fig:error_heat_maps}. A few of the $A_{k, w}$ which were given the lowest values by each of the error measures are shown in Figure~\ref{fig:n_best_anomaly_vectors}.

As shown in the heat maps, the three error measures give similar results, attaining minima and maxima in the same regions. Since $\epsilon_E$ and $\epsilon_F$ operate on binary strings and thus have discrete domains, they often assign identical errors to nearby points. This is the cause of the relatively jagged appearance in the plots of these errors compared to the smoother appearance of the $\epsilon_N$ plot.

\begin{figure}[ht]
   %  \vspace{-5pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/n_best_anomaly_vectors}
    \end{center}
   %  \vspace{-20pt}
    \caption{\small{The $n$th best $A_{k, w}$ according to the three error measures for $n = 1, 10, 50$ and $100$.}}
\label{fig:n_best_anomaly_vectors}
   %  \vspace{-10pt}
\end{figure}

Figure~\ref{fig:n_best_anomaly_vectors} shows the anomaly vectors with the $n$th lowest errors for the three distance measures. All three measures give similar anomaly vectors for $n = 1$ and $n = 10$, with $\epsilon_N$ and $\epsilon_F$ giving the same anomaly vector in both cases. For $n = 50$ and $n = 100$, however, $\epsilon_F$ seems to prioritize smooth anomaly vectors, while the other two anomaly measures prioritize anomaly vectors with few false positives.

One interesting aspect evidenced in the heat map plot is that while $\epsilon_E$ and $\epsilon_F$ are both very large for $A_{k,w}$ with small $w$, this tendency is not shared by $\epsilon_N$. As seen in Section~\ref{sect:w}, anomalies significantly larger than $w$ will not be detected by kNN methods, which means that assigning a large value to these anomaly vectors is reasonable. Since the $\epsilon_N$ (unlike the other two error measures) gives equal weight to every component, it will assign relatively low values to anomaly vectors that only partially capture anomalies as long as most of their elements are close to zero.  Indeed, this is the case for the $A_{k,w}$ with small $w$ since these anomaly vectors are close to constant everywhere except for a few spikes.

\begin{figure}[ht]
   %  \vspace{-5pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/euclidean_problem}
    \end{center}
   %  \vspace{-20pt}
    \caption{\small{A reference anomaly vector for a long sequence and two corresponding candidate anomaly vectors. The first candidate vector, while noisy, correctly marks the anomaly. The second candidate does not mark the anomaly and marks two false anomalies. $\epsilon_N$, $\epsilon_E$ and $\epsilon_F$ for the two sequences are $8.3$ and $2.2$; $0.010$ and $0.99$; and $0.0050$ and $0.99$, respectively.}}
\label{fig:euclidean_problem}
   %  \vspace{-10pt}
\end{figure}

As an illustration of the potential problems this could cause, see Figure~\ref{fig:euclidean_problem}, which shows one reference anomaly vector and two candidate anomaly vectors for a long sequence. Here, the first anomaly vector, while noisy, accurately captures the anomaly while the second not only misses the anomaly, but also introduces two false positives. While $\epsilon_F$ and $\epsilon_E$ are significantly smaller for the first candidate than for the second, the reverse is true for $\epsilon_N$. This problem is amplified as the sequence length grows. These results indicate that $\epsilon_N$ should be used with caution, and that since other two error measures are preferable since they were defined specifically to avoid problems such as this.

% \clearpage

\section{Parameter values}
\label{sect:params}

The accuracy and performance of problems around $P^*$ is now studied for the sequence $s^*$, by means of varying each of $k, \delta, t, w, s, m,$ and $\Sigma$ individually and looking at the resulting anomaly vectors and oracle execution times.

As mentioned previously, since only a small subset of the problem space is studied, and only a single sequence is used, no conclusions about the global characteristics of the problem space, or about how well the results might extend to other sequences can be drawn. Instead, the analysis in this Section should be considered a first step towards establishing a broader understanding of how problem accuracy can vary over the problem sets for distance-based problems, and as an introduction to ADRT, including some useful ways to explore performance and accuracy characteristics.

\subsection{The k value}

\begin{figure}
    \centering
    \subbottom{\includegraphics[trim=5mm 5mm 0mm 0mm, width=0.95\linewidth]{resources/k_heat_map}}
    \subbottom{\includegraphics[trim=0mm 0mm 2mm 0mm, width=0.49\linewidth]{resources/k_error}}
    \subbottom{\includegraphics[trim=2mm 0mm 0mm 0mm, width=0.49\linewidth]{resources/k_time}}
        \caption{Results for $P^*_k$, where $k = 1,2,\dots,100$. The top panel shows the anomaly vectors $A_k$ (red and blue indicate high and low anomaly scores, respectively). The anomaly vectors have been individually normalised to lie in the unit interval. In the upper right panel, the errors $\epsilon_N$, $\epsilon_E$ and $\epsilon_F$ are plotted as a function of k. The errors curves have been individually normalised. Finally, oracle computation times for a single execution are plotted as a function fo $k$ in the lower right panel.}
\label{fig:k_plot}
\end{figure}

It is important to study how the anomaly vectors vary with $k$; first, because the choice of $k$ is likely to have a large impact on the appearance of the anomaly vector, regardless of the dataset; and second, because the kNN anomaly measure only operates on a single $k$ value at a time, it is in a sense the simplest distance-based anomaly measure, and thus an ideal tool for better understanding how the choice of $k$ impacts the analysis. This understanding is crucial in effectively designing other types of distance-based anomaly measures.

In order to understand how the $k$ value affects the resulting anomaly vectors, the problems $P^*_k$ for $k = 1,2,\dots,100$ were solved for $s^*$. The results are shown in Figure~\ref{fig:k_plot}.

The smoothness with which the $A_k$ vary with $k$ indicate that using several nearby $k$ in distance-based anomaly measures is not likely to significantly improve accuracy. Furthermore, at least in this case, $k=1$ minimizes all three error measures, and there is no indication that considering additional $k$ might help. While higher $k$ do lead to other regions being marked as anomalous, these regions do not correspond to relevant features. Were this to hold in general, considering $k$ higher than $1$, or using linear combinations of several $k$ is not likely to lead to any significant increase in accuracy. It is highly doubtful, however, that this holds fin general.

Since the implemented kNN method operates by brute force, the entire reference set must be evaluated regardless of $k$, so the constant evaluation time exhibited in the bottom right of the figure is expected. For any distance measure that is also a metric---such as the Euclidean distance---more efficient methods exist.

\subsection{The distance function}
% \FloatBarrier{}

\begin{figure}[ht]
   %  \vspace{-20pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/delta_heat_map}
    \end{center}
   %  \vspace{-20pt}
    \caption{\small{Heat maps showing $A_{k, \delta}$ for the Euclidean and DTW distances.}}
   %  \vspace{-10pt}
\label{fig:delta_heat_map}
\end{figure}

For obvious reasons, the choice of the distance function $\delta$ can have a great impact on the anomaly vectors when using distance-based methods. The distance measures implemented in ADRT are the Euclidean distance, the dynamic time warp (DTW) distance, and the compression-based dissimilarity measure (CDM). To investigate the relative performance of these, the anomaly vectors $A_{k, \delta}$ were examined. Note that since $A_{\delta}$ consists of only one value per distance measure, calculating only $A_\delta$ would have yielded insufficient data.

% The ADRT implementation of the CDM performed poorly. To begin with, it ran significantly slower than the other methods, rendering any comprehensive analysis impossible. Furthermore, it produced poor anomaly vectors. There are a few possible explanations for this. First, the z-normalization step of the SAX transformation (in which each extracted subsequence is given zero empirical mean and unit variance) leads to poor results on random data regardless of the distance measure. Second, the window width of $10$ used in the standard configuration means that the extracted sequences are short and can not be efficiently compressed, leading to a roughly constant distance value. While the CDM will likely perform better and with other parameters, it was decided that the CDM would not be investigated further due to its slowness.

\begin{wrapfigure}{r}{0.5\textwidth}
\changecaptionwidth
\captionwidth{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{resources/delta_errors}
\caption{\small{Errors for $A_{k, \delta}$.}}
\label{fig:delta_errors}
\end{wrapfigure}

Since the CDM distance measure performed seemed to perform poorly both in terms of accuracy and performance around $P^*$, it is not included in this analysis. Most likely, this was caused by the elements of reference set being relatively small; letting $F_E$ be the identity transformation should give better results.

Instead, the focus was placed on comparing the Euclidean and DTW distances. Heat maps of the resulting anomaly vectors are shown in Figure~\ref{fig:delta_heat_map} and a plot of the corresponding errors is shown in Figure~\ref{fig:delta_errors}. As is seen in the heat maps, there is generally little difference between the outcomes of the two distance measures; the DTW distance gives slightly `cleaner' (i.e.\@, with non-anomalous regions closer to $0$) anomaly vectors for very low values of $k$, while the Euclidean distance assigns a slightly lower score to the false anomalies encountered at high values of $k$. While there are some differences in the obtained errors---the DTW distance gives a better normalized Euclidean error, while the Euclidean distance generally gives better values of the other two errors---the evaluation is not sufficient to draw any conclusions about the relative merits of the two measures.

However, the fact that the DTW distance does not perform worse than the Euclidean distance in this evaluation is interesting. Since the DTW was designed to recognize long, shifted but relatively similar continuous sequences, it might be expected to perform poorly on other types of data, such as the short, noisy sequence used in this evaluation. The fact that this is not the case is a positive indication.

\clearpage

\subsection{Transformations}
% \FloatBarrier{}

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{resources/dft_heat_map}
    \end{center}
    \caption{\small{Anomaly vectors $A_{k, t} $ for $k = 1,2,\dots,100$ with and without the discrete Fourier transform.}}
\label{fig:dft_heat_map}
\end{figure}

\begin{wrapfigure}{r}{0.5\textwidth}
\changecaptionwidth
\captionwidth{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{resources/dft_errors}
\caption{\small{Errors of the $A_{k, t}$.}}
\label{fig:dft_errors}
\end{wrapfigure}

As discussed in Chapter~\ref{ch:time_series}, applying transformations to extracted subsequences prior to evaluation, such as to perform dimensionality reduction, might assist in discovering certain types of anomalies. While a large number of compressions and other transformations deserving investigation have been proposed, due to time constraints, only the discrete Fourier transform (DFT) was properly implemented in ADRT.

The performance of the DFT was investigated by evaluating $s^*$ for $k = 1,2,\dots, n$ with and without the DFT\@. A heat map of the results is shown in Figure~\ref{fig:dft_heat_map}, and a plot of the corresponding errors is shown in Figure~\ref{fig:dft_errors}.

While the DFT gave fairly accurate anomaly vectors for low values of $k$, it performed poorly overall, returning less accurate anomaly vectors and higher error values over all $k$. This is reasonable: the DFT is not expected to perform well on random data. A proper evaluation of the performance characteristics of kNN methods using the DFT would require a more diverse dataset.

\subsection{The sliding window width}
\label{sect:w}

\begin{figure}
    \centering
    \subbottom{\includegraphics[trim=5mm 5mm 0mm 0mm, width=0.96\linewidth]{resources/w_heat_map}}
    \subbottom{\includegraphics[trim=0mm 0mm 2mm 0mm, width=0.49\linewidth]{resources/w_error}}
    \subbottom{\includegraphics[trim=2mm 0mm 0mm 0mm, width=0.49\linewidth]{resources/w_time}}
    \caption{Results for $P^*_w$, where $w = 1, 2, \dots, 50$.}
\label{fig:w_plot}
\end{figure}

Since the sliding window width $w$ determines the size of the elements used by the anomaly measure, it can be expected to have a significant impact on the size of detected features. To determine if this was the case, the anomaly vectors $A_w$ for $w = 1$, $2$, \dots, $50$ were computed and examined. The results are shown in Figure~\ref{fig:w_plot}.

As seen in the figures, very low values of this parameter are associated with a very high error. This is expected, since as $w$ tends to $1$, the target anomaly type is reduced to point anomalies. Furthermore, all errors increase sharply as $w$ nears $20$, indicating that large values of $w$ can lead to inaccurate results.

Interestingly, as seen in the figure, beyond $w \approx 3$, increasing $w$ essentially amounts to smoothing the resulting anomaly vectors. Since the anomaly in $s^*$ has a relatively small width of $40$, and since its surroundings have low anomaly values for low values of $w$, this could help explain why the anomaly is not detected after $w \approx 40$.

It is further interesting to note that while the errors are at a minimum when $w \approx 5$, the anomaly vectors in this area contain three separate spikes in the vicinity of the anomaly, rather than a single smooth bump. Arguably, the anomaly vectors at $w \approx 10$ are preferable, since they more clearly mark the anomaly. This suggests that the error measures may need refinement.

Finally, while the evaluation time ought to be roughly independent of $w$ (or proportional to the evaluation time of the distance metric with vectors of length $w$), there is a clear decrease in the computation time as $w$ grows. This is unsurprising, considering that the relatively small width of the evaluation sequence means fewer elements are evaluated as $w$ grows.

\subsection{The sliding window step}
\label{sect:s}

\begin{figure}
    \centering
    \subbottom{\includegraphics[trim=5mm 7mm 0mm 0mm, width=0.96\linewidth]{resources/s_heat_map}}
    \subbottom{\includegraphics[trim=0mm 0mm 2mm 0mm, width=0.49\linewidth]{resources/s_error}}
    \subbottom{\includegraphics[trim=2mm 0mm 0mm 0mm, width=0.49\linewidth]{resources/s_time}}
    \caption{Results for $P^*_s$, where $s = 1,2,\dots,10$. As expected, the oracle execution times scale as $O(1/s^2)$}
\label{fig:s_plot}
\end{figure}

The sliding window step length, $s$, is interesting mainly for the large effect it has on the execution time. For a brute-force kNN anomaly measure with the trivial context and sliding window filters, the number of comparisons performed on a sequence of length $L$ is $O({(L/s)}^2)$. It is therefore desirable to choose a value of $s$ that is as large as possible. However, it is likely that all three errors increase with $s$ for all sequences, and large $s$ values might lead to poor results.

To gain some insight into how kNN methods perform with higher $s$, the anomaly vectors $A_s$ were computed for $s = 1$ to $10$ (the value of $w$ is $10$ in the default configuration). The results are shown in Figure~\ref{fig:s_plot}.

As seen, the anomaly vectors are fairly accurate for all $s$. No major false anomalies are exhibited for $s < 8$, and the actual anomaly is still clearly detected over all $s$. This is reflected in the error plot: all errors are low until $s \geq 8$. Additionally, the evaluation time plot follows the expected $O(1/s^2)$ trend.

In light of these results, perhaps a multi-resolution scheme should be considered, in which a preliminary, `coarse' evaluation (corresponding to high $s$), and a `fine' evaluation (corresponding to low $s$) is performed only on those subsequences which are given the highest anomaly scores in the coarse evaluation. Depending on how the subsequences for the fine evaluation are selected, and on the context type, such an algorithm could achieve either lower computational complexity or an evaluation time reduction by a constant factor. If, as indicated in this evaluation, false positives but no false negatives are introduced as $s$ increases, fine evaluation would only rule out false anomalies, and there would be no loss of analytical power.

\subsection{The context width}
\label{sect:m}

\begin{figure}
    \centering
    \subbottom{\includegraphics[trim=7mm 7mm 3mm 0mm, width=0.96\linewidth]{resources/m_heat_map}}
    \subbottom{\includegraphics[trim=0mm 0mm 2mm 0mm, width=0.49\linewidth]{resources/m_error}}
    \subbottom{\includegraphics[trim=2mm 0mm 0mm 0mm, width=0.49\linewidth]{resources/m_time}}
    \caption{Results for $P^*_m$, where $m = 20,21,\dots,400$. Note the false anomaly present at the left end of the anomaly vectors until $m \approx 330$.}
\label{fig:m_plot}
\end{figure}

Which values of the context width $m$ are appropriate depends heavily on the application domain and on the types of anomalies present in the data. Ideally, the importance of the context width should be evaluated by considering several sequences with a natural context concept, such as the bottom series in Figure~\ref{fig:anomaly_types}. Constructing representative artificial datasets of such sequences is likely to be difficult, so real-world series should be used for such an evaluation.

While such datasets are not available, a simple evaluation on the available data can still prove illuminating. The sequence $s^*$ is highly homogeneous and admits no non-standard contexts. Thus, all errors should be expected to decrease monotonically with increasing $m$. To confirm this, the anomaly vectors $A_m$ were computed for $m = 20$ to $400$. The results of this evaluation are shown in Figure~\ref{fig:m_plot}.

As these figures demonstrate, the anomaly vectors identify a false anomaly at the left end until $m \approx 330$, at which point the false anomaly disappears and the errors decline sharply. That this false anomaly appears for small context widths is understandable since, as seen in Figure~\ref{fig:reference_sequence}, the sequence includes values at its left end that are not seen again until the right end. As expected, the error is minimized when the trivial context (corresponding to $m > 390$) is incorporated.

Finally, it should be noted that while the size of the reference set, and consequently the evaluation time, grows linearly with the size of the context, the average context size only grows linearly with $m$ when $m$ is much smaller than the sequence length. When $m$ is close to the sequence length, the context size for a large portion of the subsequences extracted by the evaluation filter will be limited by the sequence edges. This leads to the curve in the bottom right of the figure.

\subsection{The aggregation function}
\label{sect:A}

To get an idea of how the choice of aggregation function $\Sigma$ affects the analysis, the anomaly vectors $A_{k, \Sigma}$ were computed and analyzed for the minimum, maximum, median and mean aggregation functions, with $k = 1,2,\dots,100$. Heat map plots of the results are shown in Figure~\ref{fig:aggregator_heat_map}, and plots of the corresponding error measures are shown in Figure~\ref{fig:aggregator_error}. Single anomaly vectors for $k=1$ are shown in Figure~\ref{fig:aggregator_vectors}.

As seen in Figures~\ref{fig:aggregator_error} and~\ref{fig:aggregator_vectors}, $\Sigma_{min}$ and $\Sigma_{max}$ produce blocky, piecewise constant anomaly vectors, while $\Sigma_{mean}$ (and, to a lesser extent, $\Sigma_{median}$) produces smooth anomaly vectors.

% \begin{figure}
%     \centering
%     \subbottom[Heat map of the $A_m$. Note the false anomaly present at the left end of the anomaly vectors until $m \approx 330$.]{%
%         \includegraphics[width=\linewidth]{resources/aggregator_heat_map}}
%     \subbottom[Errors.]{%
%         \includegraphics[width=0.45\linewidth]{resources/aggregator_error}}
%     \subbottom[Evaluation times.]{%
%         \includegraphics[width=0.45\linewidth]{resources/aggregator_vectors}}
%     \caption{Results for $P^*_m$, where $m = 20,21,\dots,400$}
% \label{fig:m_plot}
% \end{figure}

\begin{figure}[!ht]
    % \vspace{-20pt}
    \begin{center}
        \includegraphics[trim = 5mm 2mm 10mm 2mm, clip, width=\textwidth]{resources/aggregator_heat_map}
    \end{center}
    % \vspace{-10pt}
    \caption{\small{Heat maps showing $A_{k, \Sigma}$ for the four aggregation functions.}}
\label{fig:aggregator_heat_map}
    % \vspace{-15pt}
\end{figure}

As is seen in the plots, $\Sigma_{mean}$ and $\Sigma_{median}$ performed almost identically for all error measures.

As could be expected, $\Sigma_{min}$ consistently leads to the highest values of $\epsilon_F$. It is likely to give a low score to a point if a single element containing that point has a low anomaly score, which effectively means that parts of anomalies will tend to be undervalued---something $\epsilon_F$ is sensitive to. In contrast, $\Sigma_{max}$ consistently led to the lowest values for $\epsilon_F$. This is also as expected, since $\Sigma_{max}$ will assign high values to any point contained in an anomalous subsequence.

Similar, but less clear, results were obtained for $\epsilon_E$. $\Sigma_{min}$ consistently performed the worst with low $k$, while $\Sigma_{max}$ performed the best, on average, with $k$ up to $40$.

Finally, $\epsilon_N$ was consistently lower for $\Sigma_{min}$ than for $\Sigma_{max}$. This is likely a consequence of the fact that $\Sigma_{min}$ tends to assign scores close to zero to all elements except for a few, while $\Sigma_{max}$ tends to assign scores close to zero to only a few elements. As discussed in Section~\ref{sect:error_measure_eval}, $\epsilon_N$ has a bias in favor of anomaly vectors where most elements are close to zero.

\begin{figure}[!ht]
    % \vspace{-5pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/aggregator_error_2}
    \end{center}
    % \vspace{-18pt}
    \caption{\small{Errors of the anomaly vectors $A_{k, \Sigma}$.}}
\label{fig:aggregator_error}
    % \vspace{-10pt}
\end{figure}

\begin{wrapfigure}{r}{0.49\textwidth}
\changecaptionwidth
\captionwidth{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{resources/aggregator_vectors}
\caption{\small{Plot of the $A_{k, \Sigma}$ for $k=1$.}}
\label{fig:aggregator_vectors}
\end{wrapfigure}

In conclusion, all evaluated choices of $\Sigma$ performed roughly equally well on $s^*$ (arguably, the minimum $\Sigma$ performed slightly worse than the others). If this holds in general, then it appears that the choice of $\Sigma$ is mainly one of aesthetics.
