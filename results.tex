\chapter{Results}
\label{ch:results}

Due to the lack of appropriate evaluation data mentioned in Chapter~\ref{ch:evaluation}, and in order to limit the scope of this report, a comprehensive evaluation of the implemented problems could not be performed. Instead, a preliminary, qualitative evaluation was performed, with the goal of gaining some insight into the relative performance of problem formulations derived from Task~\ref{task:main}, and demonstrating how \texttt{ad-eval} can be used to simplify and standardize the process of evaluating anomaly detection problems and methods. 

In this chapter, the results of this evaluation are presented. Since distance-based evaluators can be combined with a wider set of other components than classifier-based or other types of evaluators, and to keep this report from becoming overly long, the focus was placed entirely on the kNN evaluator implemented in \texttt{ad-eval}. The performance of this evaluator was investigated through the identification of all remaining unspecified parameters of the components $(\mathcal{F}_E, \mathcal{C}, \mathcal{F}_R, \mathcal{M},\mathcal{A})$, and individual investigations of how these parameters affect the analysis.

Of course, all graphs and data in this chapter were obtained using \texttt{ad-eval}. Since one of the main goals of the evaluation was to demonstrate how \texttt{ad-eval} can be used to perform standardized evaluations, all scripts used to obtain the figures and results in this chapter are available in the \texttt{ad-eval} source code repository. Modifying these scripts to use other datasets or to evaluate other components (such as the SVM evaluator implemented in \texttt{ad-eval}) is trivial.

\section{Evaluation approach}

Since the number of possible combinations of components and parameter values is enormous (even with the limited number of components implemented in \texttt{ad-eval}), it was not feasible to include a comprehensive evaluation of all these combinations in this report. Instead, the components were studied individually, using a preset configuration and varying individual parameter values. Note that this approach only allows for the assessment of local characteristics of the parameter space around the specific configuration.

As a second limitation, it was decided that the analysis would be focused on a single artificial sequence, owing to the lack of proper datasets to evaluate.
While a large artificial dataset could have been generated for the evaluation, this would have been counterproductive for several reasons.
Specifically, as discussed in Chapter~\ref{ch:evaluation}, generating dataset sufficiently diverse to accurately reflect the characteristics of real-world datasets in target domain is practically impossible, and creating even a rough artificial approximation would require substantial effort.
Any such dataset that could fit within the scope of this project would thus be severely limited, and would not be appropriate for assessing real-world performance.
To highlight these deficiencies, and to simplify the exposition, it was decided that the focus would be placed on examining the performance characteristics of problems on a single artificial sequence.
Rather than performing evaluations with different types of artificial sequences, all scripts used in the evaluation were added to the \texttt{ad-eval} source repository, and were constructed such that performing similar evaluations on new sequences would be trivial.

\begin{figure}[h]
    \vspace{-10pt}
    \begin{center}
        \includegraphics[trim = 10mm 0mm 5mm 0mm, clip, width=\textwidth]{resources/reference_sequence}
    \end{center}
    \vspace{-20pt}
    \caption{\small{The standard sequence with corresponding reference anomaly vector.}}
\label{fig:reference_sequence}
    \vspace{-10pt}
\end{figure}

Before choosing a sequence, there were a few things to consider. Since evaluating a large number of problem should be possible on modest hardware in short time spans, and since anomaly detection methods are typically rather slow, the sequence should be short. However, the sequence should still contain both normal data, homogeneous enough to establish a baseline of `normal' behaviour, as well as an anomaly that deviates appropriately from this baseline. ``Appropriate'', in this case, means that it should be relatively easy to detect with most reasonable parameter choices, but difficult enough to be detectable regardless of the parameter choices. A sequence of length $400$ was settled on, generated by a random walk, with added noise in the range $180$ to $220$. This sequence, referred to in the remainder of this chapter as the \emph{standard sequence} or $s^*$, is shown in Figure~\ref{fig:reference_sequence} along with the corresponding reference anomaly vector $a^*$.

\section{Parameter space}

When considering the set of problems derived from some task, it can be helpful to regard the set of variables necessary to fully specify a problem from it as the \emph{parameter space} of that task. Individual variables correspond to dimensions in the space, while problems correspond to points. General tasks are associated with large, high-dimensional parameter spaces while more specific tasks have smaller, more manageable spaces.  Searching for an optimal problem derived from some task for some dataset, then, means searching for a point in the parameter space of the task at which the corresponding problem can most efficiently find the anomalies in the dataset. Equivalently, this can be thought of as an attempt to minimize some error function over the parameter space. 

In this case, the parameter space corresponds to the free parameters of the components $C = (\mathcal{F}_E, \mathcal{C}, \mathcal{F}_R, \mathcal{M},\mathcal{A})$. We will denote these by $\Theta$. Assuming that some function $A(\Theta, s)$ is provided that solves the problem corresponding to $\Theta$ for a sequence $s$ (or equivalently uses the anomaly detector corresponding to $\Theta$ to evaluate $s$) and outputs an anomaly vector, the task of finding an optimal problem formulation for some dataset $S$ can be seen as the task of finding $\argmin_\Theta E(\Theta, S)$ for some error function $E$ that evaluates the error of $A(\Theta, s)$ for each $s \in S$. Assuming that this error function is a linear combination of the errors according to some error measure $\delta$ of the elements in $S$, this can be written as
\[
    E_{S, \delta}(\Theta) = \sum_{(s_i, a_i) \in (S, A)} \delta(A(\Theta, s_i), a_i).
\]
Given a dataset and a set of possible components, this task is relatively straight-forward. While filters, contexts and aggregators with infinite parameter spaces are possible, the components discussed in this report have relatively small, finite parameter spaces. This means that an exhaustive search would be possible in theory.

However, most choices of $C$ will typically have a large number of free parameters, resulting in a high-dimensional parameter space. This, in combination with the fact that $E_{S, \delta}(\Theta)$ typically takes a long time to evaluate, even for small $S$, renders such exhaustive searches prohibitively computationally expensive in practice.

\section{Standard configuration}
\label{sect:standard_config}

Due to the limited computational resources available when performing the evaluation, and in order to simplify the presentation, the parameters had to be studied in isolation. This amounts to studying the behaviour of $E_{S, \delta}$ near a single point in the parameter space by considering its behaviour along orthogonal lines meeting at this point. This is not sufficient to draw any strong conclusions about global minima of the error over the parameter space, unless the parameters influence $E$ independently, which is certainly not the case for Task~\ref{task:main}. However, this is not problematic; the aim of the evaluation was to gain insight into how the individual parameters affect the outcome, not to find global minima. 

The fixed point in the parameter space will be referred to as the \emph{standard configuration}, and denoted by $\Theta^* = (\theta_1^*, \theta_2^*, \dots, \theta_n^*)$. The choice of $\Theta^*$ is now described.

As mentioned previously, $\mathcal{E}$ was fixed as a kNN evaluator. This evaluator has the free parameters $k$ (which of the nearest neighbors to use when calculating the anomaly score---set to 1 by default), $\delta$ (the distance function---the standard Euclidean distance $\delta_E$, by default) and an optional transformation $t: \mathbb{R}^n \rightarrow \mathbb{R}^m$ to apply to the items in the reference set before the evaluation. By default, no transformation is used.

Because the distance measures implemented in \texttt{ad-eval} (except the compression-based dissimilarity measure) require extracted elements to be of the same length, sliding window filters were the only reasonable choice. It was thus decided that sliding window filters would be used for both the evaluation and reference filters, with the free parameters $w$ (window width---10 by default) and $s$ (step length---1 by default).

Since the evaluation was performed on artificial data, there was no reason to use either the novelty or asymmetric local context. Furthermore, since the trivial context is just a special case of the local symmetric context (with the context width $m$ set to a maximum), the trivial context was selected to be the default, with the single free parameter $m$, set to $400$.

Finally, since the four aggregators implemented in \texttt{ad-eval} are all parameter-free, it was decided that the aggregator itself would be treated as a parameter. Since it was assumed that the mean aggregator would give the most accurate results, it was selected as the default.

In summary, then, the parameter space for this evaluation is parametrized by $(k, \delta, t, w, s, m, \mathcal{A})$. To simplify the following discussion, we will take $\Theta^*$ to be the point where all parameters take on the values specified above, and let $\Theta^*_{\alpha_1, \alpha_2, \dots, \alpha_n}$ be the set of points where all parameters except $\alpha_1, \alpha_2, \dots, \alpha_n$ take on these values (e.g., $\Theta^*_k$ corresponds to the set of points where all parameters other than $k$ agree with the standard configuration). We will further denote the set of corresponding anomaly vectors on $s^*$ as $A_{\alpha_1, \alpha_2, \dots, \alpha_n}$. The $A_\alpha$ for $\alpha = k, \delta, t, w, s, m$, and $\mathcal{A}$ are examined in Section~\ref{sect:params}.

\section{Error measures}
\label{sect:error_measure_eval}

% TODO: figure out how to use subfigure with the nada template
% \begin{figure}[H]
%     \centering
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[trim = 30mm 5mm 35mm 15mm, clip, width=\textwidth]{resources/normalized_euclidean_error_heat_map}
%         \caption{Normalized Euclidean error}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[trim = 30mm 5mm 35mm 15mm, clip, width=\textwidth]{resources/equal_support_error_heat_map}
%         \caption{Equal support error}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[trim = 30mm 5mm 35mm 15mm, clip, width=\textwidth]{resources/full_support_error_heat_map}
%         \caption{Full support error}
%     \end{subfigure}
%     \caption{Heat maps showing the normalized values of $\epsilon(A(\Theta_{k, w}^*, s^*), a^*)$ for the three errors and $k, w = 1,2,\dots,50$. Blue and red correspond to low and high error values, respectively.}
% \label{fig:error_heat_maps}
%     \vspace{-10pt}
% \end{figure}

In Section~\ref{sect:evaluation_measures}, three error measures for anomaly vectors were introduced: the normalized Euclidean error $\epsilon_E$, the equal support error $\epsilon_{ES}$, and the full support error $\epsilon_{FS}$. Since these methods have not been previously studied with regards to sequential anomaly detection, how well they capture the intuitive notions of accuracy must be assessed before they can be used to evaluate problem formulations.

Such an assessment was performed by computing and graphing the errors $\epsilon(A(\Theta_{k, w}^*, s^*), a^*)$ for $\epsilon = \epsilon_E, \epsilon_{ES}$ and $\epsilon_{FS}$ and $(k, w) \in {\{1,2,\dots,50\}}^2$. Heat maps of these values are shown in Figure~\ref{fig:error_heat_maps}. A few of the $A_{k, w}$ given the lowest values by each of the error measures were also graphed (figure~\ref{fig:n_best_anomaly_vectors}).

As shown in the heat maps, the three error measures give similar results, attaining minima and maxima in the same regions. Since $\epsilon_{ES}$ and $\epsilon_{FS}$ operate on binary strings and thus have discrete domains, they often assign identical errors to nearby points. This is the cause of the relatively jagged appearance in the plots of these errors compared to the smoother appearance of the $\epsilon_{E}$ plot.

\begin{figure}[ht]
    \vspace{-5pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/n_best_anomaly_vectors}
    \end{center}
    \vspace{-20pt}
    \caption{\small{The $n$th best $A_{k, w}$ according to the three error measures for $n = 1, 10, 50$ and $100$.}}
\label{fig:n_best_anomaly_vectors}
    \vspace{-10pt}
\end{figure}

Figure~\ref{fig:n_best_anomaly_vectors} shows the anomaly vectors with the $n$th lowest errors for the three distance measures. All three measures give similar anomaly vectors for $n = 1$ and $n = 10$, with the normalized Euclidean and full support errors giving the same anomaly vector in both cases. For $n = 50$ and $n = 100$, however, the full support error seems to prioritize smooth anomaly vectors, while the other two anomaly measures prioritize anomaly vectors with few false positives.

One interesting aspect evidenced in the heat map plot is that while $\epsilon_{ES}$ and $\epsilon_{FS}$ are both very large for $A_{k,w}$ with small $w$, this tendency is not shared by the Euclidean error. As seen in Section~\ref{sect:w}, anomalies significantly larger than $w$ will not be detected by kNN methods, which means that assigning a large value to these anomaly vectors is reasonable. Since the normalized Euclidean error (unlike the other two distances) gives equal weight to every component, it will assign relatively low values to anomaly vectors that only partially capture anomalies as long as most of their elements are close to zero.  Indeed, this is the case for the $A_{k,w}$ with small $w$ since these anomaly vectors are close to constant everywhere except for a few spikes.

\begin{figure}[ht]
    \vspace{-5pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/euclidean_problem}
    \end{center}
    \vspace{-20pt}
    \caption{\small{A reference anomaly vector for a long sequence and two corresponding candidate anomaly vectors. The first candidate vector, while noisy, correctly marks the anomaly. The second candidate does not mark the anomaly and marks two false anomalies. $\epsilon_{E}$, $\epsilon_{ES}$ and $\epsilon_{EF}$ for the two sequences are $8.3$ and $2.2$; $0.010$ and $0.99$; and $0.0050$ and $0.99$, respectively.}}
\label{fig:euclidean_problem}
    \vspace{-10pt}
\end{figure}

As an illustration of the potential problems this could cause, see Figure~\ref{fig:euclidean_problem}, which shows one reference anomaly vector and two candidate anomaly vectors for a long sequence. Here, the first anomaly vector, while noisy, accurately captures the anomaly while the second not only misses the anomaly, but also introduces two false positives. While $\epsilon_{FS}$ and $\epsilon_{ES}$ are significantly smaller for the first candidate than for the second, the reverse is true for $\epsilon_E$. This problem is amplified as the sequence length grows. These results indicate that $\epsilon_E$ should be used with caution, and that since other two error measures are preferable since they were defined specifically to avoid problems such as this.

\clearpage

\section{Parameter values}
\label{sect:params}

Each of the free parameters $(k, \delta, t, w, s, m, \mathcal{A})$ described in Section~\ref{sect:standard_config} are now covered in detail, by means of studying their anomaly vectors on the standard sequence $A_\alpha$ as well as the corresponding errors and evaluation times as $\alpha$ varies.

As mentioned previously, since the analysis in this Section is based on studying the $\Theta_{\alpha}^*$ separately on the sequence $s^*$, it is not sufficient for any conclusions to be drawn either about global minima of $E_{\delta, S}(\Theta)$ or about how well the results might extend to other sequences. Instead, the analysis in this Section should be considered a first step towards establishing a broader understanding of how the $E_{\delta, S}(\Theta)$ vary over the parameter spaces of distance-based problems, and as an introduction to \texttt{ad-eval}, including some useful ways to explore performance characteristics.

\subsection{The k value}
\FloatBarrier{}

\begin{figure}[h]
    \vspace{-10pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/k_heat_map}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Heat map showing $A_k$ for $k$ = $1, 2, \dots, 100$. Red and blue indicate high and low anomaly scores, respectively.}}
\label{fig:k_heat_map}
\end{figure}

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-30pt}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/k_errors}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Errors as a function of $k$ for the standard sequence.}}
\label{fig:k_errors}
    \vspace{-10pt}
\end{wrapfigure}

It is important to study how the anomaly vectors vary with $k$; first, because the choice of $k$ is likely to have a large impact on the appearance of the anomaly vector, regardless of the dataset; and second, because the kNN evaluator only operates on a single $k$ value at a time, it is in a sense the simplest distance-based evaluator, and thus an ideal tool for better understanding how the choice of $k$ impacts the analysis. This understanding is crucial in effectively designing other types of distance-based evaluators. 

In order to understand how the $k$ value affects the resulting anomaly vectors, the anomaly vectors $A_{k}$ for $k = 1,2,\dots,100$ were calculated. Figure~\ref{fig:k_heat_map} shows the resulting anomaly vectors, displayed as a heat map in which all anomaly vectors have been individually normalized to lie in the unit interval. Corresponding values of the three error measures are shown in Figure~\ref{fig:k_errors}. Note that this plot shows only relative errors, as the three error graphs have been individually normalized to the unit interval.

\begin{wrapfigure}{l}{0.5\textwidth}
    \vspace{-20pt}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/k_execution_times}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Evaluation times when varying $k$ on the standard sequence.}}
    \vspace{-20pt}
\label{fig:k_execution_times}
\end{wrapfigure}

Finally, Figure~\ref{fig:k_execution_times} shows the computation times for calculating the anomaly vectors $A_k$. Since the implemented kNN method operates by brute force, the entire reference set must be evaluated regardless of $k$, so the constant evaluation time exhibited in this figure is expected. For any distance measure that is a metric, such as the Euclidean distance, more efficient methods exist.

The smoothness with which the $A_k$ vary with $k$ indicate that using several nearby $k$ in distance-based evaluators is not likely to significantly improve accuracy. Furthermore, at least in this case, $k=1$ minimizes all three error measures, and there is no indication that considering additional $k$ might help. While higher $k$ do lead to other regions being marked as anomalous, these regions do not correspond to relevant features. If this holds in general, there is no need to consider $k$ higher than $1$, and using linear combinations of several $k$ is not likely to lead to any significant increase in accuracy. However, a much more thorough evaluation is required before any conclusions can be drawn.

\subsection{The distance function}
\FloatBarrier{}

\begin{figure}[ht]
    \vspace{-20pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/delta_heat_map}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Heat maps showing $A_{k, \delta}$ for the Euclidean and DTW distances.}}
    \vspace{-10pt}
\label{fig:delta_heat_map}
\end{figure}

For obvious reasons, the choice of the distance function $\delta$ can have a great impact on the anomaly vectors when using distance-based methods. The distance measures implemented in \texttt{ad-eval} are the Euclidean distance, the dynamic time warp (DTW) distance, and the compression-based dissimilarity measure (CDM). To investigate the relative performance of these, the anomaly vectors $A_{k, \delta}$ were examined. Note that since $A_{\delta}$ consists of only one value per distance measure, calculating only $A_\delta$ would have yielded insufficient data.

The \texttt{ad-eval} implementation of the CDM performed poorly. To begin with, it ran significantly slower than the other methods, rendering any comprehensive analysis impossible. Furthermore, it produced poor anomaly vectors. There are a few possible explanations for this. First, the z-normalization step of the SAX transformation (in which each extracted subsequence is given zero empirical mean and unit variance) leads to poor results on random data regardless of the distance measure. Secondly, the window width of $10$ used in the standard configuration means that the extracted sequences are short and can not be efficiently compressed, leading to a roughly constant distance value. While the CDM will likely perform better and with other parameters, it was decided that the CDM would not be investigated further due to its slowness.

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-30pt}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/delta_errors}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Errors for $A_{k, \delta}$.}}
    \vspace{-10pt}
\label{fig:delta_errors}
\end{wrapfigure}

Instead, the focus was placed on comparing the Euclidean and DTW distances. Heat maps of the resulting anomaly vectors are shown in Figure~\ref{fig:delta_heat_map} and a plot of the corresponding errors is shown in Figure~\ref{fig:delta_errors}. As is seen in the heat map, there is generally little difference between the outcomes of the two distance measures; the DTW distance gives slightly `cleaner' (i.e.\@, with non-anomalous regions closer to $0$) anomaly vectors for very low values of $k$, while the Euclidean distance assigns a slightly lower score to the false anomalies encountered at high values of $k$. While there are some differences in the obtained errors---the DTW distance gives a better normalized Euclidean error, while the Euclidean distance generally gives better values of the other two errors---the evaluation is not sufficient to draw any conclusions about the relative merits of the two measures.

However, the fact that the DTW distance does not perform worse than the Euclidean distance in this evaluation is interesting. Since the DTW was designed to recognize long, shifted but relatively similar continuous sequences, it might be expected to perform poorly on other types of data, such as the short, random data used in this evaluation. The fact that this is not the case is a positive indication.

\clearpage

\subsection{Transformations}
\FloatBarrier{}

\begin{figure}[h]
    \vspace{-10pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/dft_heat_map}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Heat maps of the $A_{k, t} $ for $k = 1,2,\dots,100$ with and without the discrete Fourier transform.}}
    \vspace{-10pt}
\label{fig:dft_heat_map}
\end{figure}

As discussed in Chapter~\ref{ch:transformations}, applying transformations to extracted subsequences prior to evaluation, such as to perform dimensionality reduction, might assist in discovering certain types of anomalies. While a large number of compressions and other transformations deserving investigation have been proposed, due to time constraints, only the discrete Fourier transform (DFT) was implemented in \texttt{ad-eval}.

The performance of the DFT was investigated by evaluating the standard sequence for $k = 1,2,\dots, n$ with and without the DFT\@. A heat map of the results is shown in Figure~\ref{fig:dft_heat_map}, and a plot of the corresponding errors is shown in Figure~\ref{fig:dft_errors}.

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-30pt}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/dft_errors}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Errors of the $A_{k, t}$.}}
    \vspace{-10pt}
\label{fig:dft_errors}
\end{wrapfigure}

While the DFT gave fairly accurate anomaly vectors for low values of $k$, it performed poorly overall, returning less accurate anomaly vectors and higher error values over all $k$. This is reasonable: the DFT is not expected to perform well on random data. A proper evaluation of the performance characteristics of kNN methods using the DFT would require a more diverse dataset.

\clearpage

\subsection{The sliding window width}
\FloatBarrier{}
\label{sect:w}

\begin{figure}[h]
    \vspace{-15pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/w_heat_map}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Heat map of the $A_w$ for $w = 1, 2, \dots, 50$.}}
    \vspace{-10pt}
\label{fig:w_heat_map}
\end{figure}

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-20pt}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/w_error}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Errors for the anomaly vectors $A_w$.}}
\label{fig:w_error}
    \vspace{-20pt}
\end{wrapfigure}

Since $w$, the sliding window width, determines the size of the elements used by the evaluator, it should have a significant impact on the size of detected features. To determine if this was the case, the anomaly vectors $A_w$ for $w = 1$, $2$, \dots, $50$ were computed and examined. The results are shown in Figures~\ref{fig:w_heat_map},~\ref{fig:w_error}, and~\ref{fig:w_time}.

As seen in the figures, very low values of this parameter are associated with a very high error. This is expected, since as $w$ tends to $1$, the target anomaly type is reduced to point anomalies. Furthermore, all errors increase sharply as $w$ nears $20$, indicating that large values of $w$ lead to inaccurate results.

\begin{wrapfigure}{l}{0.5\textwidth}
    \vspace{-25pt}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/w_time}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Evaluation times for the anomaly vectors $A_w$.}}
\label{fig:w_time}
    \vspace{-20pt}
\end{wrapfigure}

Interestingly, the plot in Figure~\ref{fig:w_heat_map} shows that beyond $w \approx 3$, increasing $w$ essentially amounts to smoothing the resulting anomaly vectors. Since the anomaly in the standard sequence has a relatively small width of $40$, and since its surroundings have low anomaly values for low values of $w$, this could help explain why the anomaly is not detected after $w \approx 40$. 

It is further interesting to note that while the errors are at a minimum when $w \approx 5$, the anomaly vectors in this area contain three separate spikes in the vicinity of the anomaly, rather than a single smooth bump. Arguably, the anomaly vectors at $w \approx 10$ are preferable, since they more clearly mark the anomaly. This suggests that the error measures may need refinement.

Finally, while the evaluation time ought to be roughly independent of $w$ (or proportional to the evaluation time of the distance metric with vectors of length $w$), Figure~\ref{fig:w_time} shows a decrease in the evaluation time as $w$ grows. This is likely due to the fact that the relatively small width of the evaluation sequence means fewer elements are evaluated as $w$ grows. An evaluation performed on a long sequence, in which the evaluation filter operates on the middle of the sequence while the reference filter operates on the entire sequence, could be used to confirm this.

\subsection{The sliding window step}
\FloatBarrier{}
\label{sect:s}

\begin{figure}[h]
    \vspace{-15pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/s_heat_map}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Heat map of the anomaly vectors $A_s$ for $s = 1,2,\dots,10$. Note that no major false anomalies occur for $s < 8$.}}
    \vspace{-10pt}
\label{fig:s_heat_map}
\end{figure}

%\begin{wrapfigure}[20]{rh}{0.45\textwidth}
%    \begin{center}
%        \includegraphics[width=0.5\textwidth]{resources/s_time}
%    \end{center}
%    \caption{\small{Evaluation times for $A_s$. As expected, the graph indicates that the times are $O(1/s^2)$.}}
%    \label{fig:s_time}
%\end{wrapfigure}

\begin{figure}[h]
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/s_error}
        % \caption{\small{Errors of the anomaly vectors $A_s$.\\ \quad}}
\label{fig:s_error}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth,clip=true,trim=0mm 3mm 0mm 3mm]{resources/s_time}
        \caption{\small{Evaluation times for the $A_s$. As expected, the graph indicates that the times are $O(1/s^2)$.}}
\label{fig:s_time}
    \end{minipage}%
\end{figure}%

%\begin{figure}[h]
%    \begin{center}
%        \includegraphics[width=0.5\textwidth]{resources/s_error}
%    \end{center}
%    \caption{\small{Errors of the anomaly vectors $A_s$.}}
%    \label{fig:s_error}
%\end{figure}

The sliding window step, $s$, is interesting mainly for the large effect it has on the execution time. For a brute-force kNN evaluator with the trivial context and sliding window filters, the number of comparisons performed on a sequence of length $L$ is $\Theta({(L/s)}^2)$. It is therefor desirable to choose a value of $s$ that is as large as possible. However, it is likely that all three errors increase with $s$ for all sequences, and large $s$ values might lead to poor results.

To gain some insight into the performance of kNN methods for higher $s$, the anomaly vectors $A_s$ were computed for $s = 1$ to $10$ (the value of $w$ is $10$ in the default configuration). The results are shown in Figures~\ref{fig:s_heat_map},~\ref{fig:s_error}, and~\ref{fig:s_time}.

As seen in Figure~\ref{fig:s_heat_map}, the anomaly vectors are fairly accurate for all $s$. No major false anomalies are exhibited for $s < 8$, and the actual anomaly is still clearly detected over all $s$. This is reflected in the errors in Figure~\ref{fig:s_time}: all errors are low until $s \geq 8$. Additionally, the evaluation time plot follows the expected $O(1/s^2)$ trend.

In light of these results, perhaps a multi-resolution scheme should be considered, in which a preliminary, `coarse' evaluation (corresponding to high $s$), and a `fine' evaluation (corresponding to low $s$) is performed only on those subsequences which are given the highest anomaly scores in the coarse evaluation. Depending on how the subsequences for the fine evaluation are selected, and on the context type, such an algorithm could achieve either lower computational complexity or an evaluation time reduction by a constant factor. If, as indicated in this evaluation, false positives but no false negatives are introduced as $s$ increases, fine evaluation would only rule out false anomalies, and there would be no loss of analytical power.

\subsection{The context width}
\FloatBarrier{}
\label{sect:m}

\begin{figure}[H]
    \vspace{-15pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/m_heat_map}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Heat map of the $A_m$ for $m = 20, 21, \dots, 400$. Note the false anomaly present at the left end of the anomaly vectors until $m \approx 330$.}}
    \vspace{-10pt}
\label{fig:m_heat_map}
\end{figure}

Which values of the context width $m$ are appropriate depends heavily on the application domain and on the types of anomalies present in the data. Ideally, the importance of the context width should be evaluated by considering several sequences with a natural context concept, such as the bottom series in Figure~\ref{fig:anomaly_types}. Constructing representative artificial datasets of such sequences is likely to be difficult, so real-world series should be used for such an evaluation.

\begin{wrapfigure}{r}{0.49\textwidth} \vspace{-20pt}
    \vspace{-10pt}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/m_error}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Errors of the anomaly vectors $A_m$.}}
    \vspace{-20pt}
\label{fig:m_error}
\end{wrapfigure}

While such datasets are not available, a simple evaluation on the available data can still prove illuminating. The standard sequence is highly homogeneous and has no natural contexts. Thus, all errors should be expected to decrease monotonically with increasing $m$. To confirm this, the anomaly vectors $A_m$ were computed for $m = 20$ to $400$. The results of this evaluation are shown in Figures~\ref{fig:m_heat_map},~\ref{fig:m_error}, and~\ref{fig:m_time}.

As these figures demonstrate, the anomaly vectors identify a false anomaly at the left end until $m \approx 330$, at which point the false anomaly disappears and the errors decline sharply. That this false anomaly appears for small context widths is understandable since, as seen in Figure~\ref{fig:reference_sequence}, the sequence includes values at its left end that are not seen again until the right end. As expected, the error is minimized when the trivial context (corresponding to $m > 390$) is incorporated.

\begin{wrapfigure}{r}{0.49\textwidth}
    \vspace{-30pt}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/m_time}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Evaluation times of the anomaly vectors $A_m$.}}
    \vspace{-20pt}
\label{fig:m_time}
\end{wrapfigure}

Finally, it should be noted that while the size of the reference set, and consequently the evaluation time, grows linearly with the size of the context, the average context size only grows linearly with $m$ when $m$ is much smaller than the sequence length. When $m$ is close to the sequence length, the context size for a large portion of the subsequences extracted by the evaluation filter will be limited by the sequence edges. This leads to the curve in Figure~\ref{fig:m_time}.

\clearpage

\subsection{The aggregator}
\FloatBarrier{}
\label{sect:A}

To get an idea of how the choice of aggregator affects the analysis, the anomaly vectors $A_{k, \mathcal{A}}$ were computed and analyzed for the minimum, maximum, median and mean aggregators, with $k = 1,2,\dots,100$. Heat map plots of the results are shown in Figure~\ref{fig:aggregator_heat_map}, and plots of the corresponding error measures are shown in Figure~\ref{fig:aggregator_error}. Single anomaly vectors for $k=1$ are shown in Figure~\ref{fig:aggregator_vectors}.

As seen in Figures~\ref{fig:aggregator_error} and~\ref{fig:aggregator_vectors}, the min and max aggregators produce blockly, piecewice constant anomaly vectors, while the mean aggregator (and, to a lesser extent, the median aggregator) produces smooth, continuous anomaly vectors.

\begin{figure}[!ht]
    \vspace{-20pt}
    \begin{center}
        \includegraphics[trim = 5mm 2mm 10mm 2mm, clip, width=\textwidth]{resources/aggregator_heat_map}
    \end{center}
    \vspace{-10pt}
    \caption{\small{Heat maps showing $A_{k, \mathcal{A}}$ for the four aggregators.}}
\label{fig:aggregator_heat_map}
    \vspace{-15pt}
\end{figure}

As could be expected, the minimum aggregator consistently led to the highest values of $\epsilon_{FS}$. It is likely to give a low score to a point if a single element containing that point has a low anomaly score, which effectively means that parts of anomalies will tend to be undervalued---something the full support error is sensitive to. In contrast, the maximum aggregator consistently led to the lowest support error values. This is also as expected, since max will assign high values to any point contained in an anomalous subsequence. The median and mean aggregators performed roughly equally well---while the mean performed better for higher $k$, this is not relevant; both aggregators were very far off for higher $k$.

\begin{figure}[!ht]
    \vspace{-5pt}
    \begin{center}
        \includegraphics[width=\textwidth]{resources/aggregator_error}
    \end{center}
    \vspace{-18pt}
    \caption{\small{Errors of the anomaly vectors $A_{k, \mathcal{A}}$.}}
\label{fig:aggregator_error}
    \vspace{-10pt}
\end{figure}

Similar, but less clear, results were obtained for the equal support errors. The minimum aggregator consistently performed the worst with low $k$, while the maximum aggregator performed the best, on average, with $k$ up to $40$. Again, the mean and median aggregators performed too similarly for any conclusions to be drawn on their relative merits.

\begin{wrapfigure}{r}{0.49\textwidth}
    \vspace{-20pt}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/aggregator_vectors}
    \end{center}
    \vspace{-20pt}
    \caption{\small{Plot of the $A_{k, \mathcal{A}}$ for $k=1$.}}
    \vspace{-0pt}
\label{fig:aggregator_vectors}
\end{wrapfigure}

Finally, the normalized Euclidean error gives almost indentical values to the mean and median aggregators, but exhibits a clear preference for the minimum aggregator over the maximum aggregator. This is likely a consequence of the fact fact that the minimum aggregator tends to assign scores close to zero to all elements except for a few, while the maximum aggregator tends to assign scores close to zero to only a few elements. As discussed in Section~\ref{sect:error_measure_eval}, the normalized Euclidean error has a bias in favor of anomaly vectors where most elements are close to zero, unlike the type of anomaly vectors produced by the maximum aggregator.

In conclusion, all aggregators performed roughly equally well on $s^*$ (arguably, the minimum aggregator performed slightly worse than the others). If this holds in general, then it appears that the choice of aggregator is mainly one of aesthetics.
